{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"./burger_dataset/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = 1000\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))\n",
    "semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained0unlabeledsamples_4.6e-8.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    if mse_loss.requires_grad:\n",
    "        mse_loss.backward(retain_graph=False)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n"
     ]
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None\n",
    "    \n",
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "suggested_lr = 1e-7\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setting up the training epochs\n",
    "epochs1 = 100; epochs2 = 50000\n",
    "\n",
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = epochs1-100\n",
    "\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]Training the generator for \n",
      "100%|██████████| 300/300 [05:20<00:00,  1.07s/it]\n",
      "The best generator loss: -3.195648193359375\n",
      "Semi-supervised solver loss @Epoch 0:  2.08036470413208\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1000; best_generator_state_dict = None\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "            generator_loss = sinkhorn_loss(X_gen, X_u_train)-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        generator.load_state_dict(best_generator_state_dict)\n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "        X_gen = sampling_from_rows(X_gen, N_res)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "    semisup_model.train()\n",
    "    semisup_model.network.eval()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = LBFGSNew(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "#                               lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  4.982294967703638e-07\n",
      "Epoch 100:  3.2799366778135663e-08\n",
      "Epoch 200:  1.9893745317745015e-08\n",
      "Epoch 300:  1.3127571918403191e-08\n",
      "Epoch 400:  9.18277276440449e-09\n",
      "Epoch 500:  6.80095757488175e-09\n",
      "Epoch 600:  5.458014040726766e-09\n",
      "Epoch 700:  4.818772936232563e-09\n",
      "Epoch 800:  4.248957186092639e-09\n",
      "Epoch 900:  3.970467954417245e-09\n",
      "Epoch 1000:  3.73294417599368e-09\n",
      "Epoch 1100:  3.3556855072447433e-09\n",
      "Epoch 1200:  3.1231670583053983e-09\n",
      "Epoch 1300:  2.9067745987987337e-09\n",
      "Epoch 1400:  2.69957589615899e-09\n",
      "Epoch 1500:  2.552870803640417e-09\n",
      "Epoch 1600:  2.4487107896931093e-09\n",
      "Epoch 1700:  2.4276280985446874e-09\n",
      "Epoch 1800:  2.304242574524551e-09\n",
      "Epoch 1900:  2.281877575782687e-09\n",
      "Epoch 2000:  2.14301842937914e-09\n",
      "Epoch 2100:  2.03778482976702e-09\n",
      "Epoch 2200:  1.9963559694247124e-09\n",
      "Epoch 2300:  1.9434447384725217e-09\n",
      "Epoch 2400:  1.9031609621578127e-09\n",
      "Epoch 2500:  1.7885996017596995e-09\n",
      "Epoch 2600:  1.7726493606318172e-09\n",
      "Epoch 2700:  1.754705603040918e-09\n",
      "Epoch 2800:  1.7007166785987238e-09\n",
      "Epoch 2900:  1.6907093502993575e-09\n",
      "Epoch 3000:  1.6352481591042078e-09\n",
      "Epoch 3100:  1.6286013648780795e-09\n",
      "Epoch 3200:  1.6015364590060699e-09\n",
      "Epoch 3300:  1.5567405142746793e-09\n",
      "Epoch 3400:  1.5549350695920339e-09\n",
      "Epoch 3500:  1.5475138948062295e-09\n",
      "Epoch 3600:  1.4982756146864062e-09\n",
      "Epoch 3700:  1.4851018192985066e-09\n",
      "Epoch 3800:  1.4792043145916978e-09\n",
      "Epoch 3900:  1.4522929525639938e-09\n",
      "Epoch 4000:  1.416223138761552e-09\n",
      "Epoch 4100:  1.4056811270535263e-09\n",
      "Epoch 4200:  1.400765503589696e-09\n",
      "Epoch 4300:  1.3967852430241123e-09\n",
      "Epoch 4400:  1.3853526104057323e-09\n",
      "Epoch 4500:  1.3688862265937018e-09\n",
      "Epoch 4600:  1.3639211982052757e-09\n",
      "Epoch 4700:  1.3616345828637577e-09\n",
      "Epoch 4800:  1.357176149241468e-09\n",
      "Epoch 4900:  1.3520345953921264e-09\n",
      "Epoch 5000:  1.3434442447390893e-09\n",
      "Epoch 5100:  1.3261446385470776e-09\n",
      "Epoch 5200:  1.302472685260625e-09\n",
      "Epoch 5300:  1.292491336180035e-09\n",
      "Epoch 5400:  1.2904487478593296e-09\n",
      "Epoch 5500:  1.2883970557098223e-09\n",
      "Epoch 5600:  1.2859870945902685e-09\n",
      "Epoch 5700:  1.2782727099036606e-09\n",
      "Epoch 5800:  1.2647501934637262e-09\n",
      "Epoch 5900:  1.2520188219511397e-09\n",
      "Epoch 6000:  1.2366732082824683e-09\n",
      "Epoch 6100:  1.2311540675824517e-09\n",
      "Epoch 6200:  1.226207135829327e-09\n",
      "Epoch 6300:  1.2258730697212172e-09\n",
      "Epoch 6400:  1.2189576015231296e-09\n",
      "Epoch 6500:  1.217704936884445e-09\n",
      "Epoch 6600:  1.2144565353366943e-09\n",
      "Epoch 6700:  1.2073089195041575e-09\n",
      "Epoch 6800:  1.1944182309875373e-09\n",
      "Epoch 6900:  1.1852855363869708e-09\n",
      "Epoch 7000:  1.1787647524741374e-09\n",
      "Epoch 7100:  1.1744626382537149e-09\n",
      "Epoch 7200:  1.1705607594336698e-09\n",
      "Epoch 7300:  1.1676667410753794e-09\n",
      "Epoch 7400:  1.1647159903205306e-09\n",
      "Epoch 7500:  1.159269569228627e-09\n",
      "Epoch 7600:  1.150907036340243e-09\n",
      "Epoch 7700:  1.1393749277388565e-09\n",
      "Epoch 7800:  1.1299542412857022e-09\n",
      "Epoch 7900:  1.1134879684959742e-09\n",
      "Epoch 8000:  1.0978150610796433e-09\n",
      "Epoch 8100:  1.0855236709517158e-09\n",
      "Epoch 8200:  1.0791527671472068e-09\n",
      "Epoch 8300:  1.0735605737721698e-09\n",
      "Epoch 8400:  1.0706431297080599e-09\n",
      "Epoch 8500:  1.064801802286297e-09\n",
      "Epoch 8600:  1.0584597642804283e-09\n",
      "Epoch 8700:  1.0555332163875164e-09\n",
      "Epoch 8800:  1.0377569914510332e-09\n",
      "Epoch 8900:  1.0353069512802904e-09\n",
      "Epoch 9000:  1.027573026668449e-09\n",
      "Epoch 9100:  1.0172173103839555e-09\n",
      "Epoch 9200:  1.0130171146371936e-09\n",
      "Epoch 9300:  1.0097722658031216e-09\n",
      "Epoch 9400:  1.0056514510026204e-09\n",
      "Epoch 9500:  1.002646299319565e-09\n",
      "Epoch 9600:  9.983739390762025e-10\n",
      "Epoch 9700:  9.937162204209926e-10\n",
      "Epoch 9800:  9.911671483564533e-10\n",
      "Epoch 9900:  9.885843255119653e-10\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(10000):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 100) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Test MSE: 3.6361559097031204e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "MODEL_PATH = './saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples_3.6e-8.pth'\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")\n",
    "torch.save(semisup_model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6629e-08, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "# semisup_model.load_state_dict(torch.load(MODEL_PATH), strict=False)\n",
    "# semisup_model.eval()\n",
    "# F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-2000-V1.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-2000-V1.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-25600-V1.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-25600-V1.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
