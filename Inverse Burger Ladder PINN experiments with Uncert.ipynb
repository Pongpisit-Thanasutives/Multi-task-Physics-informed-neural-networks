{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC  # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 2000\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:, None]\n",
    "x = data['x'].flatten()[:, None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, uncert_mode=0):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        print('Init using xavier')\n",
    "        self.model.apply(self.xavier_init)\n",
    "        self.uncert = 0\n",
    "        self.log_vars = None\n",
    "        \n",
    "        if uncert_mode>0:\n",
    "            print(\"Createing an Uncert network\")\n",
    "            self.uncert = uncert_mode\n",
    "            self.log_vars = nn.Parameter(torch.zeros((uncert_mode)))\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, data, inference=True):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def loss(self, data, y_input, include_unsup=False):\n",
    "        uf, unsup_loss = self.forward(data)\n",
    "        mse_loss = F.mse_loss(uf, y_input)\n",
    "        if not include_unsup:\n",
    "            return mse_loss\n",
    "        \n",
    "        if self.uncert:\n",
    "            mse_loss = mse_loss.unsqueeze(0)\n",
    "            unsup_loss = unsup_loss.unsqueeze(0)\n",
    "            losses = torch.cat([mse_loss, unsup_loss])\n",
    "            weights = torch.exp(-self.log_vars)\n",
    "            return torch.dot(weights, losses)\n",
    "        \n",
    "        return mse_loss + unsup_loss\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        uf, _ = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "uncert_mode = 2\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23471723d7cd4f0fb7a425c5f7feaba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.70E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init using xavier\n",
      "Createing an Uncert network\n"
     ]
    }
   ],
   "source": [
    "print('Learning rate finding')\n",
    "bs = 4000\n",
    "bs = N if bs>N else bs\n",
    "criterion = LadderUncertLoss(n_task=2)\n",
    "tmp_optimizer = SGDGC(model.parameters(), lr=1e-7, use_gc=True, nesterov=True, momentum=0.9)\n",
    "trainloader = get_dataloader(X_u_train, u_train, bs=4000)\n",
    "lr_finder = LRFinder(model, optimizer=tmp_optimizer, criterion=criterion, device=\"cpu\")\n",
    "lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "_, suggested_lr = lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "\n",
    "lr_finder.reset()\n",
    "network = Network(model=model, uncert_mode=uncert_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pcgrad_closure():\n",
    "#     n_obj = 2 # There are two tasks\n",
    "#     losses = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "#     updated_grads = []\n",
    "    \n",
    "#     for i in range(n_obj):\n",
    "#         optimizer1.zero_grad()\n",
    "#         losses[i].backward(retain_graph=True)\n",
    "\n",
    "#         g_task = []\n",
    "#         for param in network.parameters():\n",
    "#             if param.grad is not None:\n",
    "#                 g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "#             else:\n",
    "#                 g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "#         # appending the gradients from each task\n",
    "#         updated_grads.append(g_task)\n",
    "\n",
    "#     updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "#     for idx, param in enumerate(network.parameters()): \n",
    "#         param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "#     return sum(losses)\n",
    "\n",
    "def uncert_closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer1.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=False)\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the lookahead option\n",
      "1st Phase optimization using SGDGC/AdamGC with PCGrad gradient modification\n",
      "Epoch 0:  0.8941341638565063\n",
      "Epoch 10:  0.32361406087875366\n",
      "Epoch 20:  0.16779792308807373\n",
      "Epoch 30:  0.10675227642059326\n",
      "Epoch 40:  0.07974432408809662\n",
      "Epoch 50:  0.06521487981081009\n",
      "Epoch 60:  0.05602208524942398\n",
      "Epoch 70:  0.049556128680706024\n",
      "Epoch 80:  0.044639453291893005\n",
      "Epoch 90:  0.040727823972702026\n",
      "Epoch 100:  0.03749646991491318\n",
      "Epoch 110:  0.03478012979030609\n",
      "Epoch 120:  0.03246025741100311\n",
      "Epoch 130:  0.030436057597398758\n",
      "Epoch 140:  0.028675155714154243\n",
      "Epoch 150:  0.02710013836622238\n",
      "Epoch 160:  0.02569832280278206\n",
      "Epoch 170:  0.02444814145565033\n",
      "Epoch 180:  0.02331254631280899\n",
      "Epoch 190:  0.022281596437096596\n",
      "Epoch 200:  0.021336115896701813\n",
      "Epoch 210:  0.020479463040828705\n",
      "Epoch 220:  0.019684769213199615\n",
      "Epoch 230:  0.018948208540678024\n",
      "Epoch 240:  0.018263783305883408\n",
      "Epoch 250:  0.017639946192502975\n",
      "Epoch 260:  0.01704898476600647\n",
      "Epoch 270:  0.016498642042279243\n",
      "Epoch 280:  0.01598069816827774\n",
      "Epoch 290:  0.015504512004554272\n",
      "Epoch 300:  0.015052612870931625\n",
      "Epoch 310:  0.014617313630878925\n",
      "Epoch 320:  0.014218227937817574\n",
      "Epoch 330:  0.013832451775670052\n",
      "Epoch 340:  0.01346900500357151\n",
      "Epoch 350:  0.013129801489412785\n",
      "Epoch 360:  0.012807102873921394\n",
      "Epoch 370:  0.01248951070010662\n",
      "Epoch 380:  0.012200571596622467\n",
      "Epoch 390:  0.01191760879009962\n",
      "Epoch 400:  0.01164771243929863\n",
      "Epoch 410:  0.011393522843718529\n",
      "Epoch 420:  0.01114675123244524\n",
      "Epoch 430:  0.010913938283920288\n",
      "Epoch 440:  0.010685489512979984\n",
      "Epoch 450:  0.010471065528690815\n",
      "Epoch 460:  0.010259035974740982\n",
      "Epoch 470:  0.010067777708172798\n",
      "Epoch 480:  0.00987251102924347\n",
      "Epoch 490:  0.00968700647354126\n",
      "Epoch 500:  0.009507616981863976\n",
      "Epoch 510:  0.009339001961052418\n",
      "Epoch 520:  0.009170608595013618\n",
      "Epoch 530:  0.009009717032313347\n",
      "Epoch 540:  0.00886322557926178\n",
      "Epoch 550:  0.008710178546607494\n",
      "Epoch 560:  0.008564407005906105\n",
      "Epoch 570:  0.00842650793492794\n",
      "Epoch 580:  0.00829245150089264\n",
      "Epoch 590:  0.008160777390003204\n",
      "Epoch 600:  0.008033420890569687\n",
      "Epoch 610:  0.007912971079349518\n",
      "Epoch 620:  0.0077914344146847725\n",
      "Epoch 630:  0.007676765322685242\n",
      "Epoch 640:  0.007568174973130226\n",
      "Epoch 650:  0.007454398088157177\n",
      "Epoch 660:  0.007347963750362396\n",
      "Epoch 670:  0.007247489877045155\n",
      "Epoch 680:  0.007146893069148064\n",
      "Epoch 690:  0.007047708611935377\n",
      "Epoch 700:  0.006954045034945011\n",
      "Epoch 710:  0.0068619223311543465\n",
      "Epoch 720:  0.006767870392650366\n",
      "Epoch 730:  0.006682557053864002\n",
      "Epoch 740:  0.006598402746021748\n",
      "Epoch 750:  0.006514913402497768\n",
      "Epoch 760:  0.006432906724512577\n",
      "Epoch 770:  0.006351744756102562\n",
      "Epoch 780:  0.00627466244623065\n",
      "Epoch 790:  0.006199295166879892\n",
      "Epoch 800:  0.006126163527369499\n",
      "Epoch 810:  0.006052385549992323\n",
      "Epoch 820:  0.0059828017838299274\n",
      "Epoch 830:  0.005913757719099522\n",
      "Epoch 840:  0.005849657580256462\n",
      "Epoch 850:  0.005780011415481567\n",
      "Epoch 860:  0.005714893341064453\n",
      "Epoch 870:  0.005653094965964556\n",
      "Epoch 880:  0.005590650252997875\n",
      "Epoch 890:  0.005530135706067085\n",
      "Epoch 900:  0.005471150390803814\n",
      "Epoch 910:  0.005413968116044998\n",
      "Epoch 920:  0.005356344394385815\n",
      "Epoch 930:  0.0052998242899775505\n",
      "Epoch 940:  0.005245531443506479\n",
      "Epoch 950:  0.005191325210034847\n",
      "Epoch 960:  0.00513856066390872\n",
      "Epoch 970:  0.005087650381028652\n",
      "Epoch 980:  0.0050380639731884\n",
      "Epoch 990:  0.0049901786260306835\n",
      "Epoch 999:  0.004944219719618559\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.039179615676403046\n",
      "Epoch 10:  8.384376269532368e-05\n",
      "Epoch 20:  9.273524483432993e-06\n",
      "Epoch 30:  4.398671990202274e-06\n",
      "Epoch 40:  2.8744193514285143e-06\n",
      "Epoch 50:  1.839558990468504e-06\n",
      "Epoch 60:  1.0503755447643925e-06\n",
      "Epoch 70:  1.04890966667881e-06\n",
      "Epoch 80:  1.04890966667881e-06\n",
      "Epoch 90:  1.04890966667881e-06\n",
      "Epoch 100:  1.04890966667881e-06\n",
      "Epoch 110:  1.04890966667881e-06\n",
      "Epoch 120:  1.04890966667881e-06\n",
      "Epoch 130:  1.04890966667881e-06\n",
      "Epoch 140:  1.04890966667881e-06\n",
      "Epoch 150:  1.04890966667881e-06\n",
      "Epoch 160:  1.04890966667881e-06\n",
      "Epoch 170:  1.04890966667881e-06\n",
      "Epoch 180:  1.04890966667881e-06\n",
      "Epoch 190:  1.04890966667881e-06\n",
      "Epoch 200:  1.04890966667881e-06\n",
      "Epoch 210:  1.04890966667881e-06\n",
      "Epoch 220:  1.04890966667881e-06\n",
      "Epoch 230:  1.04890966667881e-06\n",
      "Epoch 240:  1.04890966667881e-06\n",
      "Epoch 250:  1.04890966667881e-06\n",
      "Epoch 260:  1.04890966667881e-06\n",
      "Epoch 270:  1.04890966667881e-06\n",
      "Epoch 280:  1.04890966667881e-06\n",
      "Epoch 290:  1.04890966667881e-06\n",
      "Epoch 299:  1.04890966667881e-06\n"
     ]
    }
   ],
   "source": [
    "lookahead = False \n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=suggested_lr, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead:\n",
    "    print(\"Using the lookahead option\")\n",
    "    optimizer1 = Lookahead(optimizer1)\n",
    "else:\n",
    "    print(\"Not using the lookahead option\")\n",
    "    \n",
    "epochs1 = 1000 # How long this should be ??? (500 and 1000 seem to be a good number.)\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGDGC/AdamGC with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(uncert_closure)\n",
    "    l = uncert_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, uncert_mode=uncert_mode)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-3, max_iter=80, max_eval=100, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.343630841205595e-06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncert seems to also have large generalization gap with LBFGS[lr=5e-2,1e-1,]\n",
    "((network(X_star)[0].detach() - u_star)**2).mean().item()\n",
    "# BEST-full: 5.905130819883198e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
