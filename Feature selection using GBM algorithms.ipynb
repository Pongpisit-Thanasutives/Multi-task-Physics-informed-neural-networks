{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I need to implement the GPU version for faster computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "\n",
    "from mlens.ensemble import SuperLearner\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "import shap\n",
    "from utils import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.apply(self.xavier_init)\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with noisy features -> chk feature importance\n",
    "        layers = [nn.Linear(X_train_dim, 50), nn.Tanh(), nn.Linear(50, 1)]\n",
    "        self.nonlinear_model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=simple_solver_model(50))\n",
    "# selector = SeclectorNetwork(X_train_dim=6)\n",
    "\n",
    "# optimizer = torch.optim.LBFGS(list(network.parameters()) + list(selector.parameters()), \n",
    "#                               lr=5e-2, max_iter=80, max_eval=100, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "# # optimizer = torch.optim.Adam(list(network.parameters()) + list(selector.parameters()), lr=1e-3)\n",
    "# epochs = 5000; testing = False\n",
    "\n",
    "# if testing:\n",
    "#     # unsupervised_loss\n",
    "#     unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)))\n",
    "#     sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "#     # No MTL yet, apply the naive summation first to see if it's working?\n",
    "#     total_loss = unsup_loss + sup_loss\n",
    "#     print(total_loss)\n",
    "\n",
    "#     total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.train(); selector.train()\n",
    "# curr_loss = 1000\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Total loss calculation process\n",
    "#         # unsupervised_loss\n",
    "#         unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)))\n",
    "#         sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "#         # No MTL yet, apply the naive summation first to see if it's working?\n",
    "#         total_loss = unsup_loss + sup_loss\n",
    "#         total_loss.backward()\n",
    "        \n",
    "#         return total_loss\n",
    "    \n",
    "#     optimizer.step(closure)\n",
    "    \n",
    "#     l = closure()\n",
    "#     if l.item() != curr_loss:\n",
    "#         curr_loss = l.item()\n",
    "#     else: break; print(\"Stop training.\")\n",
    "    \n",
    "#     if (i % 10) == 0:\n",
    "#         print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "# print(\"Testing\")\n",
    "# network.eval()\n",
    "# F.mse_loss(network(*dimension_slicing(X_star)).detach(), u_star) # Around 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_selector, y_selector = network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "# e = shap.DeepExplainer(selector, X_selector)\n",
    "# shap_values = e.shap_values(X_selector)\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\n",
    "#     \"mean_abs_shap\": np.mean(np.abs(shap_values), axis=0), \n",
    "#     \"stdev_abs_shap\": np.std(np.abs(shap_values), axis=0), \n",
    "#     \"name\": ['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']\n",
    "# })\n",
    "\n",
    "# print(df.sort_values(\"mean_abs_shap\", ascending=False)[:10])\n",
    "\n",
    "# shap.summary_plot(shap_values, features=X_selector, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using statistical models to find feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.load(\"./saved_path_inverse_burger/data/derivatives-2000.npy\")\n",
    "y_np = np.load(\"./saved_path_inverse_burger/data/dynamics-2000.npy\")\n",
    "\n",
    "X_np_test = np.load(\"./saved_path_inverse_burger/data/derivatives-25600.npy\")\n",
    "y_np_test = np.load(\"./saved_path_inverse_burger/data/dynamics-25600.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor()\n",
    "xg = xgboost.XGBRegressor(reg_alpha=0.1)\n",
    "cat = catboost.CatBoostRegressor(iterations=None, depth=4, learning_rate=0.1, verbose=0, l2_leaf_reg=10)\n",
    "light = lightgbm.LGBMRegressor(n_estimators=200, learning_rate=0.1, reg_lambda=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearner(array_check=None, backend=None, folds=3,\n",
       "       layers=[Layer(backend='threading', dtype=<class 'numpy.float32'>, n_jobs=-1,\n",
       "   name='layer-1', propagate_features=None, raise_on_exception=True,\n",
       "   random_state=None, shuffle=False,\n",
       "   stack=[Group(backend='threading', dtype=<class 'numpy.float32'>,\n",
       "   indexer=FoldIndex(X=None, folds=3, raise_on_ex...5c97e50>)],\n",
       "   n_jobs=-1, name='group-1', raise_on_exception=True, transformers=[])],\n",
       "   verbose=0)],\n",
       "       model_selection=True, n_jobs=None, raise_on_exception=True,\n",
       "       random_state=None, sample_size=20,\n",
       "       scorer=<function mean_squared_error at 0x135c97e50>, shuffle=False,\n",
       "       verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = SuperLearner(scorer=mean_squared_error, folds=3, model_selection=True, n_jobs=3)\n",
    "ensemble.add([xg, light, cat])\n",
    "ensemble.add_meta(forest)\n",
    "ensemble.fit(X_np, y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Training MSE: 0.0033935609\n",
      "Test MSE: 0.028090756\n",
      "Training GBM algos...\n",
      "Done training\n",
      "Training MSE: 0.00017310298\n",
      "('u_x', 0.43132782464468283)\n",
      "('u_xx', 0.2725011438783386)\n",
      "('uf', 0.23068548996767577)\n",
      "('u_xt', 0.037911415523790744)\n",
      "('u_tt', 0.027574125985512055)\n",
      "('u_tx', 0.0)\n",
      "Done training\n",
      "Training MSE: 0.015096086314121248\n",
      "('u_xx', 0.2805)\n",
      "('u_x', 0.25283333333333335)\n",
      "('uf', 0.16966666666666666)\n",
      "('u_xt', 0.1495)\n",
      "('u_tt', 0.1475)\n",
      "('u_tx', 0.0)\n",
      "Done training\n",
      "Training MSE: 0.0010312966521267757\n",
      "('u_x', 0.3998005900828352)\n",
      "('uf', 0.3689910808122835)\n",
      "('u_xx', 0.12724149982994676)\n",
      "('u_tt', 0.06889158417508844)\n",
      "('u_tx', 0.01928175440192941)\n",
      "('u_xt', 0.015793490697916725)\n"
     ]
    }
   ],
   "source": [
    "sklearn_model = SklearnModel(model=ensemble, X_train=X_np, y_train=y_np, feature_names=feature_names)\n",
    "print('Test MSE:', sklearn_model.test(X_np_test, y_np_test))\n",
    "print('Training GBM algos...')\n",
    "xg_feature_importance = SklearnModel(model=xg, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()\n",
    "light_feature_importance = SklearnModel(model=light, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()\n",
    "cat_feature_importance = SklearnModel(model=cat, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Training MSE: 0.00017310298\n",
      "('u_x', 0.43132782464468283)\n",
      "('u_xx', 0.2725011438783386)\n",
      "('uf', 0.23068548996767577)\n",
      "('u_xt', 0.037911415523790744)\n",
      "('u_tt', 0.027574125985512055)\n",
      "('u_tx', 0.0)\n",
      "Done training\n",
      "Training MSE: 0.015096086314121248\n",
      "('u_xx', 0.2805)\n",
      "('u_x', 0.25283333333333335)\n",
      "('uf', 0.16966666666666666)\n",
      "('u_xt', 0.1495)\n",
      "('u_tt', 0.1475)\n",
      "('u_tx', 0.0)\n",
      "Done training\n",
      "Training MSE: 0.0010312966521267757\n",
      "('u_x', 0.3998005900828352)\n",
      "('uf', 0.3689910808122835)\n",
      "('u_xx', 0.12724149982994676)\n",
      "('u_tt', 0.06889158417508844)\n",
      "('u_tx', 0.01928175440192941)\n",
      "('u_xt', 0.015793490697916725)\n"
     ]
    }
   ],
   "source": [
    "xg_feature_importance = SklearnModel(model=xg, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()\n",
    "light_feature_importance = SklearnModel(model=light, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()\n",
    "cat_feature_importance = SklearnModel(model=cat, X_train=X_np, y_train=y_np, feature_names=feature_names).feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uf': 0.23068548996767577,\n",
       " 'u_x': 0.43132782464468283,\n",
       " 'u_xx': 0.2725011438783386,\n",
       " 'u_tt': 0.027574125985512055,\n",
       " 'u_xt': 0.037911415523790744,\n",
       " 'u_tx': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_feature_importances = {}\n",
    "for f in feature_names:\n",
    "    avg_feature_importances[f] = (xg_feature_importance[f]+cat_feature_importance[f]+light_feature_importance[f])/3\n",
    "avg_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a neural network to fit derivatives and dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = to_tensor(X_np, False)\n",
    "y_tensor = to_tensor(y_np, False).reshape(-1, 1)\n",
    "mlp = TorchMLP([6, 50, 50, 50, 50, 50, 1])\n",
    "optimizer = torch.optim.LBFGS(mlp.parameters(), \n",
    "                              lr=0.1, max_iter=100, max_eval=125, # 80 and 100 are OK!\n",
    "                              history_size=120, line_search_fn='strong_wolfe')\n",
    "mlp.train(); epochs=500\n",
    "for i in range(epochs):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        l = F.mse_loss(mlp(X_tensor), y_tensor)\n",
    "        l.backward()\n",
    "        return l\n",
    "    optimizer.step(closure)\n",
    "    l = closure()\n",
    "    if i % 100 == 0:\n",
    "        print('MSE Loss:', l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.eval(); ((mlp(X_tensor) - y_tensor)**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SHAP's outputs are not what I expected.\n",
    "\n",
    "mlp.eval()\n",
    "\n",
    "e = shap.DeepExplainer(mlp, X_tensor)\n",
    "shap_values = e.shap_values(X_tensor)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"mean_abs_shap\": np.mean(np.abs(shap_values), axis=0), \n",
    "    \"stdev_abs_shap\": np.std(np.abs(shap_values), axis=0), \n",
    "    \"name\": ['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']\n",
    "})\n",
    "\n",
    "print(df.sort_values(\"mean_abs_shap\", ascending=False))\n",
    "shap.summary_plot(shap_values, features=X_tensor, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(mlp)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(mlp)\n",
    "gs = GradientShap(mlp)\n",
    "fa = FeatureAblation(mlp)\n",
    "\n",
    "ig_attr_test = ig.attribute(X_tensor, n_steps=50)\n",
    "ig_nt_attr_test = ig_nt.attribute(X_tensor)\n",
    "dl_attr_test = dl.attribute(X_tensor)\n",
    "gs_attr_test = gs.attribute(X_tensor, X_tensor)\n",
    "fa_attr_test = fa.attribute(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare attributions for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis_data = np.arange(X_tensor.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(ig_nt_attr_test_sum, ord=1)\n",
    "\n",
    "dl_attr_test_sum = dl_attr_test.detach().numpy().sum(0)\n",
    "dl_attr_test_norm_sum = dl_attr_test_sum / np.linalg.norm(dl_attr_test_sum, ord=1)\n",
    "\n",
    "gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)\n",
    "gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)\n",
    "\n",
    "fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "lin_weight = mlp.model[0].weight[0].detach().numpy()\n",
    "y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\n",
    "width = 0.14\n",
    "legends = ['Int Grads', 'Int Grads w/SmoothGrad','DeepLift', 'GradientSHAP', 'Feature Ablation', 'Weights']\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "ax.set_ylabel('Attributions')\n",
    "\n",
    "FONT_SIZE = 16\n",
    "plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "ax.bar(x_axis_data, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "ax.bar(x_axis_data + width, ig_nt_attr_test_norm_sum, width, align='center', alpha=0.7, color='#A90000')\n",
    "ax.bar(x_axis_data + 2 * width, dl_attr_test_norm_sum, width, align='center', alpha=0.6, color='#34b8e0')\n",
    "ax.bar(x_axis_data + 3 * width, gs_attr_test_norm_sum, width, align='center',  alpha=0.8, color='#4260f5')\n",
    "ax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "ax.bar(x_axis_data + 5 * width, y_axis_lin_weight, width, align='center', alpha=1.0, color='grey')\n",
    "ax.autoscale_view()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_xticks(x_axis_data + 0.5)\n",
    "ax.set_xticklabels(x_axis_data_labels)\n",
    "\n",
    "plt.legend(legends, loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argsort(ig_attr_test_norm_sum))\n",
    "print(np.argsort(dl_attr_test_norm_sum))\n",
    "print(np.argsort(gs_attr_test_norm_sum))\n",
    "print(np.argsort(fa_attr_test_norm_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
