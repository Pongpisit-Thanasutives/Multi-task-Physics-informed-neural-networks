{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Distance loss\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 30000 samples\n",
      "Training with 15000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "include_N_res = True\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = 30000\n",
    "print(f\"Training with {N} samples\")\n",
    "\n",
    "# idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "idx = np.arange(N) # arange for faster training due to the easy data\n",
    "\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = 0.09817477042468103\n",
      "dt = 0.4\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X, T, Exact, acc_order=10)\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)[idx, :]\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.793009904582092e-11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "fd_derivatives = tmp[idx, :]\n",
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "\n",
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.25):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 1/layers[0]\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss+self.reg_intensity*torch.norm(F.relu(self.latest_weighted_features-self.th), p=0)\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train, fd_derivatives=None, fd_u_t=None, fd_weight=0.0, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        \n",
    "        fd_guidance = 0.0\n",
    "        if fd_weight>0.0 and fd_derivatives is not None and fd_u_t is not None:\n",
    "            # Traditional MSE Loss btw uf and u_train + the fd_guidance loss\n",
    "            row, col = fd_derivatives.shape\n",
    "            fd_guidance += F.mse_loss(X_selector[:row, 0:1], fd_derivatives[:, 0:1])\n",
    "            fd_guidance += fd_weight*(col-1)*F.mse_loss(X_selector[:row, 1:], fd_derivatives[:, 1:])\n",
    "            fd_guidance += fd_weight*F.mse_loss(y_selector[:row, :], fd_u_t)\n",
    "            \n",
    "        else: fd_guidance = self.network.uf\n",
    "            \n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "        \n",
    "        return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load(\"./saved_path_inverse_ks/pretrained5000samples_semisup_model_with_LayerNormDropout_without_physical_reg.pth\")\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained30000labeledsamples_trained0unlabeledsamples.pth\")\n",
    "# pretrained_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = not use_pretrained_weights\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "#     referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)\n",
    "\n",
    "# Highly inefficient code here\n",
    "# Why I cannnot load the network state_dict inside the if statement???\n",
    "tmp = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "tmp.load_state_dict(pretrained_state_dict); network_state_dict = tmp.network.state_dict()\n",
    "del tmp, pretrained_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25849437713623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0008335122838616371"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chk the performance both MSE and PDE relation loss\n",
    "semisup_model.eval()\n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "#     pretraining_optimizer =  MADGRAD(semisup_model.network.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    best_state_dict = None; curr_loss = 1000\n",
    "    semisup_model.network.train()\n",
    "    for i in range(300):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        \n",
    "        if l.item() < curr_loss:\n",
    "            curr_loss = l.item()\n",
    "            best_state_dict = semisup_model.state_dict()\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    \n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_u_train, fd_derivatives=fd_derivatives, \n",
    "                                            fd_u_t=fd_u_t, fd_weight=1.0, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def selector_closure():\n",
    "    global ders, dys\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer3.zero_grad()\n",
    "    unsup_loss = semisup_model.selector.loss(ders, dys)\n",
    "    if unsup_loss.requires_grad:\n",
    "        unsup_loss.backward(retain_graph=True)\n",
    "    return unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n",
      "Assigning the suggested_lr to optimizer1\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "suggested_lr = 1e-6\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "print(\"Assigning the suggested_lr to optimizer1\")\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 200; epochs2 = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 500; generator_training_limit = epochs1-100\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = MADGRAD(generator.parameters(), lr=1e-3, momentum=0.95)\n",
    "# sinkhorn distance loss\n",
    "distance_func = distance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:40:57<00:00, 12.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: 6.783370494842529\n",
      "Semi-supervised solver loss @Epoch 0:  0.27556559443473816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:32:01<00:00, 11.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: 6.67909574508667\n",
      "Semi-supervised solver loss @Epoch 100:  0.27618640661239624\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 500; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "# How long should I pretrain selector part of the model?\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1e6; best_generator_state_dict = None\n",
    "        o_tensor = X_u_train[:N, :] # or X_u_train ?\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            \n",
    "            # Do I need to scale o_tensor before feeding into the generator?\n",
    "            X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "            \n",
    "            # Choose the distance function that works well with the X_u_train structure\n",
    "            d_loss = distance_func(X_gen, o_tensor)\n",
    "#             d_loss = F.mse_loss(X_gen, o_tensor)\n",
    "            generator_loss = 0.05*d_loss-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            \n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        if best_generator_state_dict is not None: \n",
    "            generator.load_state_dict(best_generator_state_dict)\n",
    "            \n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "        if N_res<X_gen.shape[0]: X_gen = sampling_from_rows(X_gen, N_res)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "        \n",
    "# Re init\n",
    "with torch.no_grad(): semisup_model.network.load_state_dict(network_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[91.3306,  0.2741],\n",
       "        [91.3386,  0.2886],\n",
       "        [91.3307,  0.2743],\n",
       "        ...,\n",
       "        [91.3812,  0.3666],\n",
       "        [91.4546,  0.5035],\n",
       "        [99.4909, 24.8249]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage II: Train semisup_model.network\n",
    "# def closure():\n",
    "#     global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "#     if torch.is_grad_enabled():\n",
    "#         optimizer2.zero_grad()\n",
    "#     # With fd guidance\n",
    "#     mse_loss = semisup_model(X_u_train, fd_derivatives, fd_u_t, 2.0, False)[0]\n",
    "#     if mse_loss.requires_grad:\n",
    "#         mse_loss.backward(retain_graph=True)\n",
    "#     return mse_loss\n",
    "\n",
    "# # optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "# #                                      lr=1e-1, max_iter=100, \n",
    "# #                                      max_eval=int(100*1.25), history_size=150, \n",
    "# #                                      line_search_fn='strong_wolfe')\n",
    "\n",
    "# optimizer2 = MADGRAD(semisup_model.network.parameters(), lr=1e-6, momentum=0.95)\n",
    "\n",
    "# curr_loss = 10000\n",
    "# semisup_model.network.train()\n",
    "# semisup_model.selector.eval()\n",
    "# for i in range(epochs2):\n",
    "#     optimizer2.step(closure)\n",
    "#     if (i % 2) == 0:\n",
    "#         l = closure()\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_derivatives = to_numpy(fd_derivatives)\n",
    "fd_u_t = to_numpy(fd_u_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003179807390552014\n"
     ]
    }
   ],
   "source": [
    "n_test = 12000\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[:n_test, :]))\n",
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "print((((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item())\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = alpha*referenced_derivatives+(1-alpha)*fd_derivatives[:n_test, :]\n",
    "y_input = alpha*u_t+(1-alpha)*fd_u_t[:n_test, :]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.210340371976184"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(2000/0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use your big brain to choose, not looking at the BIC and AIC blindly.\n",
      "\n",
      "Alpha: 1\n",
      "-4.516689777374268u_xx*u_xxxx  -1.1222424507141113uf*u_x  -0.41278791427612305u_x*u_xxx  -0.31177589297294617uf*u_xxx  -0.11617869138717651uf*u_xx  -0.1023745909333229u_xxxx  0.04281386733055115u_x  \n",
      "\n",
      "Alpha: 2\n",
      "-1.0640901327133179uf*u_x  -0.41822391748428345u_xx  -0.3353460729122162u_xxxx  -0.1905515193939209uf*u_xxx  -0.04392003268003464uf*u_xx  \n",
      "\n",
      "Alpha: 3\n",
      "-1.0630955696105957uf*u_x  -0.42448076605796814u_xx  -0.3391903340816498u_xxxx  -0.18595367670059204uf*u_xxx  \n",
      "\n",
      "Alpha: 4\n",
      "-1.0550096035003662uf*u_x  -0.4673525094985962u_xx  -0.36447858810424805u_xxxx  \n",
      "\n",
      "Alpha: 16\n",
      "-1.0869503021240234uf*u_x  -0.15811364352703094u_xx  \n",
      "\n",
      "(Random) heuristically chosen alpha: 4\n"
     ]
    }
   ],
   "source": [
    "# My trick to make it work\n",
    "# threshold=0.02 -> alpha=1.0 to alpha=100.0, normalize=False\n",
    "# threshold=0.01 -> alpha=0.5 to alpha=500.0, normalize=True\n",
    "# Is this bullshit???\n",
    "\n",
    "# TODO: Try find min -np.log(MSE/maxMSE)/(Complexity - minComplexity)\n",
    "\n",
    "print(\"Use your big brain to choose, not looking at the BIC and AIC blindly.\\n\")\n",
    "\n",
    "scores = []\n",
    "const_range = (-1.5, 1.5)\n",
    "alphas = np.arange(99)+1\n",
    "for al in alphas:\n",
    "    # print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "    sparse_regressor = STRidge(threshold=0.02, alpha=al, max_iter=1000, normalize=False)\n",
    "    sparse_regressor.fit(X_input, y_input.ravel()); coef = sparse_regressor.coef_\n",
    "    n_params = 2*len(np.nonzero(coef)[0]) # this might not be a good choice ?\n",
    "    idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "    # print(coef)\n",
    "    # idxs = np.nonzero(coef)[0]\n",
    "\n",
    "    score = bic(sparse_regressor.predict(X_input).astype(float), \n",
    "                y_input.ravel().astype(float), n_params)\n",
    "\n",
    "    if score not in scores:\n",
    "        scores.append(score)\n",
    "        print('Alpha:', al)\n",
    "        \n",
    "        for idx in idxs[:]:\n",
    "            if not np.isclose(coef[idx], 0.0):\n",
    "#             if not np.isclose(coef[idx], 0.0) and coef[idx]<const_range[1] and coef[idx]>const_range[0]:\n",
    "                print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "scores = np.array(scores)\n",
    "tmp = np.array(list(set(scores)))\n",
    "print('(Random) heuristically chosen alpha:', alphas[np.argmin(abs(scores-np.median(tmp)))]); del tmp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0550096035003662uf*u_x  -0.4673525094985962u_xx  -0.36447858810424805u_xxxx  "
     ]
    }
   ],
   "source": [
    "sparse_regressor = STRidge(threshold=0.02, alpha=4, max_iter=1000, normalize=False)\n",
    "sparse_regressor.fit(X_input, y_input.ravel()); coef = sparse_regressor.coef_\n",
    "n_params = 2*len(np.nonzero(coef)[0])\n",
    "idxs = np.argsort(np.abs(coef))[::-1]\n",
    "for idx in idxs[:]:\n",
    "    if not np.isclose(coef[idx], 0.0) and coef[idx]<const_range[1] and coef[idx]>const_range[0]:\n",
    "        print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0915979146957397uf*u_x  -0.03191467374563217u_xxxx  -0.02631389908492565u_xx  "
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# n_test = 12000\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[:n_test, :]))\n",
    "# referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "# print((((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item())\n",
    "\n",
    "# alpha = 1\n",
    "# const_range = (-1.5, 1.5)\n",
    "\n",
    "# X_input = alpha*referenced_derivatives+(1-alpha)*fd_derivatives[:n_test, :]\n",
    "# y_input = alpha*u_t+(1-alpha)*fd_u_t[:n_test, :]\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# X_input = poly.fit_transform(X_input)\n",
    "# poly_feature_names = poly.get_feature_names(feature_names)\n",
    "# for i, f in enumerate(poly_feature_names):\n",
    "#     poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "\n",
    "# from sparsereg.model import STRidge\n",
    "# # print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "# coef = STRidge(threshold=0.02, alpha=20, max_iter=1000, normalize=False).fit(X_input, y_input.ravel()).coef_\n",
    "# idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "# # print(coef)\n",
    "# # idxs = np.nonzero(coef)[0]\n",
    "\n",
    "# for idx in idxs[:]:\n",
    "#     if not np.isclose(coef[idx], 0.0):\n",
    "#         print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Test MSE: 4.2e-04\n"
     ]
    }
   ],
   "source": [
    "# Test on 2048 samples => 2.0e-6\n",
    "n_test_samples = 2048\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star[:n_test_samples, :])).detach(), u_star[:n_test_samples, :]).item()\n",
    "string_test_performance = scientific2string(test_performance)\n",
    "print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ders, dys = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "ders = (ders-semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)\n",
    "\n",
    "# optimizer3 = LBFGSNew(semisup_model.selector.parameters(),\n",
    "#                       lr=learning_rate2, max_iter=100, max_eval=int(100*1.25),\n",
    "#                       history_size=150, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# # Stage II: Train semisup_model.selector\n",
    "# semisup_model.network.eval()\n",
    "# semisup_model.selector.train()\n",
    "# for i in range(20):\n",
    "#     optimizer3.step(selector_closure)\n",
    "#     if (i % 5) == 0:\n",
    "#         l = selector_closure()\n",
    "#         print(\"Epoch {}: \".format(i), l.item())\n",
    "#         print(np.argsort((semisup_model.selector.latest_weighted_features).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "u_xx 0.26986682\n",
      "u_xxx 0.19863041\n",
      "u_x 0.19213098\n",
      "u_xxxx 0.17010696\n",
      "uf 0.16926484\n"
     ]
    }
   ],
   "source": [
    "feature_importance = (semisup_model.selector.latest_weighted_features).detach().numpy()\n",
    "feature_importance = feature_importance\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Training MSE: 0.0005716726526455858\n",
      "('uf', 0.3011065812463599)\n",
      "('u_xx', 0.24403028538147933)\n",
      "('u_x', 0.18811881188118812)\n",
      "('u_xxxx', 0.168899242865463)\n",
      "('u_xxx', 0.0978450786255096)\n"
     ]
    }
   ],
   "source": [
    "light = lightgbm.LGBMRegressor(n_estimators=300, learning_rate=0.1, reg_alpha=10)\n",
    "light = SklearnModel(model=light, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "light_feature_importance = light.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost\n",
      "Done training\n",
      "Training MSE: 0.00037053964\n",
      "('u_xx', 0.37224612472011426)\n",
      "('uf', 0.26302061072783045)\n",
      "('u_x', 0.18129133093263203)\n",
      "('u_xxx', 0.09492312583900524)\n",
      "('u_xxxx', 0.08851880778041797)\n",
      "\n",
      "Catboost\n",
      "Done training\n",
      "Training MSE: 0.0001787834072033223\n",
      "('uf', 0.30847894374375673)\n",
      "('u_x', 0.2762164853296332)\n",
      "('u_xx', 0.24057343531444456)\n",
      "('u_xxxx', 0.09747290567721091)\n",
      "('u_xxx', 0.07725822993495447)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Ensemble model\")\n",
    "# sklearn_model = SklearnModel(model=ensemble, X_train=X_np_full, y_train=y_np_full, feature_names=feature_names)\n",
    "# print('Training GBM algos...')\n",
    "# print()\n",
    "\n",
    "xg = xgboost.XGBRegressor(reg_alpha=10)\n",
    "cat = catboost.CatBoostRegressor(iterations=None, depth=4, learning_rate=0.1, verbose=0, reg_lambda=10)\n",
    "\n",
    "print(\"Xgboost\")\n",
    "xg = SklearnModel(model=xg, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "xg_feature_importance = xg.feature_importance()\n",
    "print()\n",
    "\n",
    "print(\"Catboost\")\n",
    "# Showing the best performance\n",
    "cat = SklearnModel(model=cat, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "cat_feature_importance = cat.feature_importance()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uf 0.30847894374375673\n",
      "u_x 0.2762164853296332\n",
      "u_xx 0.24057343531444456\n",
      "u_xxx 0.07725822993495447\n",
      "u_xxxx 0.09747290567721091\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_feature_importances = {}\n",
    "for f in feature_names:\n",
    "    avg_feature_importances[f] = (xg_feature_importance[f]+cat_feature_importance[f]+light_feature_importance[f])/3\n",
    "    print(f, avg_feature_importances[f])\n",
    "\n",
    "tmp = sorted([(v, f) for (f, v) in avg_feature_importances.items()])[::-1]\n",
    "xxx = [f for (f, v) in tmp]\n",
    "yyy = [v for (f, v) in tmp]\n",
    "\n",
    "plt.plot(yyy, xxx, 'bo')\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.xlabel(\"Partial deriavative features (possible candidates)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
