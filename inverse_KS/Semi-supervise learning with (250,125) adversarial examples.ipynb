{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10000 samples\n",
      "Training with 5000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "include_N_res = True\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = 10000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = 0.09817477042468103\n",
      "dt = 0.4\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X, T, Exact, acc_order=2)\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)[idx, :]\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "fd_derivatives = tmp[idx, :]; del tmp;\n",
    "# fd_derivatives, fd_u_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012268107093404979"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.25):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 0.5\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss+self.reg_intensity*torch.norm(F.relu(self.latest_weighted_features-self.th), p=0)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n"
     ]
    }
   ],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None,\n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "pretrained_state_dict = torch.load(\"./saved_path_inverse_ks/pretrained5000samples_semisup_model_with_LayerNormDropout_without_physical_reg.pth\")\n",
    "# pretrained_state_dict = None\n",
    "network_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = True\n",
    "\n",
    "# semisup_model = SemiSupModel(network=Network(\n",
    "#                                     model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "#                                                    activation_function=nn.Tanh,\n",
    "#                                                    bn=nn.LayerNorm, dropout=None),\n",
    "#                                     index2features=feature_names),\n",
    "#                             selector=SeclectorNetwork(X_train_dim=len(feature_names), bn=nn.LayerNorm),\n",
    "#                             normalize_derivative_features=True,\n",
    "#                             mini=None,\n",
    "#                             maxi=None)\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=torch.sigmoid, bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "    network_state_dict = semisup_model.network.state_dict()\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.104759216308594"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026049395091831684"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining\n",
      "Epoch 0:  1.0554888248443604\n",
      "Test MSE: 1.0e+00\n",
      "Epoch 10:  1.0554888248443604\n",
      "Test MSE: 1.2e+01\n",
      "Epoch 20:  1.0554888248443604\n",
      "Test MSE: 3.2e+00\n",
      "Epoch 30:  1.0554888248443604\n",
      "Test MSE: 1.8e+00\n",
      "Epoch 40:  1.0554888248443604\n",
      "Test MSE: 1.3e+00\n",
      "Epoch 50:  1.0554888248443604\n",
      "Test MSE: 1.1e+00\n",
      "Epoch 60:  1.0538270473480225\n",
      "Test MSE: 1.1e+00\n",
      "Epoch 70:  1.0095279216766357\n",
      "Test MSE: 1.0e+00\n",
      "Epoch 80:  0.976584792137146\n",
      "Test MSE: 1.0e+00\n",
      "Epoch 90:  0.9530888795852661\n",
      "Test MSE: 9.8e-01\n",
      "Epoch 100:  0.9342461824417114\n",
      "Test MSE: 9.6e-01\n",
      "Epoch 110:  0.9181293845176697\n",
      "Test MSE: 9.5e-01\n",
      "Epoch 120:  0.903810441493988\n",
      "Test MSE: 9.3e-01\n",
      "Epoch 130:  0.8909321427345276\n",
      "Test MSE: 9.2e-01\n",
      "Epoch 140:  0.8792228698730469\n",
      "Test MSE: 9.1e-01\n",
      "Epoch 150:  0.8684861063957214\n",
      "Test MSE: 8.9e-01\n",
      "Epoch 160:  0.858507513999939\n",
      "Test MSE: 8.8e-01\n",
      "Epoch 170:  0.8490742444992065\n",
      "Test MSE: 8.7e-01\n",
      "Epoch 180:  0.8399915099143982\n",
      "Test MSE: 8.7e-01\n",
      "Epoch 190:  0.8310840129852295\n",
      "Test MSE: 8.6e-01\n",
      "Epoch 200:  0.8221849799156189\n",
      "Test MSE: 8.5e-01\n",
      "Epoch 210:  0.8131492137908936\n",
      "Test MSE: 8.4e-01\n",
      "Epoch 220:  0.8039333820343018\n",
      "Test MSE: 8.3e-01\n",
      "Epoch 230:  0.7946875691413879\n",
      "Test MSE: 8.2e-01\n",
      "Epoch 240:  0.7856684327125549\n",
      "Test MSE: 8.1e-01\n",
      "Epoch 250:  0.7770224809646606\n",
      "Test MSE: 8.0e-01\n",
      "Epoch 260:  0.7687472105026245\n",
      "Test MSE: 7.9e-01\n",
      "Epoch 270:  0.760792076587677\n",
      "Test MSE: 7.8e-01\n",
      "Epoch 280:  0.7531136870384216\n",
      "Test MSE: 7.8e-01\n",
      "Epoch 290:  0.7456818222999573\n",
      "Test MSE: 7.7e-01\n",
      "Epoch 300:  0.738474428653717\n",
      "Test MSE: 7.6e-01\n",
      "Epoch 310:  0.7314718961715698\n",
      "Test MSE: 7.5e-01\n",
      "Epoch 320:  0.7246597409248352\n",
      "Test MSE: 7.5e-01\n",
      "Epoch 330:  0.7180277109146118\n",
      "Test MSE: 7.4e-01\n",
      "Epoch 340:  0.7115685939788818\n",
      "Test MSE: 7.3e-01\n",
      "Epoch 350:  0.7052760720252991\n",
      "Test MSE: 7.3e-01\n",
      "Epoch 360:  0.6991417407989502\n",
      "Test MSE: 7.2e-01\n",
      "Epoch 370:  0.693154513835907\n",
      "Test MSE: 7.2e-01\n",
      "Epoch 380:  0.6873012781143188\n",
      "Test MSE: 7.1e-01\n",
      "Epoch 390:  0.6815659403800964\n",
      "Test MSE: 7.0e-01\n",
      "Epoch 400:  0.6759307384490967\n",
      "Test MSE: 7.0e-01\n",
      "Epoch 410:  0.6703773140907288\n",
      "Test MSE: 6.9e-01\n",
      "Epoch 420:  0.6648859977722168\n",
      "Test MSE: 6.9e-01\n",
      "Epoch 430:  0.6594367027282715\n",
      "Test MSE: 6.8e-01\n",
      "Epoch 440:  0.6540110111236572\n",
      "Test MSE: 6.8e-01\n",
      "Epoch 450:  0.6485920548439026\n",
      "Test MSE: 6.7e-01\n",
      "Epoch 460:  0.6431671380996704\n",
      "Test MSE: 6.7e-01\n",
      "Epoch 470:  0.6377283930778503\n",
      "Test MSE: 6.6e-01\n",
      "Epoch 480:  0.6322727799415588\n",
      "Test MSE: 6.6e-01\n",
      "Epoch 490:  0.6268022656440735\n",
      "Test MSE: 6.5e-01\n",
      "Epoch 500:  0.6213231682777405\n",
      "Test MSE: 6.5e-01\n",
      "Epoch 510:  0.6158446073532104\n",
      "Test MSE: 6.4e-01\n",
      "Epoch 520:  0.6103780269622803\n",
      "Test MSE: 6.4e-01\n",
      "Epoch 530:  0.6049367785453796\n",
      "Test MSE: 6.3e-01\n",
      "Epoch 540:  0.6014586091041565\n",
      "Test MSE: 6.3e-01\n",
      "Epoch 550:  0.6014586091041565\n",
      "Test MSE: 7.1e-01\n",
      "Epoch 560:  0.6014586091041565\n",
      "Test MSE: 8.8e-01\n",
      "Epoch 570:  0.6014586091041565\n",
      "Test MSE: 7.0e-01\n",
      "Epoch 580:  0.6014586091041565\n",
      "Test MSE: 6.3e-01\n",
      "Epoch 590:  0.58730149269104\n",
      "Test MSE: 6.1e-01\n",
      "Epoch 600:  0.5793088674545288\n",
      "Test MSE: 6.0e-01\n",
      "Epoch 610:  0.572165846824646\n",
      "Test MSE: 6.0e-01\n",
      "Epoch 620:  0.5654789805412292\n",
      "Test MSE: 5.9e-01\n",
      "Epoch 630:  0.5598713159561157\n",
      "Test MSE: 5.9e-01\n",
      "Epoch 640:  0.5544272065162659\n",
      "Test MSE: 5.8e-01\n",
      "Epoch 650:  0.5491952896118164\n",
      "Test MSE: 5.8e-01\n",
      "Epoch 660:  0.5440175533294678\n",
      "Test MSE: 5.7e-01\n",
      "Epoch 670:  0.5389125347137451\n",
      "Test MSE: 5.7e-01\n",
      "Epoch 680:  0.5338634252548218\n",
      "Test MSE: 5.6e-01\n",
      "Epoch 690:  0.5288681387901306\n",
      "Test MSE: 5.6e-01\n",
      "Epoch 700:  0.5239348411560059\n",
      "Test MSE: 5.5e-01\n",
      "Epoch 710:  0.5190790295600891\n",
      "Test MSE: 5.5e-01\n",
      "Epoch 720:  0.5143213868141174\n",
      "Test MSE: 5.4e-01\n",
      "Epoch 730:  0.5096839070320129\n",
      "Test MSE: 5.4e-01\n",
      "Epoch 740:  0.5051854252815247\n",
      "Test MSE: 5.3e-01\n",
      "Epoch 750:  0.5008378624916077\n",
      "Test MSE: 5.3e-01\n",
      "Epoch 760:  0.49664515256881714\n",
      "Test MSE: 5.2e-01\n",
      "Epoch 770:  0.49260273575782776\n",
      "Test MSE: 5.2e-01\n",
      "Epoch 780:  0.48870137333869934\n",
      "Test MSE: 5.2e-01\n",
      "Epoch 790:  0.4849291443824768\n",
      "Test MSE: 5.1e-01\n",
      "Epoch 800:  0.4812737703323364\n",
      "Test MSE: 5.1e-01\n",
      "Epoch 810:  0.47772303223609924\n",
      "Test MSE: 5.1e-01\n",
      "Epoch 820:  0.4742649793624878\n",
      "Test MSE: 5.0e-01\n",
      "Epoch 830:  0.47088828682899475\n",
      "Test MSE: 5.0e-01\n",
      "Epoch 840:  0.4675818979740143\n",
      "Test MSE: 5.0e-01\n",
      "Epoch 850:  0.4643348753452301\n",
      "Test MSE: 4.9e-01\n",
      "Epoch 860:  0.4611371159553528\n",
      "Test MSE: 4.9e-01\n",
      "Epoch 870:  0.4579789638519287\n",
      "Test MSE: 4.9e-01\n",
      "Epoch 880:  0.45485204458236694\n",
      "Test MSE: 4.8e-01\n",
      "Epoch 890:  0.4517492651939392\n",
      "Test MSE: 4.8e-01\n",
      "Epoch 900:  0.44867265224456787\n",
      "Test MSE: 4.8e-01\n",
      "Epoch 910:  0.44719067215919495\n",
      "Test MSE: 4.8e-01\n",
      "Epoch 920:  0.44719067215919495\n",
      "Test MSE: 7.1e-01\n",
      "Epoch 930:  0.44719067215919495\n",
      "Test MSE: 6.4e-01\n",
      "Epoch 940:  0.44719067215919495\n",
      "Test MSE: 4.9e-01\n",
      "Epoch 950:  0.4458857476711273\n",
      "Test MSE: 4.8e-01\n",
      "Epoch 960:  0.439616858959198\n",
      "Test MSE: 4.7e-01\n",
      "Epoch 970:  0.435260534286499\n",
      "Test MSE: 4.6e-01\n",
      "Epoch 980:  0.4316255748271942\n",
      "Test MSE: 4.6e-01\n",
      "Epoch 990:  0.42827147245407104\n",
      "Test MSE: 4.6e-01\n",
      "Epoch 1000:  0.425447016954422\n",
      "Test MSE: 4.5e-01\n",
      "Epoch 1010:  0.4225645959377289\n",
      "Test MSE: 4.5e-01\n",
      "Epoch 1020:  0.41995805501937866\n",
      "Test MSE: 4.5e-01\n",
      "Epoch 1030:  0.41744503378868103\n",
      "Test MSE: 4.4e-01\n",
      "Epoch 1040:  0.4150466322898865\n",
      "Test MSE: 4.4e-01\n",
      "Epoch 1050:  0.4127388596534729\n",
      "Test MSE: 4.4e-01\n",
      "Epoch 1060:  0.4105023443698883\n",
      "Test MSE: 4.4e-01\n",
      "Epoch 1070:  0.40832075476646423\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1080:  0.4061793088912964\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1090:  0.40406590700149536\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1100:  0.401970237493515\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1110:  0.39988449215888977\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1120:  0.3978026807308197\n",
      "Test MSE: 4.2e-01\n",
      "Epoch 1130:  0.3957206606864929\n",
      "Test MSE: 4.2e-01\n",
      "Epoch 1140:  0.3936355412006378\n",
      "Test MSE: 4.2e-01\n",
      "Epoch 1150:  0.39154544472694397\n",
      "Test MSE: 4.2e-01\n",
      "Epoch 1160:  0.38944947719573975\n",
      "Test MSE: 4.1e-01\n",
      "Epoch 1170:  0.3876088857650757\n",
      "Test MSE: 4.1e-01\n",
      "Epoch 1180:  0.3876088857650757\n",
      "Test MSE: 1.2e+00\n",
      "Epoch 1190:  0.3876088857650757\n",
      "Test MSE: 7.3e-01\n",
      "Epoch 1200:  0.3876088857650757\n",
      "Test MSE: 5.4e-01\n",
      "Epoch 1210:  0.3876088857650757\n",
      "Test MSE: 4.3e-01\n",
      "Epoch 1220:  0.3876088857650757\n",
      "Test MSE: 4.2e-01\n",
      "Epoch 1230:  0.3876088857650757\n",
      "Test MSE: 4.1e-01\n",
      "Epoch 1240:  0.3845004737377167\n",
      "Test MSE: 4.1e-01\n",
      "Epoch 1250:  0.3805390000343323\n",
      "Test MSE: 4.0e-01\n",
      "Epoch 1260:  0.3778981864452362\n",
      "Test MSE: 4.0e-01\n",
      "Epoch 1270:  0.3752635717391968\n",
      "Test MSE: 4.0e-01\n",
      "Epoch 1280:  0.3727935552597046\n",
      "Test MSE: 4.0e-01\n",
      "Epoch 1290:  0.370434433221817\n",
      "Test MSE: 3.9e-01\n",
      "Epoch 1300:  0.3681429922580719\n",
      "Test MSE: 3.9e-01\n",
      "Epoch 1310:  0.3658965826034546\n",
      "Test MSE: 3.9e-01\n",
      "Epoch 1320:  0.3636782765388489\n",
      "Test MSE: 3.9e-01\n",
      "Epoch 1330:  0.36148694157600403\n",
      "Test MSE: 3.8e-01\n",
      "Epoch 1340:  0.35931840538978577\n",
      "Test MSE: 3.8e-01\n",
      "Epoch 1350:  0.357171893119812\n",
      "Test MSE: 3.8e-01\n",
      "Epoch 1360:  0.3550470769405365\n",
      "Test MSE: 3.8e-01\n",
      "Epoch 1370:  0.352944016456604\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1380:  0.3508632183074951\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1390:  0.3488050401210785\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1400:  0.346769779920578\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1410:  0.3447572886943817\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1420:  0.34276720881462097\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1430:  0.3407990038394928\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1440:  0.3388519287109375\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1450:  0.33692675828933716\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1460:  0.33576706051826477\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1470:  0.33576706051826477\n",
      "Test MSE: 1.5e+00\n",
      "Epoch 1480:  0.33576706051826477\n",
      "Test MSE: 6.5e-01\n",
      "Epoch 1490:  0.33576706051826477\n",
      "Test MSE: 3.8e-01\n",
      "Epoch 1500:  0.33576706051826477\n",
      "Test MSE: 3.9e-01\n",
      "Epoch 1510:  0.33576706051826477\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1520:  0.3348234295845032\n",
      "Test MSE: 3.5e-01\n",
      "Epoch 1530:  0.3317030668258667\n",
      "Test MSE: 3.5e-01\n",
      "Epoch 1540:  0.32912319898605347\n",
      "Test MSE: 3.5e-01\n",
      "Epoch 1550:  0.3265320956707001\n",
      "Test MSE: 3.4e-01\n",
      "Epoch 1560:  0.3242747485637665\n",
      "Test MSE: 3.4e-01\n",
      "Epoch 1570:  0.3221651017665863\n",
      "Test MSE: 3.4e-01\n",
      "Epoch 1580:  0.32017213106155396\n",
      "Test MSE: 3.4e-01\n",
      "Epoch 1590:  0.3182259202003479\n",
      "Test MSE: 3.4e-01\n",
      "Epoch 1600:  0.3163194954395294\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 1610:  0.3144465684890747\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 1620:  0.31260260939598083\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 1630:  0.31078246235847473\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 1640:  0.308982789516449\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 1650:  0.30720004439353943\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1660:  0.3054313361644745\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1670:  0.30367401242256165\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1680:  0.30192577838897705\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1690:  0.30018436908721924\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1700:  0.29844799637794495\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1710:  0.2967151403427124\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 1720:  0.2949846088886261\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 1730:  0.29332655668258667\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 1740:  0.2932020425796509\n",
      "Test MSE: 4.4e-01\n",
      "Epoch 1750:  0.2932020425796509\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1760:  0.2932020425796509\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 1770:  0.2932020425796509\n",
      "Test MSE: 3.5e-01\n",
      "Epoch 1780:  0.2932020425796509\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1790:  0.2932020425796509\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 1800:  0.2893492877483368\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 1810:  0.2867085933685303\n",
      "Test MSE: 3.0e-01\n",
      "Epoch 1820:  0.284304678440094\n",
      "Test MSE: 3.0e-01\n",
      "Epoch 1830:  0.28237584233283997\n",
      "Test MSE: 3.0e-01\n",
      "Epoch 1840:  0.2803758382797241\n",
      "Test MSE: 3.0e-01\n",
      "Epoch 1850:  0.2784944474697113\n",
      "Test MSE: 3.0e-01\n",
      "Epoch 1860:  0.27665549516677856\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1870:  0.2748448848724365\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1880:  0.2730599343776703\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1890:  0.27129632234573364\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1900:  0.2695504426956177\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1910:  0.2678205668926239\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1920:  0.266104131937027\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 1930:  0.26439937949180603\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 1940:  0.2627042829990387\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 1950:  0.26103606820106506\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 1960:  0.2605593800544739\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 1970:  0.2605593800544739\n",
      "Test MSE: 3.2e-01\n",
      "Epoch 1980:  0.2605593800544739\n",
      "Test MSE: 3.6e-01\n",
      "Epoch 1990:  0.2605593800544739\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 2000:  0.2605593800544739\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 2010:  0.25751224160194397\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 2020:  0.2551657259464264\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2030:  0.2530879080295563\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2040:  0.25075003504753113\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2050:  0.24879609048366547\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2060:  0.2469210922718048\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2070:  0.24506708979606628\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2080:  0.2432582974433899\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2090:  0.24145442247390747\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2100:  0.23965446650981903\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2110:  0.23786038160324097\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2120:  0.23609748482704163\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2130:  0.23556388914585114\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2140:  0.23556388914585114\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 2150:  0.23556388914585114\n",
      "Test MSE: 3.1e-01\n",
      "Epoch 2160:  0.23556388914585114\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2170:  0.23556388914585114\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2180:  0.23156552016735077\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2190:  0.2292715311050415\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2200:  0.2269744575023651\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2210:  0.22482304275035858\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2220:  0.22276262938976288\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2230:  0.22084446251392365\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2240:  0.21897979080677032\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2250:  0.2171654999256134\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2260:  0.21544602513313293\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2270:  0.21517431735992432\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2280:  0.21517431735992432\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 2290:  0.21517431735992432\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2300:  0.21517431735992432\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2310:  0.21351279318332672\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2320:  0.20998495817184448\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2330:  0.20671674609184265\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2340:  0.20524951815605164\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2350:  0.20524951815605164\n",
      "Test MSE: 2.8e-01\n",
      "Epoch 2360:  0.20524951815605164\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2370:  0.20262303948402405\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2380:  0.20026594400405884\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2390:  0.20026594400405884\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 2400:  0.19727395474910736\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2410:  0.19543442130088806\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2420:  0.19420190155506134\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2430:  0.19420190155506134\n",
      "Test MSE: 2.5e-01\n",
      "Epoch 2440:  0.19388943910598755\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2450:  0.19326844811439514\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2460:  0.1903858631849289\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2470:  0.1903858631849289\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 2480:  0.1903858631849289\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2490:  0.1903858631849289\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2500:  0.18695196509361267\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2510:  0.18370671570301056\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2520:  0.18324579298496246\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2530:  0.18324579298496246\n",
      "Test MSE: 2.6e-01\n",
      "Epoch 2540:  0.18324579298496246\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2550:  0.18324579298496246\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2560:  0.17892779409885406\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2570:  0.17880982160568237\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2580:  0.17880982160568237\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2590:  0.17767222225666046\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2600:  0.1733333319425583\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2610:  0.1733333319425583\n",
      "Test MSE: 2.7e-01\n",
      "Epoch 2620:  0.1733333319425583\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2630:  0.1733333319425583\n",
      "Test MSE: 2.1e-01\n",
      "Epoch 2640:  0.17015033960342407\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2650:  0.16787183284759521\n",
      "Test MSE: 1.9e-01\n",
      "Epoch 2660:  0.1649651676416397\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2670:  0.1649651676416397\n",
      "Test MSE: 2.9e-01\n",
      "Epoch 2680:  0.1649651676416397\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2690:  0.1649651676416397\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2700:  0.1649651676416397\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2710:  0.16139215230941772\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2720:  0.15871885418891907\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2730:  0.158070906996727\n",
      "Test MSE: 2.2e-01\n",
      "Epoch 2740:  0.158070906996727\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2750:  0.158070906996727\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2760:  0.15528427064418793\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2770:  0.15244409441947937\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2780:  0.15180639922618866\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2790:  0.15180639922618866\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 2800:  0.15180639922618866\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 2810:  0.15180639922618866\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2820:  0.1486617922782898\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2830:  0.14766183495521545\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2840:  0.14566588401794434\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2850:  0.14294999837875366\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2860:  0.14244474470615387\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2870:  0.14244474470615387\n",
      "Test MSE: 3.7e-01\n",
      "Epoch 2880:  0.14244474470615387\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2890:  0.14244474470615387\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 2900:  0.14244474470615387\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2910:  0.13764165341854095\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 2920:  0.13633841276168823\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 2930:  0.13633841276168823\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 2940:  0.13633841276168823\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 2950:  0.13633841276168823\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2960:  0.13322965800762177\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2970:  0.13322965800762177\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2980:  0.13322965800762177\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 2990:  0.1279924362897873\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3000:  0.1275986135005951\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 3010:  0.1275986135005951\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 3020:  0.1275986135005951\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3030:  0.1275986135005951\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3040:  0.12614411115646362\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3050:  0.12226355075836182\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3060:  0.12183873355388641\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 3070:  0.12122076749801636\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3080:  0.12012609839439392\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3090:  0.11888252943754196\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3100:  0.11800428479909897\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3110:  0.11800428479909897\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 3120:  0.11800428479909897\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3130:  0.11800428479909897\n",
      "Test MSE: 1.6e-01\n",
      "Epoch 3140:  0.11551274359226227\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3150:  0.11134038120508194\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3160:  0.11134038120508194\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3170:  0.11134038120508194\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3180:  0.11134038120508194\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 3190:  0.11134038120508194\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3200:  0.1080528199672699\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3210:  0.10654313862323761\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3220:  0.10582983493804932\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3230:  0.10582983493804932\n",
      "Test MSE: 3.3e-01\n",
      "Epoch 3240:  0.10582983493804932\n",
      "Test MSE: 1.8e-01\n",
      "Epoch 3250:  0.10582983493804932\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3260:  0.10582983493804932\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3270:  0.10518226027488708\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3280:  0.1030905544757843\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3290:  0.10074581205844879\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3300:  0.10074581205844879\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3310:  0.10074581205844879\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3320:  0.09814561158418655\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3330:  0.09645747393369675\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3340:  0.09630029648542404\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3350:  0.09630029648542404\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3360:  0.09630029648542404\n",
      "Test MSE: 2.4e-01\n",
      "Epoch 3370:  0.09630029648542404\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3380:  0.09630029648542404\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3390:  0.09630029648542404\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3400:  0.09407598525285721\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3410:  0.09219831228256226\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3420:  0.09219831228256226\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3430:  0.09219831228256226\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3440:  0.09219831228256226\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3450:  0.09034287184476852\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3460:  0.08788573741912842\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3470:  0.08788573741912842\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3480:  0.08788573741912842\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3490:  0.08788573741912842\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3500:  0.08788573741912842\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3510:  0.08788573741912842\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3520:  0.08788573741912842\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3530:  0.0862661674618721\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3540:  0.08421079069375992\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3550:  0.08266168087720871\n",
      "Test MSE: 9.8e-02\n",
      "Epoch 3560:  0.08266168087720871\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3570:  0.08266168087720871\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3580:  0.08266168087720871\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3590:  0.0812544897198677\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3600:  0.08073023706674576\n",
      "Test MSE: 9.6e-02\n",
      "Epoch 3610:  0.07912169396877289\n",
      "Test MSE: 9.5e-02\n",
      "Epoch 3620:  0.07776406407356262\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 3630:  0.07750532776117325\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 3640:  0.07750532776117325\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 3650:  0.07750532776117325\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3660:  0.07750532776117325\n",
      "Test MSE: 2.0e-01\n",
      "Epoch 3670:  0.07750532776117325\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3680:  0.07750532776117325\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3690:  0.07750532776117325\n",
      "Test MSE: 9.2e-02\n",
      "Epoch 3700:  0.0745784193277359\n",
      "Test MSE: 8.9e-02\n",
      "Epoch 3710:  0.0745784193277359\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3720:  0.0745784193277359\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3730:  0.0745784193277359\n",
      "Test MSE: 9.4e-02\n",
      "Epoch 3740:  0.0745784193277359\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 3750:  0.07432419061660767\n",
      "Test MSE: 8.9e-02\n",
      "Epoch 3760:  0.07432419061660767\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3770:  0.07432419061660767\n",
      "Test MSE: 1.7e-01\n",
      "Epoch 3780:  0.07432419061660767\n",
      "Test MSE: 9.8e-02\n",
      "Epoch 3790:  0.07432419061660767\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 3800:  0.07432419061660767\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3810:  0.07026468217372894\n",
      "Test MSE: 8.4e-02\n",
      "Epoch 3820:  0.06882377713918686\n",
      "Test MSE: 8.7e-02\n",
      "Epoch 3830:  0.06882377713918686\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3840:  0.06882377713918686\n",
      "Test MSE: 9.0e-02\n",
      "Epoch 3850:  0.06882377713918686\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3860:  0.06878893077373505\n",
      "Test MSE: 9.8e-02\n",
      "Epoch 3870:  0.06681746989488602\n",
      "Test MSE: 8.5e-02\n",
      "Epoch 3880:  0.06681746989488602\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 3890:  0.06670121103525162\n",
      "Test MSE: 9.2e-02\n",
      "Epoch 3900:  0.06566484272480011\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 3910:  0.06483656913042068\n",
      "Test MSE: 8.6e-02\n",
      "Epoch 3920:  0.06483656913042068\n",
      "Test MSE: 1.5e-01\n",
      "Epoch 3930:  0.06483656913042068\n",
      "Test MSE: 1.0e-01\n",
      "Epoch 3940:  0.06483656913042068\n",
      "Test MSE: 8.5e-02\n",
      "Epoch 3950:  0.06323495507240295\n",
      "Test MSE: 7.6e-02\n",
      "Epoch 3960:  0.062458183616399765\n",
      "Test MSE: 7.7e-02\n",
      "Epoch 3970:  0.062458183616399765\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 3980:  0.062458183616399765\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 3990:  0.062458183616399765\n",
      "Test MSE: 8.1e-02\n",
      "Epoch 4000:  0.062458183616399765\n",
      "Test MSE: 7.7e-02\n",
      "Epoch 4010:  0.060744207352399826\n",
      "Test MSE: 7.6e-02\n",
      "Epoch 4020:  0.0596185065805912\n",
      "Test MSE: 7.3e-02\n",
      "Epoch 4030:  0.05900063365697861\n",
      "Test MSE: 7.2e-02\n",
      "Epoch 4040:  0.058598827570676804\n",
      "Test MSE: 7.1e-02\n",
      "Epoch 4050:  0.058598827570676804\n",
      "Test MSE: 2.3e-01\n",
      "Epoch 4060:  0.058598827570676804\n",
      "Test MSE: 1.4e-01\n",
      "Epoch 4070:  0.058598827570676804\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 4080:  0.058598827570676804\n",
      "Test MSE: 8.1e-02\n",
      "Epoch 4090:  0.058598827570676804\n",
      "Test MSE: 7.2e-02\n",
      "Epoch 4100:  0.05823172628879547\n",
      "Test MSE: 7.1e-02\n",
      "Epoch 4110:  0.05823172628879547\n",
      "Test MSE: 7.5e-02\n",
      "Epoch 4120:  0.05823172628879547\n",
      "Test MSE: 7.9e-02\n",
      "Epoch 4130:  0.056125152856111526\n",
      "Test MSE: 7.6e-02\n",
      "Epoch 4140:  0.05499035492539406\n",
      "Test MSE: 6.7e-02\n",
      "Epoch 4150:  0.05499035492539406\n",
      "Test MSE: 7.4e-02\n",
      "Epoch 4160:  0.05499035492539406\n",
      "Test MSE: 7.6e-02\n",
      "Epoch 4170:  0.05499035492539406\n",
      "Test MSE: 7.5e-02\n",
      "Epoch 4180:  0.05499035492539406\n",
      "Test MSE: 7.3e-02\n",
      "Epoch 4190:  0.053740277886390686\n",
      "Test MSE: 6.6e-02\n",
      "Epoch 4200:  0.052823394536972046\n",
      "Test MSE: 7.0e-02\n",
      "Epoch 4210:  0.052823394536972046\n",
      "Test MSE: 8.3e-02\n",
      "Epoch 4220:  0.052823394536972046\n",
      "Test MSE: 7.5e-02\n",
      "Epoch 4230:  0.052823394536972046\n",
      "Test MSE: 1.3e-01\n",
      "Epoch 4240:  0.052823394536972046\n",
      "Test MSE: 7.8e-02\n",
      "Epoch 4250:  0.052823394536972046\n",
      "Test MSE: 6.6e-02\n",
      "Epoch 4260:  0.052823394536972046\n",
      "Test MSE: 6.5e-02\n",
      "Epoch 4270:  0.05192665383219719\n",
      "Test MSE: 6.5e-02\n",
      "Epoch 4280:  0.05192665383219719\n",
      "Test MSE: 9.7e-02\n",
      "Epoch 4290:  0.05094435065984726\n",
      "Test MSE: 7.2e-02\n",
      "Epoch 4300:  0.05094435065984726\n",
      "Test MSE: 9.4e-02\n",
      "Epoch 4310:  0.05094435065984726\n",
      "Test MSE: 6.6e-02\n",
      "Epoch 4320:  0.05061113461852074\n",
      "Test MSE: 6.3e-02\n",
      "Epoch 4330:  0.05061113461852074\n",
      "Test MSE: 8.3e-02\n",
      "Epoch 4340:  0.05061113461852074\n",
      "Test MSE: 7.4e-02\n",
      "Epoch 4350:  0.04904872179031372\n",
      "Test MSE: 6.4e-02\n",
      "Epoch 4360:  0.04904872179031372\n",
      "Test MSE: 6.3e-02\n",
      "Epoch 4370:  0.04749417304992676\n",
      "Test MSE: 6.1e-02\n",
      "Epoch 4380:  0.04749417304992676\n",
      "Test MSE: 9.3e-02\n",
      "Epoch 4390:  0.04749417304992676\n",
      "Test MSE: 8.3e-02\n",
      "Epoch 4400:  0.04749417304992676\n",
      "Test MSE: 1.1e-01\n",
      "Epoch 4410:  0.04749417304992676\n",
      "Test MSE: 8.0e-02\n",
      "Epoch 4420:  0.04749417304992676\n",
      "Test MSE: 6.1e-02\n",
      "Epoch 4430:  0.04749417304992676\n",
      "Test MSE: 6.1e-02\n",
      "Epoch 4440:  0.04749417304992676\n",
      "Test MSE: 8.6e-02\n",
      "Epoch 4450:  0.04749417304992676\n",
      "Test MSE: 6.5e-02\n",
      "Epoch 4460:  0.04749417304992676\n",
      "Test MSE: 6.0e-02\n",
      "Epoch 4470:  0.04749417304992676\n",
      "Test MSE: 6.4e-02\n",
      "Epoch 4480:  0.047049183398485184\n",
      "Test MSE: 6.5e-02\n",
      "Epoch 4490:  0.047049183398485184\n",
      "Test MSE: 6.9e-02\n",
      "Epoch 4500:  0.047049183398485184\n",
      "Test MSE: 6.9e-02\n",
      "Epoch 4510:  0.047049183398485184\n",
      "Test MSE: 6.2e-02\n",
      "Epoch 4520:  0.045500095933675766\n",
      "Test MSE: 5.7e-02\n",
      "Epoch 4530:  0.045500095933675766\n",
      "Test MSE: 6.5e-02\n",
      "Epoch 4540:  0.045500095933675766\n",
      "Test MSE: 8.7e-02\n",
      "Epoch 4550:  0.045500095933675766\n",
      "Test MSE: 6.9e-02\n",
      "Epoch 4560:  0.045500095933675766\n",
      "Test MSE: 6.3e-02\n",
      "Epoch 4570:  0.044449977576732635\n",
      "Test MSE: 5.7e-02\n",
      "Epoch 4580:  0.044449977576732635\n",
      "Test MSE: 7.6e-02\n",
      "Epoch 4590:  0.044449977576732635\n",
      "Test MSE: 6.0e-02\n",
      "Epoch 4600:  0.044449977576732635\n",
      "Test MSE: 5.8e-02\n",
      "Epoch 4610:  0.04401187226176262\n",
      "Test MSE: 5.9e-02\n",
      "Epoch 4620:  0.04401187226176262\n",
      "Test MSE: 5.7e-02\n",
      "Epoch 4630:  0.04401187226176262\n",
      "Test MSE: 8.5e-02\n",
      "Epoch 4640:  0.04401187226176262\n",
      "Test MSE: 8.1e-02\n",
      "Epoch 4650:  0.04401187226176262\n",
      "Test MSE: 5.7e-02\n",
      "Epoch 4660:  0.04401187226176262\n",
      "Test MSE: 5.8e-02\n",
      "Epoch 4670:  0.04401187226176262\n",
      "Test MSE: 7.4e-02\n",
      "Epoch 4680:  0.043785855174064636\n",
      "Test MSE: 5.5e-02\n",
      "Epoch 4690:  0.04236419126391411\n",
      "Test MSE: 6.9e-02\n",
      "Epoch 4700:  0.04236419126391411\n",
      "Test MSE: 7.8e-02\n",
      "Epoch 4710:  0.04236419126391411\n",
      "Test MSE: 5.9e-02\n",
      "Epoch 4720:  0.0418531596660614\n",
      "Test MSE: 5.3e-02\n",
      "Epoch 4730:  0.03996345400810242\n",
      "Test MSE: 5.4e-02\n",
      "Epoch 4740:  0.03996345400810242\n",
      "Test MSE: 7.9e-02\n",
      "Epoch 4750:  0.03996345400810242\n",
      "Test MSE: 6.7e-02\n",
      "Epoch 4760:  0.03996345400810242\n",
      "Test MSE: 5.5e-02\n",
      "Epoch 4770:  0.03996345400810242\n",
      "Test MSE: 5.3e-02\n",
      "Epoch 4780:  0.03996345400810242\n",
      "Test MSE: 5.4e-02\n",
      "Epoch 4790:  0.03996345400810242\n",
      "Test MSE: 8.3e-02\n",
      "Epoch 4800:  0.03996345400810242\n",
      "Test MSE: 7.0e-02\n",
      "Epoch 4810:  0.03996345400810242\n",
      "Test MSE: 5.6e-02\n",
      "Epoch 4820:  0.03903822973370552\n",
      "Test MSE: 5.2e-02\n",
      "Epoch 4830:  0.03903822973370552\n",
      "Test MSE: 5.1e-02\n",
      "Epoch 4840:  0.03903822973370552\n",
      "Test MSE: 5.9e-02\n",
      "Epoch 4850:  0.03903822973370552\n",
      "Test MSE: 5.3e-02\n",
      "Epoch 4860:  0.03903822973370552\n",
      "Test MSE: 5.9e-02\n",
      "Epoch 4870:  0.03903822973370552\n",
      "Test MSE: 1.2e-01\n",
      "Epoch 4880:  0.03903822973370552\n",
      "Test MSE: 5.6e-02\n",
      "Epoch 4890:  0.03903822973370552\n",
      "Test MSE: 5.3e-02\n",
      "Epoch 4900:  0.03759869560599327\n",
      "Test MSE: 5.1e-02\n",
      "Epoch 4910:  0.03759869560599327\n",
      "Test MSE: 7.7e-02\n",
      "Epoch 4920:  0.037459153681993484\n",
      "Test MSE: 5.5e-02\n",
      "Epoch 4930:  0.03744465112686157\n",
      "Test MSE: 5.1e-02\n",
      "Epoch 4940:  0.03583375737071037\n",
      "Test MSE: 4.7e-02\n",
      "Epoch 4950:  0.03583375737071037\n",
      "Test MSE: 5.1e-02\n",
      "Epoch 4960:  0.03583375737071037\n",
      "Test MSE: 8.2e-02\n",
      "Epoch 4970:  0.03583375737071037\n",
      "Test MSE: 5.4e-02\n",
      "Epoch 4980:  0.03583375737071037\n",
      "Test MSE: 5.0e-02\n",
      "Epoch 4990:  0.03583375737071037\n",
      "Test MSE: 5.1e-02\n",
      "Computing derivatives features\n"
     ]
    }
   ],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "#     pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "#                                      lr=1e-1, max_iter=300, \n",
    "#                                      max_eval=int(300*1.25), history_size=150, \n",
    "#                                      line_search_fn=True, batch_mode=False)\n",
    "    pretraining_optimizer =  MADGRAD(semisup_model.network.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    best_state_dict = None; curr_loss = 1000\n",
    "    semisup_model.network.train()\n",
    "    for i in range(5000):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        \n",
    "        if l.item() < curr_loss:\n",
    "            curr_loss = l.item()\n",
    "            best_state_dict = semisup_model.state_dict()\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 13.836886405944824\n"
     ]
    }
   ],
   "source": [
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "print(\"Precision\", (((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    if mse_loss.requires_grad:\n",
    "        mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss\n",
    "\n",
    "def selector_closure():\n",
    "    global ders, dys\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer3.zero_grad()\n",
    "    unsup_loss = semisup_model.selector.loss(ders, dys)\n",
    "    if unsup_loss.requires_grad:\n",
    "        unsup_loss.backward(retain_graph=True)\n",
    "    return unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32d5a85c90a41208c8c0320e8733470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 5.66E-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n",
      "Assigning the suggested_lr to optimizer1\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "# suggested_lr = 1e-5\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "print(\"Assigning the suggested_lr to optimizer1\")\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 300; epochs2 = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = epochs1-100\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 300/300 [07:35<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: 403.34674072265625\n",
      "Semi-supervised solver loss @Epoch 0:  1.1709946393966675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 6/300 [00:09<07:26,  1.52s/it]"
     ]
    }
   ],
   "source": [
    "curr_loss = 500; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "# How long should I pretrain selector part of the model?\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1000; best_generator_state_dict = None\n",
    "        o_tensor = X_u_train[:N, :] # or X_u_train ?\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            # Do I need to scale o_tensor before feeding into the generator?\n",
    "            X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "            d_loss = F.mse_loss(X_gen, o_tensor)\n",
    "            generator_loss = d_loss-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        generator.load_state_dict(best_generator_state_dict)\n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "        if N_res<X_gen.shape[0]: X_gen = sampling_from_rows(X_gen, N_res)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = LBFGSNew(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=300, max_eval=int(300*1.25),\n",
    "                              history_size=150, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "curr_loss = 1000\n",
    "# Stage II: Train semisup_model.network\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 100) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "string_test_performance = scientific2string(test_performance)\n",
    "print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(semisup_model.selector.latest_weighted_features).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ders, dys = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "optimizer3 = LBFGSNew(semisup_model.selector.parameters(), \n",
    "                      lr=learning_rate2, max_iter=300, max_eval=int(300*1.25),\n",
    "                      history_size=150, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# Stage II: Train semisup_model.selector\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "for i in range(30):\n",
    "    optimizer3.step(selector_closure)\n",
    "    l = selector_closure()\n",
    "    if (i % 5) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(np.argsort((semisup_model.selector.latest_weighted_features).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = (semisup_model.selector.latest_weighted_features).detach().numpy()\n",
    "feature_importance = feature_importance\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
