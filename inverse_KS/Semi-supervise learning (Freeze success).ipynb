{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Distance loss\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10000 samples\n",
      "Not including N_res\n"
     ]
    }
   ],
   "source": [
    "include_N_res = False\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = 10000\n",
    "print(f\"Training with {N} samples\")\n",
    "\n",
    "# idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "idx = np.arange(N) # arange for faster training due to the easy data\n",
    "\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = 0.09817477042468103\n",
      "dt = 0.4\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X, T, Exact, acc_order=10)\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.4235734852263704e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "fd_derivatives = tmp[:, :]\n",
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "\n",
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.25):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 1/layers[0]\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss+self.reg_intensity*torch.norm(F.relu(self.latest_weighted_features-self.th), p=0)\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train, fd_derivatives=None, fd_u_t=None, fd_weight=0.0, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        \n",
    "        fd_guidance = 0.0\n",
    "        if fd_weight>0.0 and fd_derivatives is not None and fd_u_t is not None:\n",
    "            # Traditional MSE Loss btw uf and u_train + the fd_guidance loss\n",
    "            row, col = fd_derivatives.shape\n",
    "            fd_guidance += F.mse_loss(X_selector[:row, 0:1], fd_derivatives[:, 0:1])\n",
    "            fd_guidance += fd_weight*(col-1)*F.mse_loss(X_selector[:row, 1:], fd_derivatives[:, 1:])\n",
    "            fd_guidance += fd_weight*F.mse_loss(y_selector[:row, :], fd_u_t)\n",
    "            \n",
    "        else: fd_guidance = self.network.uf\n",
    "            \n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "        \n",
    "        return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load(\"./saved_path_inverse_ks/pretrained5000samples_semisup_model_with_LayerNormDropout_without_physical_reg.pth\")\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained10000labeledsamples_trained0unlabeledsamples.pth\")\n",
    "# pretrained_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = not use_pretrained_weights\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "#     referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)\n",
    "\n",
    "# Highly inefficient code here\n",
    "# Why I cannnot load the network state_dict inside the if statement???\n",
    "tmp = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "tmp.load_state_dict(pretrained_state_dict); network_state_dict = tmp.network.state_dict()\n",
    "del tmp, pretrained_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.407444953918457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.013510518532712e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chk the performance both MSE and PDE relation loss\n",
    "semisup_model.eval()\n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "#     pretraining_optimizer =  MADGRAD(semisup_model.network.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    best_state_dict = None; curr_loss = 1000\n",
    "    semisup_model.network.train()\n",
    "    for i in range(300):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        \n",
    "        if l.item() < curr_loss:\n",
    "            curr_loss = l.item()\n",
    "            best_state_dict = semisup_model.state_dict()\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    \n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_u_train, fd_derivatives=fd_derivatives, \n",
    "                                            fd_u_t=fd_u_t, fd_weight=1.0, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def selector_closure():\n",
    "    global ders, dys\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer3.zero_grad()\n",
    "    unsup_loss = semisup_model.selector.loss(ders, dys)\n",
    "    if unsup_loss.requires_grad:\n",
    "        unsup_loss.backward(retain_graph=True)\n",
    "    return unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n",
      "Assigning the suggested_lr to optimizer1\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "suggested_lr = 1e-6\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "print(\"Assigning the suggested_lr to optimizer1\")\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 300; epochs2 = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the generator\n",
    "# generator = TorchMLP([2, 50, 50, 2])\n",
    "# # generator_training_epochs indicates how string the generator is\n",
    "# adv_f = 100; generator_training_epochs = 300; generator_training_limit = epochs1-100\n",
    "# # I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "# generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "# # sinkhorn distance loss\n",
    "# sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=1, blur=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_loss = 500; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# # Stage I\n",
    "# # How long should I pretrain selector part of the model?\n",
    "# for i in range(epochs1):\n",
    "#     if i%adv_f==0 and i<=generator_training_limit:\n",
    "#         best_generator_loss = 1e6; best_generator_state_dict = None\n",
    "#         o_tensor = X_u_train[:N, :] # or X_u_train ?\n",
    "#         print(\"Training the generator for \")\n",
    "#         for _ in trange(generator_training_epochs):\n",
    "#             semisup_model.eval()\n",
    "#             generator.train()\n",
    "#             generator_optimizer.zero_grad()\n",
    "            \n",
    "#             # Do I need to scale o_tensor before feeding into the generator?\n",
    "#             X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "#             unsup_loss = semisup_model(X_gen)[1]\n",
    "            \n",
    "#             # Choose the distance function that works well with the X_u_train structure\n",
    "#             d_loss = sinkhorn_loss(X_gen, o_tensor)\n",
    "# #             d_loss = F.mse_loss(X_gen, o_tensor)\n",
    "\n",
    "#             generator_loss = 0.05*d_loss-unsup_loss\n",
    "#             generator_loss.backward(retain_graph=True)\n",
    "#             generator_optimizer.step()\n",
    "            \n",
    "#             # Saving the best_generator_state_dict\n",
    "#             if generator_loss.item() < best_generator_loss:\n",
    "#                 best_generator_loss = generator_loss.item()\n",
    "#                 best_generator_state_dict = generator.state_dict()\n",
    "\n",
    "#         print(\"The best generator loss:\", best_generator_loss)\n",
    "#         if best_generator_state_dict is not None: \n",
    "#             generator.load_state_dict(best_generator_state_dict)\n",
    "            \n",
    "#         generator.eval()\n",
    "#         X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "#         if N_res<X_gen.shape[0]: X_gen = sampling_from_rows(X_gen, N_res)\n",
    "#         X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "        \n",
    "#         # Re init\n",
    "#         with torch.no_grad(): semisup_model.network.load_state_dict(network_state_dict)\n",
    "\n",
    "#     semisup_model.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     optimizer1.step(pcgrad_closure)\n",
    "#     l = pcgrad_closure()\n",
    "#     if (i % F_print) == 0:\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         else:\n",
    "#             print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "#             print(\"Finishing the first stage\")\n",
    "#             break\n",
    "#         print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage II: Train semisup_model.network\n",
    "# def closure():\n",
    "#     global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "#     if torch.is_grad_enabled():\n",
    "#         optimizer2.zero_grad()\n",
    "#     # With fd guidance\n",
    "#     mse_loss = semisup_model(X_u_train, fd_derivatives, fd_u_t, 2.0, False)[0]\n",
    "#     if mse_loss.requires_grad:\n",
    "#         mse_loss.backward(retain_graph=True)\n",
    "#     return mse_loss\n",
    "\n",
    "# # optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "# #                                      lr=1e-1, max_iter=100, \n",
    "# #                                      max_eval=int(100*1.25), history_size=150, \n",
    "# #                                      line_search_fn='strong_wolfe')\n",
    "\n",
    "# optimizer2 = MADGRAD(semisup_model.network.parameters(), lr=1e-6, momentum=0.95)\n",
    "\n",
    "# curr_loss = 10000\n",
    "# semisup_model.network.train()\n",
    "# semisup_model.selector.eval()\n",
    "# for i in range(epochs2):\n",
    "#     optimizer2.step(closure)\n",
    "#     if (i % 2) == 0:\n",
    "#         l = closure()\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(semisup_model.state_dict(), \"../inverse_KS/saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained7500labeledsamples_trained0unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_derivatives = to_numpy(fd_derivatives)\n",
    "fd_u_t = to_numpy(fd_u_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3217648049467243e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_test = 12000\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[:n_test, :]))\n",
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1236142274555205u_xx-1.0062766635476028uf*u_x-0.31973993926315875u_xxxx"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = alpha*referenced_derivatives+(1-alpha)*fd_derivatives[:n_test, :]\n",
    "y_input = alpha*u_t+(1-alpha)*fd_u_t[:n_test, :]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "\n",
    "from sparsereg.model import STRidge\n",
    "# print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "coef = STRidge(threshold=0.01, alpha=100, max_iter=1000, normalize=True).fit(X_input, y_input.ravel()).coef_\n",
    "idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "# print(coef)\n",
    "# idxs = np.nonzero(coef)[0]\n",
    "\n",
    "for idx in idxs[:]:\n",
    "    if not np.isclose(coef[idx], 0.0) and coef[idx]<const_range[1] and coef[idx]>const_range[0]:\n",
    "        print(str(coef[idx])+poly_feature_names[idx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 2048 samples => 2.0e-6\n",
    "n_test_samples = 2048\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star[:n_test_samples, :])).detach(), u_star[:n_test_samples, :]).item()\n",
    "string_test_performance = scientific2string(test_performance)\n",
    "print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ders, dys = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "ders = (ders-semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)\n",
    "\n",
    "optimizer3 = LBFGSNew(semisup_model.selector.parameters(),\n",
    "                      lr=learning_rate2, max_iter=100, max_eval=int(100*1.25),\n",
    "                      history_size=150, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# Stage II: Train semisup_model.selector\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "for i in range(20):\n",
    "    optimizer3.step(selector_closure)\n",
    "    if (i % 5) == 0:\n",
    "        l = selector_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(np.argsort((semisup_model.selector.latest_weighted_features).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = (semisup_model.selector.latest_weighted_features).detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light = lightgbm.LGBMRegressor(n_estimators=300, learning_rate=0.1, reg_lambda=1)\n",
    "light = SklearnModel(model=light, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "light_feature_importance = light.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(20000)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx, :]))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_gp = SymbolicRegressor(population_size=60000, generations=3, tournament_size=50,\n",
    "                           function_set=('add', 'sub', 'mul'), const_range=(-1, 1),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, parsimony_coefficient=0.001, max_samples=0.9,\n",
    "                           verbose=1, low_memory=False, n_jobs=3)\n",
    "est_gp.fit(referenced_derivatives, u_t.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_exp(est_gp._program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import pysr, best, best_callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(to_numpy(referenced_derivatives), to_numpy(u_t).ravel(), niterations=5, \n",
    "                 binary_operators=[\"-\", \"*\"], unary_operators=[], procs=3, \n",
    "                 populations=20, npop=4000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
