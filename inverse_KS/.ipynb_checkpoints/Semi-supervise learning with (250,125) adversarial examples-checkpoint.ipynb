{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Distance loss\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 20000 samples\n",
      "Training with 10000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "include_N_res = True\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = 20000\n",
    "print(f\"Training with {N} samples\")\n",
    "\n",
    "# idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "idx = np.arange(N) # arange for faster training due to the easy data\n",
    "\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = 0.09817477042468103\n",
      "dt = 0.4\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X, T, Exact, acc_order=10)\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)[idx, :]\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.908906334343826e-11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "fd_derivatives = tmp[idx, :]\n",
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "\n",
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.25):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 1/layers[0]\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss+self.reg_intensity*torch.norm(F.relu(self.latest_weighted_features-self.th), p=0)\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train, fd_derivatives=None, fd_u_t=None, fd_weight=0.0, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        \n",
    "        fd_guidance = 0.0\n",
    "        if fd_weight>0.0 and fd_derivatives is not None and fd_u_t is not None:\n",
    "            # Traditional MSE Loss btw uf and u_train + the fd_guidance loss\n",
    "            row, col = fd_derivatives.shape\n",
    "            fd_guidance += F.mse_loss(X_selector[:row, 0:1], fd_derivatives[:, 0:1])\n",
    "            fd_guidance += fd_weight*(col-1)*F.mse_loss(X_selector[:row, 1:], fd_derivatives[:, 1:])\n",
    "            fd_guidance += fd_weight*F.mse_loss(y_selector[:row, :], fd_u_t)\n",
    "            \n",
    "        else: fd_guidance = self.network.uf\n",
    "            \n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "        \n",
    "        return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load(\"./saved_path_inverse_ks/pretrained5000samples_semisup_model_with_LayerNormDropout_without_physical_reg.pth\")\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_ks/pretrained_KS_GPU_semisup_model.pth\")\n",
    "# pretrained_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = not use_pretrained_weights\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "#     referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)\n",
    "    del tmp\n",
    "\n",
    "# Highly inefficient code here\n",
    "# Why I cannnot load the network state_dict inside the if statement???\n",
    "tmp = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "tmp.load_state_dict(pretrained_state_dict); network_state_dict = tmp.network.state_dict()\n",
    "del tmp, pretrained_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006000507273711264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008324743248522282"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chk the performance both MSE and PDE relation loss\n",
    "semisup_model.eval()\n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "#     pretraining_optimizer =  MADGRAD(semisup_model.network.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    best_state_dict = None; curr_loss = 1000\n",
    "    semisup_model.network.train()\n",
    "    for i in range(300):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        \n",
    "        if l.item() < curr_loss:\n",
    "            curr_loss = l.item()\n",
    "            best_state_dict = semisup_model.state_dict()\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    \n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_u_train, fd_derivatives=fd_derivatives, \n",
    "                                            fd_u_t=fd_u_t, fd_weight=1.0, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def selector_closure():\n",
    "    global ders, dys\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer3.zero_grad()\n",
    "    unsup_loss = semisup_model.selector.loss(ders, dys)\n",
    "    if unsup_loss.requires_grad:\n",
    "        unsup_loss.backward(retain_graph=True)\n",
    "    return unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n",
      "Assigning the suggested_lr to optimizer1\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "suggested_lr = 1e-6\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "print(\"Assigning the suggested_lr to optimizer1\")\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 300; epochs2 = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = epochs1-100\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "# sinkhorn distance loss\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=1, blur=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:55<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: -0.38208329677581787\n",
      "Semi-supervised solver loss @Epoch 0:  0.2831014394760132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:14<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: -0.20552769303321838\n",
      "Semi-supervised solver loss @Epoch 100:  0.27898430824279785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [03:12<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: -0.3834551274776459\n",
      "Semi-supervised solver loss @Epoch 200:  0.5251495838165283\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 500; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "# How long should I pretrain selector part of the model?\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1e6; best_generator_state_dict = None\n",
    "        o_tensor = X_u_train[:N, :] # or X_u_train ?\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            \n",
    "            # Do I need to scale o_tensor before feeding into the generator?\n",
    "            X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "            \n",
    "            # Choose the distance function that works well with the X_u_train structure\n",
    "            d_loss = sinkhorn_loss(X_gen, o_tensor)\n",
    "#             d_loss = F.mse_loss(X_gen, o_tensor)\n",
    "\n",
    "            generator_loss = 0.05*d_loss-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            \n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        if best_generator_state_dict is not None: \n",
    "            generator.load_state_dict(best_generator_state_dict)\n",
    "            \n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "        if N_res<X_gen.shape[0]: X_gen = sampling_from_rows(X_gen, N_res)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "        \n",
    "        # Re init\n",
    "        with torch.no_grad(): semisup_model.network.load_state_dict(network_state_dict)\n",
    "\n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0016935294261202216\n"
     ]
    }
   ],
   "source": [
    "# Stage II: Train semisup_model.network\n",
    "def closure():\n",
    "    global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    # With fd guidance\n",
    "    mse_loss = semisup_model(X_u_train, fd_derivatives, fd_u_t, 1.0, False)[0]\n",
    "    if mse_loss.requires_grad:\n",
    "        mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss\n",
    "\n",
    "optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=100, \n",
    "                                     max_eval=int(100*1.25), history_size=150, \n",
    "                                     line_search_fn='strong_wolfe')\n",
    "\n",
    "curr_loss = 10000\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 10) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 2048 samples => 2.0e-6\n",
    "n_test_samples = 2048\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star[:n_test_samples, :])).detach(), u_star[:n_test_samples, :]).item()\n",
    "string_test_performance = scientific2string(test_performance)\n",
    "print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.25608184933662415\n",
      "[0 4 1 3 2]\n",
      "Epoch 5:  0.2557809352874756\n",
      "[0 4 1 3 2]\n",
      "Epoch 10:  0.2557263672351837\n",
      "[0 4 1 3 2]\n",
      "Epoch 15:  0.25573232769966125\n",
      "[0 4 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "ders, dys = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "ders = (ders-semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)\n",
    "\n",
    "optimizer3 = LBFGSNew(semisup_model.selector.parameters(),\n",
    "                      lr=learning_rate2, max_iter=100, max_eval=int(100*1.25),\n",
    "                      history_size=150, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# Stage II: Train semisup_model.selector\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "for i in range(20):\n",
    "    optimizer3.step(selector_closure)\n",
    "    l = selector_closure()\n",
    "    if (i % 5) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(np.argsort((semisup_model.selector.latest_weighted_features).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "u_xx 0.26936862\n",
      "u_xxx 0.19929047\n",
      "u_x 0.19154088\n",
      "u_xxxx 0.17000432\n",
      "uf 0.16979569\n"
     ]
    }
   ],
   "source": [
    "feature_importance = (semisup_model.selector.latest_weighted_features).detach().numpy()\n",
    "feature_importance = feature_importance\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Training MSE: 0.0010755533675015016\n",
      "('uf', 0.22848565710473648)\n",
      "('u_x', 0.22059150544807649)\n",
      "('u_xx', 0.21436513231042917)\n",
      "('u_xxx', 0.18467867467200355)\n",
      "('u_xxxx', 0.1518790304647543)\n"
     ]
    }
   ],
   "source": [
    "light = lightgbm.LGBMRegressor(n_estimators=300, learning_rate=0.1, reg_lambda=1)\n",
    "light = SklearnModel(model=light, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "light_feature_importance = light.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(20000)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx, :]))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_gp = SymbolicRegressor(population_size=60000, generations=3, tournament_size=50,\n",
    "                           function_set=('add', 'sub', 'mul'), const_range=(-1, 1),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, parsimony_coefficient=0.001, max_samples=0.9,\n",
    "                           verbose=1, low_memory=False, n_jobs=3)\n",
    "est_gp.fit(referenced_derivatives, u_t.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_exp(est_gp._program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import pysr, best, best_callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on julia -O3 /var/folders/z3/_stfms3523dd5mnfr3ch5n100000gp/T/tmpzjq27up8/runfile.jl\n",
      "Activating environment on workers.\n",
      "      From worker 3:\t  Activating environment at `/usr/local/lib/python3.9/site-packages/Project.toml`\n",
      "      From worker 4:\t  Activating environment at `/usr/local/lib/python3.9/site-packages/Project.toml`\n",
      "      From worker 2:\t  Activating environment at `/usr/local/lib/python3.9/site-packages/Project.toml`\n",
      "Importing installed module on workers...Finished!\n",
      "Testing module on workers...Finished!\n",
      "Testing entire pipeline on workers...Finished!\n",
      "Started!\n",
      "\n",
      "Cycles per second: 2.750e+03\n",
      "Progress: 1 / 100 total iterations (1.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 1.340e+04\n",
      "Progress: 2 / 100 total iterations (2.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 1.000e+04\n",
      "Progress: 3 / 100 total iterations (3.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 1.350e+04\n",
      "Progress: 4 / 100 total iterations (4.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 1.130e+04\n",
      "Progress: 5 / 100 total iterations (5.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 1.250e+04\n",
      "Progress: 8 / 100 total iterations (8.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.440e+04\n",
      "Progress: 12 / 100 total iterations (12.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.170e+04\n",
      "Progress: 13 / 100 total iterations (13.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.470e+04\n",
      "Progress: 15 / 100 total iterations (15.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.300e+04\n",
      "Progress: 16 / 100 total iterations (16.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.310e+04\n",
      "Progress: 17 / 100 total iterations (17.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.280e+04\n",
      "Progress: 23 / 100 total iterations (23.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 3.050e+04\n",
      "Progress: 27 / 100 total iterations (27.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 3.050e+04\n",
      "Progress: 28 / 100 total iterations (28.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 3.060e+04\n",
      "Progress: 29 / 100 total iterations (29.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.970e+04\n",
      "Progress: 32 / 100 total iterations (32.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.390e+04\n",
      "Progress: 38 / 100 total iterations (38.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.420e+04\n",
      "Progress: 39 / 100 total iterations (39.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.180e+04\n",
      "Progress: 40 / 100 total iterations (40.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.180e+04\n",
      "Progress: 41 / 100 total iterations (41.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.620e+04\n",
      "Progress: 43 / 100 total iterations (43.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.490e+04\n",
      "Progress: 44 / 100 total iterations (44.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.160e+04\n",
      "Progress: 46 / 100 total iterations (46.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.000e+04\n",
      "Progress: 47 / 100 total iterations (47.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.200e+04\n",
      "Progress: 48 / 100 total iterations (48.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.200e+04\n",
      "Progress: 49 / 100 total iterations (49.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.070e+04\n",
      "Progress: 50 / 100 total iterations (50.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "\n",
      "Cycles per second: 2.100e+04\n",
      "Progress: 51 / 100 total iterations (51.000%)\n",
      "Hall of Fame:\n",
      "-----------------------------------------\n",
      "Complexity  Loss       Score     Equation\n",
      "1           2.859e-03  1.090e-02  x4\n",
      "3           1.945e-03  1.927e-01  (x2 * -1.5709802)\n",
      "5           4.308e-04  7.536e-01  (x1 * (0.1105973 - x0))\n",
      "7           2.857e-04  2.053e-01  ((x1 * (0.1105973 - x0)) - x2)\n",
      "9           2.405e-04  8.608e-02  ((x1 * ((x3 - x1) - x0)) - x2)\n",
      "11          2.239e-04  3.575e-02  ((x1 * (((x3 * x0) - x1) - x0)) - x2)\n",
      "13          1.778e-04  1.154e-01  (((x1 - (x4 - x3)) * ((x4 - x1) - x0)) - x2)\n",
      "15          1.728e-04  1.417e-02  (((x1 - ((x4 * 0.5488605) - x3)) * ((x3 - x1) - x0)) - x2)\n",
      "17          1.647e-04  2.401e-02  (((x1 - (((x4 - x3) * 0.43561998) - x3)) * ((0.005396383 - x1) - x0)) - x2)\n",
      "19          1.624e-04  7.023e-03  (((x1 - (((x4 - x3) * 0.4353089) - x3)) * (((x0 * 0.031610653) - x1) - x0)) - x2)\n",
      "\n",
      "Killing process... will return when done.\n"
     ]
    }
   ],
   "source": [
    "equations = pysr(to_numpy(referenced_derivatives), to_numpy(u_t).ravel(), niterations=5, \n",
    "                 binary_operators=[\"-\", \"*\"], unary_operators=[], procs=3, \n",
    "                 populations=20, npop=4000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
