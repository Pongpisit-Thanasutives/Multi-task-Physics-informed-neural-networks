{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Distance loss\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is arranged in an uniform grid\n",
      "Training with 30000 samples\n",
      "Training with 15000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "include_N_res = True\n",
    "\n",
    "x_limit = 300\n",
    "t_limit = 100\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True, uniform=True, x_limit=None, t_limit=None)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = x_limit*t_limit\n",
    "if N == X_star.shape[0]:\n",
    "    print(\"Using the entire domain\")\n",
    "    include_N_res = False\n",
    "    \n",
    "print(f\"Training with {N} samples\")\n",
    "\n",
    "# idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "idx = np.arange(N) # arange for faster training due to the easy data\n",
    "\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not use this class with complex-valued input arrays.\n",
      "This class applies 1 transpose to the Exact before doing the job.\n",
      "dx = 0.09817477042468103\n",
      "dt = 0.4\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X, T, Exact, acc_order=10)\n",
    "\n",
    "# Even if you use the true vals of dx and dt -> The finite difference is not working with a non-uniform grid\n",
    "fdc.dx = 0.09817477042468103\n",
    "fdc.dt = 0.4\n",
    "\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)[idx, :]\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.793009904582092e-11"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "fd_derivatives = tmp[idx, :]\n",
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0000315917550011u_xx-1.0000125811028373u_xxxx-0.9999838361820689uf*u_x"
     ]
    }
   ],
   "source": [
    "X_input = fd_derivatives\n",
    "y_input = fd_u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "\n",
    "from sparsereg.model import STRidge\n",
    "# print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "coef = STRidge(threshold=0.01, alpha=0.1, max_iter=1000, normalize=True).fit(X_input, y_input.ravel()).coef_\n",
    "idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "for idx in idxs[:]:\n",
    "    if not np.isclose(coef[idx], 0.0):\n",
    "        print(str(coef[idx])+poly_feature_names[idx], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "\n",
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.1):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 0.1\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        return mse_loss+self.reg_intensity*(torch.norm(reg_term, p=0)+(torch.tensor([1.0, 1.0, 2.0, 3.0, 4.0])*reg_term).sum())\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train, fd_derivatives=None, fd_u_t=None, fd_weight=0.0, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        \n",
    "        fd_guidance = 0.0\n",
    "        if fd_weight>0.0 and fd_derivatives is not None and fd_u_t is not None:\n",
    "            # Traditional MSE Loss btw uf and u_train + the fd_guidance loss\n",
    "            row, col = fd_derivatives.shape\n",
    "            fd_guidance += F.mse_loss(X_selector[:row, 0:1], fd_derivatives[:, 0:1])\n",
    "            fd_guidance += fd_weight*(col-1)*F.mse_loss(X_selector[:row, 1:], fd_derivatives[:, 1:])\n",
    "            fd_guidance += fd_weight*F.mse_loss(y_selector[:row, :], fd_u_t)\n",
    "            \n",
    "        else: fd_guidance = self.network.uf\n",
    "            \n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "        \n",
    "        return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load(\"./saved_path_inverse_ks/pretrained5000samples_semisup_model_with_LayerNormDropout_without_physical_reg.pth\")\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained30000labeledsamples_trained15000unlabeledsamples.pth\")\n",
    "# pretrained_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = not use_pretrained_weights\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "#     referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)\n",
    "\n",
    "# Highly inefficient code here\n",
    "# Why I cannnot load the network state_dict inside the if statement???\n",
    "tmp = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "tmp.load_state_dict(pretrained_state_dict); network_state_dict = tmp.network.state_dict()\n",
    "del tmp, pretrained_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.25849437713623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0007950526778586209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chk the performance both MSE and PDE relation loss\n",
    "semisup_model.eval()\n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "(((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "#     pretraining_optimizer =  MADGRAD(semisup_model.network.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    best_state_dict = None; curr_loss = 1000\n",
    "    semisup_model.network.train()\n",
    "    for i in range(300):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        \n",
    "        if l.item() < curr_loss:\n",
    "            curr_loss = l.item()\n",
    "            best_state_dict = semisup_model.state_dict()\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    \n",
    "#     semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "\n",
    "    semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "    semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_u_train, fd_derivatives=fd_derivatives, \n",
    "                                            fd_u_t=fd_u_t, fd_weight=1.0, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def selector_closure():\n",
    "    global ders, dys\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer3.zero_grad()\n",
    "    unsup_loss = semisup_model.selector.loss(ders, dys)\n",
    "    if unsup_loss.requires_grad:\n",
    "        unsup_loss.backward(retain_graph=True)\n",
    "    return unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n",
      "Assigning the suggested_lr to optimizer1\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "suggested_lr = 1e-6\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "print(\"Assigning the suggested_lr to optimizer1\")\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 200; epochs2 = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the generator\n",
    "# generator = TorchMLP([2, 50, 50, 2])\n",
    "# # generator_training_epochs indicates how string the generator is\n",
    "# adv_f = 100; generator_training_epochs = 500; generator_training_limit = epochs1-100\n",
    "# # I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "# generator_optimizer = MADGRAD(generator.parameters(), lr=1e-3, momentum=0.95)\n",
    "# # sinkhorn distance loss\n",
    "# distance_func = distance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_loss = 500; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# # Stage I\n",
    "# # How long should I pretrain selector part of the model?\n",
    "# for i in range(epochs1):\n",
    "#     if i%adv_f==0 and i<=generator_training_limit:\n",
    "#         best_generator_loss = 1e6; best_generator_state_dict = None\n",
    "#         o_tensor = X_u_train[:N, :] # or X_u_train ?\n",
    "#         print(\"Training the generator for \")\n",
    "#         for _ in trange(generator_training_epochs):\n",
    "#             semisup_model.eval()\n",
    "#             generator.train()\n",
    "#             generator_optimizer.zero_grad()\n",
    "            \n",
    "#             # Do I need to scale o_tensor before feeding into the generator?\n",
    "#             X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "#             unsup_loss = semisup_model(X_gen)[1]\n",
    "            \n",
    "#             # Choose the distance function that works well with the X_u_train structure\n",
    "#             d_loss = distance_func(X_gen, o_tensor)\n",
    "# #             d_loss = F.mse_loss(X_gen, o_tensor)\n",
    "#             generator_loss = 0.05*d_loss-unsup_loss\n",
    "#             generator_loss.backward(retain_graph=True)\n",
    "#             generator_optimizer.step()\n",
    "            \n",
    "#             # Saving the best_generator_state_dict\n",
    "#             if generator_loss.item() < best_generator_loss:\n",
    "#                 best_generator_loss = generator_loss.item()\n",
    "#                 best_generator_state_dict = generator.state_dict()\n",
    "\n",
    "#         print(\"The best generator loss:\", best_generator_loss)\n",
    "#         if best_generator_state_dict is not None: \n",
    "#             generator.load_state_dict(best_generator_state_dict)\n",
    "            \n",
    "#         generator.eval()\n",
    "#         X_gen = scale_to_range(generator(o_tensor), lb, ub)\n",
    "#         if N_res<X_gen.shape[0]: X_gen = sampling_from_rows(X_gen, N_res)\n",
    "#         X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "#     semisup_model.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     optimizer1.step(pcgrad_closure)\n",
    "#     l = pcgrad_closure()\n",
    "#     if (i % F_print) == 0:\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         else:\n",
    "#             print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "#             print(\"Finishing the first stage\")\n",
    "#             break\n",
    "#         print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "        \n",
    "# # Re init\n",
    "# with torch.no_grad(): semisup_model.network.load_state_dict(network_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage II: Train semisup_model.network\n",
    "# def closure():\n",
    "#     global N, X_u_train, u_train, fd_derivatives, fd_u_t\n",
    "#     if torch.is_grad_enabled():\n",
    "#         optimizer2.zero_grad()\n",
    "#     # With fd guidance\n",
    "#     mse_loss = semisup_model(X_u_train, fd_derivatives, fd_u_t, 2.0, False)[0]\n",
    "#     if mse_loss.requires_grad:\n",
    "#         mse_loss.backward(retain_graph=True)\n",
    "#     return mse_loss\n",
    "\n",
    "# # optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "# #                                      lr=1e-1, max_iter=100, \n",
    "# #                                      max_eval=int(100*1.25), history_size=150, \n",
    "# #                                      line_search_fn='strong_wolfe')\n",
    "\n",
    "# optimizer2 = MADGRAD(semisup_model.network.parameters(), lr=1e-6, momentum=0.95)\n",
    "\n",
    "# curr_loss = 10000\n",
    "# semisup_model.network.train()\n",
    "# semisup_model.selector.eval()\n",
    "# for i in range(epochs2):\n",
    "#     optimizer2.step(closure)\n",
    "#     if (i % 2) == 0:\n",
    "#         l = closure()\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_derivatives = to_numpy(fd_derivatives)\n",
    "fd_u_t = to_numpy(fd_u_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003179807390552014\n"
     ]
    }
   ],
   "source": [
    "n_test = 12000\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[:n_test, :]))\n",
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "print((((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item())\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = alpha*referenced_derivatives+(1-alpha)*fd_derivatives[:n_test, :]\n",
    "y_input = alpha*u_t+(1-alpha)*fd_u_t[:n_test, :]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also https://cran.r-project.org/web/packages/L0Learn/vignettes/L0Learn-vignette.html is useful For finding PDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use your big brain to choose, not looking at the BIC and AIC blindly.\n",
      "\n",
      "Alpha: 1\n",
      "-4.516689777374268u_xx*u_xxxx  -1.1222424507141113uf*u_x  -0.41278791427612305u_x*u_xxx  -0.31177589297294617uf*u_xxx  -0.11617869138717651uf*u_xx  -0.1023745909333229u_xxxx  0.04281386733055115u_x  \n",
      "\n",
      "Alpha: 2\n",
      "-1.0640901327133179uf*u_x  -0.41822391748428345u_xx  -0.3353460729122162u_xxxx  -0.1905515193939209uf*u_xxx  -0.04392003268003464uf*u_xx  \n",
      "\n",
      "Alpha: 3\n",
      "-1.0630955696105957uf*u_x  -0.42448076605796814u_xx  -0.3391903340816498u_xxxx  -0.18595367670059204uf*u_xxx  \n",
      "\n",
      "Alpha: 4\n",
      "-1.0550096035003662uf*u_x  -0.4673525094985962u_xx  -0.36447858810424805u_xxxx  \n",
      "\n",
      "Alpha: 16\n",
      "-1.0869503021240234uf*u_x  -0.15811364352703094u_xx  \n",
      "\n",
      "(Occam's razor) heuristically chosen alpha: 4\n"
     ]
    }
   ],
   "source": [
    "# My trick to make it work\n",
    "# threshold=0.02 -> alpha=1.0 to alpha=100.0, normalize=False\n",
    "# threshold=0.01 -> alpha=0.5 to alpha=500.0, normalize=True\n",
    "# Is this bullshit???\n",
    "\n",
    "# TODO: Try find min -np.log(MSE/maxMSE)/(Complexity - minComplexity)\n",
    "\n",
    "print(\"Use your big brain to choose, not looking at the BIC and AIC blindly.\\n\")\n",
    "oc_scores = []\n",
    "scores = []\n",
    "const_range = (-1.5, 1.5)\n",
    "alphas = np.arange(99)+1\n",
    "for al in alphas:\n",
    "    # print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "    sparse_regressor = STRidge(threshold=0.02, alpha=al, max_iter=1000, normalize=False)\n",
    "    sparse_regressor.fit(X_input, y_input.ravel()); coef = sparse_regressor.coef_\n",
    "    n_params = len(np.nonzero(coef)[0]) # this might not be a good choice ?\n",
    "    idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "    # print(coef)\n",
    "    # idxs = np.nonzero(coef)[0]\n",
    "    \n",
    "    predictions = sparse_regressor.predict(X_input).astype(float)\n",
    "    truths = y_input.ravel().astype(float)\n",
    "    oc_scores.append((mean_squared_error(predictions, truths), n_params))\n",
    "    score = bic(predictions, truths, n_params)\n",
    "\n",
    "    if score not in scores:\n",
    "        scores.append(score)\n",
    "        print('Alpha:', al)\n",
    "        \n",
    "        for idx in idxs[:]:\n",
    "            if not np.isclose(coef[idx], 0.0):\n",
    "#             if not np.isclose(coef[idx], 0.0) and coef[idx]<const_range[1] and coef[idx]>const_range[0]:\n",
    "                print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "print(\"(Occam's razor) heuristically chosen alpha:\", alphas[np.argmax(np.array(occam_razor(oc_scores)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0550096035003662uf*u_x  -0.4673525094985962u_xx  -0.36447858810424805u_xxxx  "
     ]
    }
   ],
   "source": [
    "sparse_regressor = STRidge(threshold=0.02, alpha=4, max_iter=1000, normalize=False)\n",
    "sparse_regressor.fit(X_input, y_input.ravel()); coef = sparse_regressor.coef_\n",
    "n_params = 2*len(np.nonzero(coef)[0])\n",
    "idxs = np.argsort(np.abs(coef))[::-1]\n",
    "for idx in idxs[:]:\n",
    "    if not np.isclose(coef[idx], 0.0) and coef[idx]<const_range[1] and coef[idx]>const_range[0]:\n",
    "        print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# n_test = 12000\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[:n_test, :]))\n",
    "# referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "# print((((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean().item())\n",
    "\n",
    "# alpha = 1\n",
    "# const_range = (-1.5, 1.5)\n",
    "\n",
    "# X_input = alpha*referenced_derivatives+(1-alpha)*fd_derivatives[:n_test, :]\n",
    "# y_input = alpha*u_t+(1-alpha)*fd_u_t[:n_test, :]\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# X_input = poly.fit_transform(X_input)\n",
    "# poly_feature_names = poly.get_feature_names(feature_names)\n",
    "# for i, f in enumerate(poly_feature_names):\n",
    "#     poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "\n",
    "# from sparsereg.model import STRidge\n",
    "# # print(STRidge(unbias=False).fit(fd_derivatives, fd_u_t.ravel()).coef_)\n",
    "# coef = STRidge(threshold=0.02, alpha=20, max_iter=1000, normalize=False).fit(X_input, y_input.ravel()).coef_\n",
    "# idxs = np.argsort(np.abs(coef))[::-1]\n",
    "\n",
    "# # print(coef)\n",
    "# # idxs = np.nonzero(coef)[0]\n",
    "\n",
    "# for idx in idxs[:]:\n",
    "#     if not np.isclose(coef[idx], 0.0):\n",
    "#         print(str(coef[idx])+poly_feature_names[idx], \" \", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "Test MSE: 4.2e-04\n"
     ]
    }
   ],
   "source": [
    "# Test on 2048 samples => 2.0e-6\n",
    "n_test_samples = 2048\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star[:n_test_samples, :])).detach(), u_star[:n_test_samples, :]).item()\n",
    "string_test_performance = scientific2string(test_performance)\n",
    "print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.6306882500648499\n",
      "[0 4 1 3 2]\n",
      "Epoch 5:  0.6255757808685303\n",
      "[4 0 1 3 2]\n",
      "Epoch 10:  0.617821991443634\n",
      "[4 0 3 1 2]\n",
      "Epoch 15:  0.6093431711196899\n",
      "[4 3 0 1 2]\n",
      "Epoch 20:  0.6014670133590698\n",
      "[4 3 0 1 2]\n",
      "Epoch 25:  0.5941519141197205\n",
      "[4 3 0 1 2]\n",
      "Epoch 30:  0.5873293876647949\n",
      "[4 3 0 1 2]\n",
      "Epoch 35:  0.5807583928108215\n",
      "[4 3 0 1 2]\n",
      "Epoch 40:  0.4761818051338196\n",
      "[4 3 0 2 1]\n",
      "Epoch 45:  0.47260046005249023\n",
      "[4 3 2 0 1]\n",
      "Epoch 50:  0.46863824129104614\n",
      "[4 3 2 0 1]\n",
      "Epoch 55:  0.367110013961792\n",
      "[4 3 2 0 1]\n",
      "Epoch 60:  0.3652147054672241\n",
      "[4 3 2 0 1]\n",
      "Epoch 65:  0.36316633224487305\n",
      "[3 4 2 0 1]\n",
      "Epoch 70:  0.3610832095146179\n",
      "[3 4 2 0 1]\n",
      "Epoch 75:  0.3589746654033661\n",
      "[3 4 2 0 1]\n",
      "Epoch 80:  0.356893926858902\n",
      "[3 4 2 0 1]\n",
      "Epoch 85:  0.45591145753860474\n",
      "[3 4 2 0 1]\n",
      "Epoch 90:  0.3552916944026947\n",
      "[2 4 3 0 1]\n",
      "Epoch 95:  0.3559991717338562\n",
      "[2 4 3 0 1]\n",
      "Epoch 100:  0.25602397322654724\n",
      "[2 4 3 0 1]\n",
      "Epoch 105:  0.25588005781173706\n",
      "[2 3 4 0 1]\n",
      "Epoch 110:  0.3556603789329529\n",
      "[2 3 4 0 1]\n",
      "Epoch 115:  0.35501211881637573\n",
      "[2 4 3 0 1]\n",
      "Epoch 120:  0.35470378398895264\n",
      "[4 3 2 0 1]\n",
      "Epoch 125:  0.35488632321357727\n",
      "[4 3 2 0 1]\n",
      "Epoch 130:  0.3549439013004303\n",
      "[4 3 2 0 1]\n",
      "Epoch 135:  0.45487985014915466\n",
      "[4 3 2 0 1]\n",
      "Epoch 140:  0.25446203351020813\n",
      "[3 4 2 0 1]\n",
      "Epoch 145:  0.3546387851238251\n",
      "[2 4 3 0 1]\n",
      "Epoch 150:  0.3545762002468109\n",
      "[2 3 4 0 1]\n",
      "Epoch 155:  0.454657644033432\n",
      "[4 3 2 0 1]\n",
      "Epoch 160:  0.3546334505081177\n",
      "[4 3 2 0 1]\n",
      "Epoch 165:  0.35447728633880615\n",
      "[2 3 4 0 1]\n",
      "Epoch 170:  0.25446683168411255\n",
      "[4 3 2 0 1]\n",
      "Epoch 175:  0.4545978009700775\n",
      "[3 4 2 0 1]\n",
      "Epoch 180:  0.2545391023159027\n",
      "[4 2 3 0 1]\n",
      "Epoch 185:  0.45452970266342163\n",
      "[2 4 3 0 1]\n",
      "Epoch 190:  0.3545420169830322\n",
      "[4 3 2 0 1]\n",
      "Epoch 195:  0.2544245719909668\n",
      "[4 2 3 0 1]\n",
      "Epoch 200:  0.25439146161079407\n",
      "[2 3 4 0 1]\n",
      "Epoch 205:  0.4544547498226166\n",
      "[4 3 2 0 1]\n",
      "Epoch 210:  0.25452008843421936\n",
      "[2 3 4 0 1]\n",
      "Epoch 215:  0.45446303486824036\n",
      "[3 4 2 0 1]\n",
      "Epoch 220:  0.3543925881385803\n",
      "[4 2 3 0 1]\n",
      "Epoch 225:  0.45442891120910645\n",
      "[4 3 2 0 1]\n",
      "Epoch 230:  0.354476660490036\n",
      "[3 2 4 0 1]\n",
      "Epoch 235:  0.2544867992401123\n",
      "[4 3 2 0 1]\n",
      "Epoch 240:  0.25443777441978455\n",
      "[4 3 2 0 1]\n",
      "Epoch 245:  0.45440372824668884\n",
      "[4 2 3 0 1]\n",
      "Epoch 250:  0.35443487763404846\n",
      "[4 2 3 0 1]\n",
      "Epoch 255:  0.4544735252857208\n",
      "[3 4 2 0 1]\n",
      "Epoch 260:  0.2545817196369171\n",
      "[4 2 3 0 1]\n",
      "Epoch 265:  0.45449191331863403\n",
      "[3 2 4 0 1]\n",
      "Epoch 270:  0.2544529139995575\n",
      "[4 2 3 0 1]\n",
      "Epoch 275:  0.2543981671333313\n",
      "[3 2 4 0 1]\n",
      "Epoch 280:  0.35442984104156494\n",
      "[4 3 2 0 1]\n",
      "Epoch 285:  0.2544291913509369\n",
      "[4 2 3 0 1]\n",
      "Epoch 290:  0.3544377088546753\n",
      "[3 4 2 0 1]\n",
      "Epoch 295:  0.35440513491630554\n",
      "[4 3 2 0 1]\n",
      "Epoch 300:  0.25444406270980835\n",
      "[4 2 3 0 1]\n",
      "Epoch 305:  0.4544368386268616\n",
      "[4 3 2 0 1]\n",
      "Epoch 310:  0.35438665747642517\n",
      "[4 3 2 0 1]\n",
      "Epoch 315:  0.35442259907722473\n",
      "[3 2 4 0 1]\n",
      "Epoch 320:  0.5544635057449341\n",
      "[2 4 3 0 1]\n",
      "Epoch 325:  0.3544834554195404\n",
      "[4 3 2 0 1]\n",
      "Epoch 330:  0.4545130729675293\n",
      "[2 4 3 0 1]\n",
      "Epoch 335:  0.4545043706893921\n",
      "[3 4 2 0 1]\n",
      "Epoch 340:  0.4544415771961212\n",
      "[4 3 2 0 1]\n",
      "Epoch 345:  0.3543908894062042\n",
      "[2 3 4 0 1]\n",
      "Epoch 350:  0.4543907046318054\n",
      "[4 3 2 0 1]\n",
      "Epoch 355:  0.25443458557128906\n",
      "[4 3 2 0 1]\n",
      "Epoch 360:  0.3545152544975281\n",
      "[4 3 2 0 1]\n",
      "Epoch 365:  0.4544564485549927\n",
      "[2 3 4 0 1]\n",
      "Epoch 370:  0.35444676876068115\n",
      "[4 3 2 0 1]\n",
      "Epoch 375:  0.25441569089889526\n",
      "[3 2 4 0 1]\n",
      "Epoch 380:  0.35444527864456177\n",
      "[4 2 3 0 1]\n",
      "Epoch 385:  0.45451080799102783\n",
      "[4 3 2 0 1]\n",
      "Epoch 390:  0.35446983575820923\n",
      "[3 2 4 0 1]\n",
      "Epoch 395:  0.4544956684112549\n",
      "[4 3 2 0 1]\n",
      "Epoch 400:  0.35443615913391113\n",
      "[3 2 4 0 1]\n",
      "Epoch 405:  0.5545083284378052\n",
      "[4 3 2 0 1]\n",
      "Epoch 410:  0.25463563203811646\n",
      "[3 4 2 0 1]\n",
      "Epoch 415:  0.5544894337654114\n",
      "[3 4 2 0 1]\n",
      "Epoch 420:  0.2544810473918915\n",
      "[3 2 4 0 1]\n",
      "Epoch 425:  0.35444891452789307\n",
      "[3 4 2 0 1]\n",
      "Epoch 430:  0.35446134209632874\n",
      "[4 3 2 0 1]\n",
      "Epoch 435:  0.35450026392936707\n",
      "[2 4 3 0 1]\n",
      "Epoch 440:  0.3544667661190033\n",
      "[3 4 2 0 1]\n",
      "Epoch 445:  0.45442625880241394\n",
      "[2 4 3 0 1]\n",
      "Epoch 450:  0.35446470975875854\n",
      "[4 3 2 0 1]\n",
      "Epoch 455:  0.354463130235672\n",
      "[4 3 2 0 1]\n",
      "Epoch 460:  0.454486608505249\n",
      "[2 4 3 0 1]\n",
      "Epoch 465:  0.35438716411590576\n",
      "[4 2 3 0 1]\n",
      "Epoch 470:  0.3545157313346863\n",
      "[4 3 2 0 1]\n",
      "Epoch 475:  0.2544447183609009\n",
      "[2 4 3 0 1]\n",
      "Epoch 480:  0.2545015215873718\n",
      "[2 4 3 0 1]\n",
      "Epoch 485:  0.3544466197490692\n",
      "[3 4 2 0 1]\n",
      "Epoch 490:  0.4544413387775421\n",
      "[4 2 3 0 1]\n",
      "Epoch 495:  0.35446786880493164\n",
      "[2 4 3 0 1]\n",
      "Epoch 500:  0.45449137687683105\n",
      "[4 3 2 0 1]\n",
      "Epoch 505:  0.25448936223983765\n",
      "[4 2 3 0 1]\n",
      "Epoch 510:  0.2543790936470032\n",
      "[2 3 4 0 1]\n",
      "Epoch 515:  0.3544151782989502\n",
      "[3 4 2 0 1]\n",
      "Epoch 520:  0.25440049171447754\n",
      "[2 4 3 0 1]\n",
      "Epoch 525:  0.35440361499786377\n",
      "[4 3 2 0 1]\n",
      "Epoch 530:  0.35438573360443115\n",
      "[2 3 4 0 1]\n",
      "Epoch 535:  0.35444775223731995\n",
      "[4 3 2 0 1]\n",
      "Epoch 540:  0.4544052183628082\n",
      "[2 3 4 0 1]\n",
      "Epoch 545:  0.454427570104599\n",
      "[3 4 2 0 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7111ca738709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msemisup_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_weighted_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9fe57d3723cf>\u001b[0m in \u001b[0;36mselector_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0munsup_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemisup_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsup_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0munsup_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munsup_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ders, dys = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "ders = (ders-semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)\n",
    "\n",
    "# optimizer3 = LBFGSNew(semisup_model.selector.parameters(),\n",
    "#                       lr=learning_rate2, max_iter=100, max_eval=int(100*1.25),\n",
    "#                       history_size=150, line_search_fn=True, batch_mode=False)\n",
    "optimizer3 = MADGRAD(semisup_model.selector.parameters(), lr=5e-3, momentum=0.95)\n",
    "\n",
    "# Stage II: Train semisup_model.selector\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "for i in range(1000):\n",
    "    optimizer3.step(selector_closure)\n",
    "    if (i % 5) == 0:\n",
    "        l = selector_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(np.argsort((semisup_model.selector.latest_weighted_features).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "u_x 0.37561023\n",
      "uf 0.32502457\n",
      "u_xx 0.10013802\n",
      "u_xxx 0.09990077\n",
      "u_xxxx 0.099326454\n"
     ]
    }
   ],
   "source": [
    "feature_importance = (semisup_model.selector.latest_weighted_features).detach().numpy()\n",
    "feature_importance = feature_importance\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.5, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Softmax layer's outputs as feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=1/len(feature_names), save_path='../visualization/ks_feature_importances_selector_with_softmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Training MSE: 0.0005890103479770972\n",
      "('uf', 0.3169302870533099)\n",
      "('u_xx', 0.22378441710603397)\n",
      "('u_x', 0.1611013473930873)\n",
      "('u_xxxx', 0.15289982425307558)\n",
      "('u_xxx', 0.14528412419449327)\n"
     ]
    }
   ],
   "source": [
    "light = lightgbm.LGBMRegressor(n_estimators=300, learning_rate=0.1, reg_alpha=10)\n",
    "light = SklearnModel(model=light, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "light_feature_importance = light.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost\n",
      "Done training\n",
      "Training MSE: 0.00036422763\n",
      "('u_xx', 0.3272193719032313)\n",
      "('u_x', 0.24156895639850134)\n",
      "('uf', 0.22134472759581697)\n",
      "('u_xxx', 0.12260762784790438)\n",
      "('u_xxxx', 0.08725931625454598)\n",
      "\n",
      "Catboost\n",
      "Done training\n",
      "Training MSE: 0.00018550439011075538\n",
      "('u_xx', 0.2928676094879013)\n",
      "('uf', 0.2901900573230381)\n",
      "('u_x', 0.2611325741653077)\n",
      "('u_xxx', 0.08789615538736091)\n",
      "('u_xxxx', 0.06791360363639201)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Ensemble model\")\n",
    "# sklearn_model = SklearnModel(model=ensemble, X_train=X_np_full, y_train=y_np_full, feature_names=feature_names)\n",
    "# print('Training GBM algos...')\n",
    "# print()\n",
    "\n",
    "xg = xgboost.XGBRegressor(reg_alpha=10)\n",
    "cat = catboost.CatBoostRegressor(iterations=None, depth=4, learning_rate=0.1, verbose=0, reg_lambda=10)\n",
    "\n",
    "print(\"Xgboost\")\n",
    "xg = SklearnModel(model=xg, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "xg_feature_importance = xg.feature_importance()\n",
    "print()\n",
    "\n",
    "print(\"Catboost\")\n",
    "# Showing the best performance\n",
    "cat = SklearnModel(model=cat, X_train=to_numpy(ders), y_train=to_numpy(dys).ravel(), feature_names=feature_names)\n",
    "cat_feature_importance = cat.feature_importance()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uf 0.3169302870533099\n",
      "u_x 0.1611013473930873\n",
      "u_xx 0.22378441710603397\n",
      "u_xxx 0.14528412419449327\n",
      "u_xxxx 0.15289982425307558\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_feature_importances = {}\n",
    "for f in feature_names:\n",
    "    avg_feature_importances[f] = (light_feature_importance[f])\n",
    "    print(f, avg_feature_importances[f])\n",
    "\n",
    "tmp = sorted([(v, f) for (f, v) in avg_feature_importances.items()])[::-1]\n",
    "xxx = [f for (f, v) in tmp]\n",
    "yyy = [v for (f, v) in tmp]\n",
    "\n",
    "plt.plot(yyy, xxx, 'bo')\n",
    "plt.ylabel(\"Feature importance\")\n",
    "plt.xlabel(\"Partial deriavative features (possible candidates)\")\n",
    "# plt.savefig(\"../visualization/ks_trees_feature_importances_with_softmax.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
