{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.6\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, \n",
    "                    cplx2tensor, ComplexTorchMLP, complex_mse, TanhProb)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../deephpms_data/KdV_simple2.pkl\n",
      "Training with 500 samples\n",
      "Training with 500 unsup samples\n"
     ]
    }
   ],
   "source": [
    "data = pickle_load('../deephpms_data/KdV_simple2.pkl')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['u'])\n",
    "\n",
    "X_sol, T_sol = np.meshgrid(x, t)\n",
    "\n",
    "x_star = X_sol.flatten()[:,None]\n",
    "t_star = T_sol.flatten()[:,None]\n",
    "\n",
    "X_star = np.hstack((x_star, t_star))\n",
    "if Exact.shape[1]==X_sol.shape[0] and Exact.shape[0]==X_sol.shape[1]:\n",
    "    Exact = Exact.T\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "# DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "# X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True, uniform=True, x_limit=None, t_limit=None)\n",
    "# X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Bound\n",
    "ub = X_star.max(axis=0)\n",
    "lb = X_star.min(axis=0)\n",
    "\n",
    "# For identification\n",
    "N = 500\n",
    "# idx = np.arange(N)\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "print(\"Training with\", N, \"samples\")\n",
    "\n",
    "noise_intensity = 0.01\n",
    "noisy_xt, noisy_labels = False, False\n",
    "if noisy_xt: X_train = perturb(X_train, noise_intensity); print(\"Noisy (x, t)\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "if noisy_labels: u_train = perturb(u_train, noise_intensity); print(\"Noisy labels\")\n",
    "else: print(\"Clean labels\")\n",
    "\n",
    "# Unsup data\n",
    "include_N_res = True\n",
    "if include_N_res:\n",
    "    N_res = N\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=7.5e-4): #5e-4 and training with 400 epochs are also OK!\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])-(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = torch.tensor([1.0, 1.0, 2.0, 3.0])/10.0\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*(F.threshold(self.weighted_features(inn), self.th, 0.0)))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        \n",
    "        l1 = mse_loss\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1+self.reg_intensity*(l2)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = None\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_small_KdV/simple2_semisup_model_state_dict_250labeledsamples.pth\")\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=TanhProb(), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=False,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "#     semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "#     if best_state_dict is not None: semisup_model.load_state_dict(best_state_dict)\n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    predictions, unsup_loss = semisup_model(X_train)\n",
    "    losses = [F.mse_loss(predictions[:N, :], u_train[:N, :]), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.0003, grad_fn=<MseLossBackward>), tensor(18.6473, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0096, grad_fn=<MseLossBackward>), tensor(0.0932, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0070, grad_fn=<MseLossBackward>), tensor(0.0245, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0038, grad_fn=<MseLossBackward>), tensor(0.0150, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0019, grad_fn=<MseLossBackward>), tensor(0.0110, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0011, grad_fn=<MseLossBackward>), tensor(0.0077, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward>), tensor(0.0056, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward>), tensor(0.0046, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0003, grad_fn=<MseLossBackward>), tensor(0.0042, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0002, grad_fn=<MseLossBackward>), tensor(0.0039, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0002, grad_fn=<MseLossBackward>), tensor(0.0038, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0002, grad_fn=<MseLossBackward>), tensor(0.0037, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0001, grad_fn=<MseLossBackward>), tensor(0.0037, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0001, grad_fn=<MseLossBackward>), tensor(0.0037, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0001, grad_fn=<MseLossBackward>), tensor(0.0036, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0001, grad_fn=<MseLossBackward>), tensor(0.0036, grad_fn=<AddBackward0>)]\n",
      "[tensor(9.8942e-05, grad_fn=<MseLossBackward>), tensor(0.0036, grad_fn=<AddBackward0>)]\n",
      "[tensor(9.3835e-05, grad_fn=<MseLossBackward>), tensor(0.0036, grad_fn=<AddBackward0>)]\n",
      "[tensor(8.9663e-05, grad_fn=<MseLossBackward>), tensor(0.0028, grad_fn=<AddBackward0>)]\n",
      "[tensor(8.6114e-05, grad_fn=<MseLossBackward>), tensor(0.0028, grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Joint training\n",
    "optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "optimizer.param_groups[0]['lr'] = 1e-7 # Used to be 1e-11.\n",
    "optimizer.param_groups[1]['lr'] = 1e-2\n",
    "\n",
    "# Use ~idx to sample adversarial data points\n",
    "for i in range(500):\n",
    "    semisup_model.train()\n",
    "    optimizer.step(pcgrad_closure)\n",
    "    loss = pcgrad_closure(return_list=True)\n",
    "    if i%25==0: print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0604286444504396e-06\n",
      "4.343440025422751e-07\n",
      "4.3369294644435286e-07\n",
      "4.3369294644435286e-07\n",
      "4.3369294644435286e-07\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(50):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "# X_selector = (X_selector - semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028705946169793606\n",
      "[2 3 0 1]\n",
      "0.00288115325383842\n",
      "[2 3 0 1]\n",
      "0.002877661259844899\n",
      "[2 3 0 1]\n",
      "0.002867615781724453\n",
      "[2 3 0 1]\n",
      "0.0028592045418918133\n",
      "[2 3 0 1]\n",
      "0.002871139207854867\n",
      "[2 3 0 1]\n",
      "0.0028576015029102564\n",
      "[2 3 0 1]\n",
      "0.002881326712667942\n",
      "[2 3 0 1]\n",
      "0.0028710784390568733\n",
      "[2 3 0 1]\n",
      "0.002874873112887144\n",
      "[2 3 0 1]\n"
     ]
    }
   ],
   "source": [
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=500, max_eval=int(1.25*500), history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # Am I forget to normalize the derivative features?, NVM\n",
    "#     loss = F.mse_loss(semisup_model.selector(X_selector), y_selector)\n",
    "    loss = semisup_model.selector.loss(X_selector, y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "max_it = 10\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%1==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        print(np.argsort(semisup_model.selector.latest_weighted_features.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_test = X_star.shape[0]\n",
    "n_test = 50000\n",
    "idx_test = np.arange(n_test)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.948363 +0.000000i)u_xxx\n",
      "    + (-5.835543 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Set normalize=1\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 1000, maxit=100, STR_iters=100, split=0.8, l0_penalty=None, normalize=2)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36536175, 0.5783518 , 0.248983  , 0.2742597 ], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.latest_weighted_features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(semisup_model, \"./saved_path_inverse_small_KdV/simple2_semisup_model_state_dict_500labeledsamples500unlabeledsamples_tanhV2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all (500 labels are enough.)\n",
    "# PDE derived using STRidge \n",
    "# u_t = (-0.856551 +0.000000i)u_xxx\n",
    "#     + (-5.615004 +0.000000i)uf*u_x\n",
    "# V2\n",
    "# u_t = (-0.948363 +0.000000i)u_xxx\n",
    "#     + (-5.835543 +0.000000i)uf*u_x\n",
    "\n",
    "# Clean (x, t) but noisy labels (500 labels are enough.) \n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.998013 +0.000000i)u_xxx\n",
    "#     + (-6.004286 +0.000000i)uf*u_x\n",
    "\n",
    "# Noisy (x, t) and noisy labels (2000 labels)\n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.846196 +0.000000i)u_xxx\n",
    "#     + (-5.573660 +0.000000i)uf*u_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
