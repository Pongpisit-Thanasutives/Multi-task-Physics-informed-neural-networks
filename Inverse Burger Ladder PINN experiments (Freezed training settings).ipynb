{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae120e2-ac70-442b-bab0-db69808a7aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable, grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e20158-6dd9-407e-ada8-498fcd23331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc2907e-aafd-42d4-a5b0-d52b1e89810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 25600\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 25600\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5f7298-dbf9-4f27-abc5-9e8031c42186",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, lambda_1_init, lambda_2_init):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.apply(self.xavier_init)\n",
    "        self.lambda_1 = torch.nn.Parameter(torch.tensor([lambda_1_init]))\n",
    "        self.lambda_2 = torch.nn.Parameter(torch.tensor([lambda_2_init]))\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.model(torch.cat([x, t], dim=1))\n",
    "    \n",
    "    def loss(self, x, t, y_input, is_pde_parameters_update=False, include_unsup=False):\n",
    "        total_loss = []\n",
    "        \n",
    "        uf, unsup_loss = self.forward(x, t)\n",
    "        \n",
    "        total_loss.append(F.mse_loss(uf, y_input))\n",
    "        if unsup_loss: \n",
    "                    total_loss.append(unsup_loss)\n",
    "        \n",
    "        if is_pde_parameters_update:\n",
    "            lambda_1 = self.lambda_1\n",
    "            lambda_2 = torch.exp(self.lambda_2)\n",
    "            \n",
    "            # PDE Loss calculation\n",
    "            u_t = self.gradients(uf, t)[0]\n",
    "            u_x = self.gradients(uf, x)[0]\n",
    "            u_xx = self.gradients(u_x, x)[0]\n",
    "            l_eq = (u_t + lambda_1*uf*u_x - lambda_2*u_xx)\n",
    "            l_eq = (l_eq**2).mean()\n",
    "            \n",
    "            total_loss = total_loss + l_eq\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def get_theta(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        uf, _ = self.forward(x, t)\n",
    "        \n",
    "        # PDE Loss calculation\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        \n",
    "        X = torch.cat([-uf*u_x, u_xx], dim=1)\n",
    "        y = u_t\n",
    "        \n",
    "        theta = (torch.inverse(X.T@X))@(X.T@y)\n",
    "        \n",
    "        theta_1 = np.maximum(theta[:, 0][0].detach().item(), torch.finfo(torch.float32).eps)\n",
    "        theta_2 = np.log(np.maximum(theta[:, 0][1].detach().item(), torch.finfo(torch.float32).eps))\n",
    "        \n",
    "        return theta_1, theta_2\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        uf, _ = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def set_lambdas(self, lambda_1_init, lambda_2_init):\n",
    "        self.lambda_1 = torch.nn.Parameter(torch.tensor([lambda_1_init]))\n",
    "        self.lambda_2 = torch.nn.Parameter(torch.tensor([lambda_2_init]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5314def5-1131-4217-b246-a98339d6d7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "\n",
    "# model = nn.Sequential(nn.Linear(2, hidden_nodes), \n",
    "#                         nn.Tanh(), \n",
    "#                         nn.Linear(hidden_nodes, hidden_nodes),\n",
    "#                         nn.Tanh(), \n",
    "#                         nn.Linear(hidden_nodes, hidden_nodes),\n",
    "#                         nn.Tanh(), \n",
    "#                         nn.Linear(hidden_nodes, hidden_nodes),\n",
    "#                         nn.Tanh(),\n",
    "#                         nn.Linear(hidden_nodes, 1))\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "\n",
    "# Doesn't matter, can be anything.\n",
    "lambda_1_init = 0.0\n",
    "lambda_2_init = 0.0\n",
    "\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a52bbca6-55bc-4ac0-ba5d-4526067227ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "479a44da-dd37-43de-b69f-393dfd1bd640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=False, include_unsup=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(network.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=False, include_unsup=False)[0]\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6e1b5-1874-470e-b047-374402e88501",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931f6c24-0778-41ba-baee-a01dfe0f1208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using SGD/Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.8225613832473755\n",
      "Epoch 10:  0.7488536834716797\n",
      "Epoch 20:  0.6759630441665649\n",
      "Epoch 30:  0.6567462682723999\n",
      "Epoch 40:  0.6371355056762695\n",
      "Epoch 50:  0.6105217933654785\n",
      "Epoch 60:  0.5865285396575928\n",
      "Epoch 70:  0.5654343962669373\n",
      "Epoch 80:  0.5464804172515869\n",
      "Epoch 90:  0.5304383635520935\n",
      "Epoch 100:  0.5182356238365173\n",
      "Epoch 110:  0.5105104446411133\n",
      "Epoch 120:  0.5076316595077515\n",
      "Epoch 130:  0.509384274482727\n",
      "Epoch 140:  0.5154348015785217\n",
      "Epoch 150:  0.5251656770706177\n",
      "Epoch 160:  0.5376631021499634\n",
      "Epoch 170:  0.5516762733459473\n",
      "Epoch 180:  0.5653824806213379\n",
      "Epoch 190:  0.5762892365455627\n",
      "Epoch 200:  0.5474542379379272\n",
      "Epoch 210:  0.5207949280738831\n",
      "Epoch 220:  0.5183008313179016\n",
      "Epoch 230:  0.5253952145576477\n",
      "Epoch 240:  0.5343295335769653\n",
      "Epoch 250:  0.48493772745132446\n",
      "Epoch 260:  0.4776107966899872\n",
      "Epoch 270:  0.4761551022529602\n",
      "Epoch 280:  0.4773373007774353\n",
      "Epoch 290:  0.4291597902774811\n",
      "Epoch 300:  0.4243885576725006\n",
      "Epoch 310:  0.423099160194397\n",
      "Epoch 320:  0.39344722032546997\n",
      "Epoch 330:  0.3736143112182617\n",
      "Epoch 340:  0.37713325023651123\n",
      "Epoch 350:  0.35436928272247314\n",
      "Epoch 360:  0.333467036485672\n",
      "Epoch 370:  0.3423287868499756\n",
      "Epoch 380:  0.3484496772289276\n",
      "Epoch 390:  0.3128604292869568\n",
      "Epoch 400:  0.31494784355163574\n",
      "Epoch 410:  0.30754268169403076\n",
      "Epoch 420:  0.28601378202438354\n",
      "Epoch 430:  0.2940344214439392\n",
      "Epoch 440:  0.29290008544921875\n",
      "Epoch 450:  0.27646705508232117\n",
      "Epoch 460:  0.28447338938713074\n",
      "Epoch 470:  0.27106890082359314\n",
      "Epoch 480:  0.2629413604736328\n",
      "Epoch 490:  0.27398252487182617\n",
      "Epoch 499:  0.26088425517082214\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.020115727558732033\n",
      "Epoch 10:  6.151912384666502e-05\n",
      "Epoch 20:  4.788831574842334e-06\n",
      "Epoch 30:  1.6984450894597103e-06\n",
      "Epoch 40:  1.2329290939305793e-06\n",
      "Epoch 50:  1.1224090030736988e-06\n",
      "Epoch 60:  1.0031163810708676e-06\n",
      "Epoch 70:  9.736418178363238e-07\n",
      "Epoch 80:  8.445001640211558e-07\n",
      "Epoch 90:  8.052352313825395e-07\n",
      "Epoch 100:  7.205088081718714e-07\n",
      "Epoch 110:  7.022397880973585e-07\n",
      "Epoch 120:  7.014490392975858e-07\n",
      "Epoch 130:  7.014490392975858e-07\n",
      "Epoch 140:  7.014490392975858e-07\n",
      "Epoch 150:  7.014490392975858e-07\n",
      "Epoch 160:  7.014490392975858e-07\n",
      "Epoch 170:  7.014490392975858e-07\n",
      "Epoch 180:  7.014490392975858e-07\n",
      "Epoch 190:  7.014490392975858e-07\n",
      "Epoch 200:  7.014490392975858e-07\n",
      "Epoch 210:  7.014490392975858e-07\n",
      "Epoch 220:  7.014490392975858e-07\n",
      "Epoch 230:  7.014490392975858e-07\n",
      "Epoch 240:  7.014490392975858e-07\n",
      "Epoch 250:  7.014490392975858e-07\n",
      "Epoch 260:  7.014490392975858e-07\n",
      "Epoch 270:  7.014490392975858e-07\n",
      "Epoch 280:  7.014490392975858e-07\n",
      "Epoch 290:  7.014490392975858e-07\n",
      "Epoch 299:  7.014490392975858e-07\n"
     ]
    }
   ],
   "source": [
    "lookahead = False\n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=5e-3, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead: optimizer1 = Lookahead(optimizer1)\n",
    "\n",
    "epochs1 = 500 # How long this should be ???\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGD/Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=60, max_eval=75, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d53927-8a3d-4958-aed9-f0f786033259",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc61987-1f66-4310-ac74-2a771ebd44f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.014490961410047e-07"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_ladder_network_mse(network, X_star, u_star) # BEST: 5.905130819883198e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813bd31-79e5-43cd-bcec-1beceefe9397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3e6ce-47b4-44d7-81ed-35c7839a33c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c5ff0-8507-4b8c-8169-6525c2e21ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270669f9-fc69-409b-8ca8-f5b0561079b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e213f79-0c03-4d70-8b80-1c637cb1c6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc2df745-c037-4402-8f4f-734a683ed163",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bac2e-3c19-452b-a0ee-d907c32b4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5e484-c515-4807-b0c7-e8042f7b0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056d90c-2d71-4c8e-9072-36120aeecffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668ca84-bf19-43d9-9ff9-568182ddcd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd25c5-0333-41b0-b05b-900993180536",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c7240-b960-49de-af67-5bf52a2222ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b133bb1-20e6-46f8-b97b-6f5108f6355b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80901d-1b9b-4942-9ca5-10bdb47122f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a7b6b-81c5-43b0-97b8-e9fa015dd155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba852976-795f-4ff9-8272-7500b63c5808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d138cec-8ca2-4d90-84e5-fa61e9d2f05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b717d8-1c64-464e-8dae-dfdd5241eb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9924a70-37e7-4e88-9c97-480ca44199f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f334bedf-6d36-4c99-b0ae-679fa354e6b7",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c93628-d10d-49e2-868a-f4226ea7bf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77382e-10a3-4d05-985b-2cc690590ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d35b5c-e2d8-4918-9184-e2a1680fbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c10b89-d5c9-4aac-8769-0963f0309481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f2325-1b16-4cfc-b2fe-a77fd58782c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0aa9b8-ea72-4b63-9394-c219b642f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5511f61-c948-4ecc-a518-71847e926dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86e48e-f02a-4085-bd0a-50dc7775b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
