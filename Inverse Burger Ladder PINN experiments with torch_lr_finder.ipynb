{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01588b0a-2e22-4fc7-b853-2bdb0e7a02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC  # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f334226-14bc-4ba8-9f54-b313cfefdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1405080-bfff-4155-afdf-b5fe632bae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 2000\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:, None]\n",
    "x = data['x'].flatten()[:, None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0888bd-10cc-442b-9e97-9601fccc06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        print('Init using xavier')\n",
    "        self.model.apply(self.xavier_init)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def loss(self, data, y_input, include_unsup=False):\n",
    "        total_loss = []\n",
    "        \n",
    "        uf, unsup_loss = self.forward(data)\n",
    "        \n",
    "        total_loss.append(F.mse_loss(uf, y_input))\n",
    "        if include_unsup: # or if unsup_loss: ?, lets chk\n",
    "            total_loss.append(unsup_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        data = torch.cat([x, t], dim=-1)\n",
    "        uf, _ = self.forward(data)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597beb8-deff-480c-8ec6-616bbe89f6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96666527-e474-4cb8-b116-5b4fbc57b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa0cfb31374fb4aaa813ec0d2e5560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 7.13E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate finding')\n",
    "bs = 4000\n",
    "bs = N if bs>N else bs\n",
    "criterion = LadderLoss()\n",
    "tmp_optimizer = SGDGC(model.parameters(), lr=1e-7, use_gc=True, nesterov=True, momentum=0.9)\n",
    "trainloader = get_dataloader(X_u_train, u_train, bs=4000)\n",
    "lr_finder = LRFinder(model, optimizer=tmp_optimizer, criterion=criterion, device=\"cpu\")\n",
    "lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "_, suggested_lr = lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ccfd3a-1353-43d1-a286-aeea221a50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init using xavier\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "network = Network(model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973af118-178f-477e-b2a1-d42a9123a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(network.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "#         param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=False)[0]\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d31ae1-c3ae-4be7-9c0a-eb1279539ea5",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6797e381-a987-4838-817c-10c017d93c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the lookahead option\n",
      "1st Phase optimization using SGD/Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.7160319685935974\n",
      "Epoch 10:  0.5841265916824341\n",
      "Epoch 20:  0.5032790899276733\n",
      "Epoch 30:  0.5011416673660278\n",
      "Epoch 40:  0.48378312587738037\n",
      "Epoch 50:  0.4533221423625946\n",
      "Epoch 60:  0.3900904655456543\n",
      "Epoch 70:  0.3509014844894409\n",
      "Epoch 80:  0.34847292304039\n",
      "Epoch 90:  0.3429672122001648\n",
      "Epoch 100:  0.3414376974105835\n",
      "Epoch 110:  0.34009599685668945\n",
      "Epoch 120:  0.33908772468566895\n",
      "Epoch 130:  0.3381412625312805\n",
      "Epoch 140:  0.33713299036026\n",
      "Epoch 150:  0.3359459936618805\n",
      "Epoch 160:  0.33437633514404297\n",
      "Epoch 170:  0.33196237683296204\n",
      "Epoch 180:  0.32776692509651184\n",
      "Epoch 190:  0.32018330693244934\n",
      "Epoch 200:  0.3092633783817291\n",
      "Epoch 210:  0.2995707094669342\n",
      "Epoch 220:  0.2888162136077881\n",
      "Epoch 230:  0.27772507071495056\n",
      "Epoch 240:  0.2714713215827942\n",
      "Epoch 250:  0.2676730155944824\n",
      "Epoch 260:  0.26442527770996094\n",
      "Epoch 270:  0.2621324062347412\n",
      "Epoch 280:  0.260136216878891\n",
      "Epoch 290:  0.2583584189414978\n",
      "Epoch 300:  0.2567967474460602\n",
      "Epoch 310:  0.2553151845932007\n",
      "Epoch 320:  0.2539428174495697\n",
      "Epoch 330:  0.25263792276382446\n",
      "Epoch 340:  0.2513870596885681\n",
      "Epoch 350:  0.25020480155944824\n",
      "Epoch 360:  0.24902719259262085\n",
      "Epoch 370:  0.24789959192276\n",
      "Epoch 380:  0.2468116581439972\n",
      "Epoch 390:  0.2457711547613144\n",
      "Epoch 400:  0.24477016925811768\n",
      "Epoch 410:  0.24382591247558594\n",
      "Epoch 420:  0.24290494620800018\n",
      "Epoch 430:  0.24205201864242554\n",
      "Epoch 440:  0.24126404523849487\n",
      "Epoch 450:  0.24051377177238464\n",
      "Epoch 460:  0.23983612656593323\n",
      "Epoch 470:  0.23920652270317078\n",
      "Epoch 480:  0.23863384127616882\n",
      "Epoch 490:  0.2381465882062912\n",
      "Epoch 500:  0.23763920366764069\n",
      "Epoch 510:  0.2372356653213501\n",
      "Epoch 520:  0.23682382702827454\n",
      "Epoch 530:  0.23646140098571777\n",
      "Epoch 540:  0.2361413836479187\n",
      "Epoch 550:  0.23583343625068665\n",
      "Epoch 560:  0.23554956912994385\n",
      "Epoch 570:  0.23525077104568481\n",
      "Epoch 580:  0.2350328266620636\n",
      "Epoch 590:  0.23476329445838928\n",
      "Epoch 600:  0.2345360517501831\n",
      "Epoch 610:  0.2343364953994751\n",
      "Epoch 620:  0.23412710428237915\n",
      "Epoch 630:  0.23394912481307983\n",
      "Epoch 640:  0.2337484359741211\n",
      "Epoch 650:  0.2336125373840332\n",
      "Epoch 660:  0.23343525826931\n",
      "Epoch 670:  0.23334476351737976\n",
      "Epoch 680:  0.2331565022468567\n",
      "Epoch 690:  0.2330349087715149\n",
      "Epoch 700:  0.2329288125038147\n",
      "Epoch 710:  0.23281404376029968\n",
      "Epoch 720:  0.232692152261734\n",
      "Epoch 730:  0.23256349563598633\n",
      "Epoch 740:  0.23245322704315186\n",
      "Epoch 750:  0.23231284320354462\n",
      "Epoch 760:  0.23213693499565125\n",
      "Epoch 770:  0.23193475604057312\n",
      "Epoch 780:  0.2317172884941101\n",
      "Epoch 790:  0.23138833045959473\n",
      "Epoch 800:  0.23103806376457214\n",
      "Epoch 810:  0.2305978685617447\n",
      "Epoch 820:  0.23014190793037415\n",
      "Epoch 830:  0.22961929440498352\n",
      "Epoch 840:  0.22897475957870483\n",
      "Epoch 850:  0.22848746180534363\n",
      "Epoch 860:  0.22787190973758698\n",
      "Epoch 870:  0.22741428017616272\n",
      "Epoch 880:  0.22704938054084778\n",
      "Epoch 890:  0.2267051637172699\n",
      "Epoch 900:  0.2265230119228363\n",
      "Epoch 910:  0.22629907727241516\n",
      "Epoch 920:  0.2261040210723877\n",
      "Epoch 930:  0.22604337334632874\n",
      "Epoch 940:  0.22586381435394287\n",
      "Epoch 950:  0.2257930040359497\n",
      "Epoch 960:  0.22575393319129944\n",
      "Epoch 970:  0.22562086582183838\n",
      "Epoch 980:  0.22563603520393372\n",
      "Epoch 990:  0.2255631983280182\n",
      "Epoch 999:  0.22580662369728088\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.014392074197530746\n",
      "Epoch 10:  2.635919736349024e-05\n",
      "Epoch 20:  3.98295287595829e-06\n",
      "Epoch 30:  1.997875415327144e-06\n",
      "Epoch 40:  1.7689969808998285e-06\n",
      "Epoch 50:  1.6218950804613996e-06\n",
      "Epoch 60:  1.5381201592390426e-06\n",
      "Epoch 70:  1.4868953712721122e-06\n",
      "Epoch 80:  1.1953219427596196e-06\n",
      "Epoch 90:  1.143873873843404e-06\n",
      "Epoch 100:  1.1154539834024035e-06\n",
      "Epoch 110:  1.113085204451636e-06\n",
      "Epoch 120:  1.113085204451636e-06\n"
     ]
    }
   ],
   "source": [
    "lookahead = False \n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=suggested_lr, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead:\n",
    "    print(\"Using the lookahead option\")\n",
    "    optimizer1 = Lookahead(optimizer1)\n",
    "else:\n",
    "    print(\"Not using the lookahead option\")\n",
    "    \n",
    "epochs1 = 1000 # How long this should be ??? (500 seems to be a good number.)\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGD/Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=80, max_eval=100, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "cur_loss = 1e-6\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 100) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        \n",
    "        # To early stop from the loop\n",
    "        if cur_loss != l.item(): cur_loss = l.item()\n",
    "        else:\n",
    "            print(\"Duplicating training loss => Early stop\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8dcd3-04f5-400c-9ea1-6317eef36b94",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3802d3d9-a25d-4674-afd2-e0ef3176c2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.044602635782212e-06"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((network(X_star)[0].detach() - u_star)**2).mean().item()\n",
    "# BEST-full: 5.905130819883198e-07\n",
    "# BEST-2000: 2.4505434339516796e-06, 3.473501465123263e-06, 5.008192601962946e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248d1dfc-6cb8-4a44-91bd-75cd6c5acea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporting derivatives and dynamics to .npy files ###\n",
    "# network.load_state_dict(torch.load(\"./saved_path_inverse_burger/nn_ladder_without_physical_reg_trained2000samples.pth\"))\n",
    "\n",
    "grads_dict, dynamics = network.get_gradients_dict(*dimension_slicing(X_star))\n",
    "index2features = grads_dict.keys()\n",
    "derivatives = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "dynamics = torch.squeeze(dynamics).detach().numpy()\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-25600.npy\", derivatives)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-25600.npy\", dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eaf7dd-3eff-4799-bfbb-c5c3efd08c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64c7d5-7cfb-4d36-b99b-c3586ac44d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0add1f9-9cd7-4bdf-b5a1-55bf750a403c",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7dc94-8253-4ee6-9781-c17e2728b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07367905-4fd8-437c-a273-f7336fa04cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05d873-7549-446c-a74d-54d6aa9a414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d61c0-ef35-49fd-ad92-bea99bea8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54aa2a5-710e-46f5-8911-9c11ec91d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9a904-9c5d-4357-a948-4030d36bc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc4d74-3c7e-4ad7-ac86-b8661e9fe4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15e257-03d1-4028-8cf0-5117623cca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c927fc-f993-40b7-85d6-41a94ee58a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daf27d-ae44-497e-af18-99571086c455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d481e-f32e-4374-90b2-fd4558e3d5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef6e68-d6c9-49cf-924a-ca2e6af4e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a456d-1e99-48be-8b3f-8615ac00df0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "377ac2df-6c6a-4665-998b-5a15520afe5f",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee654cf1-39f3-4eb4-b53a-91f34986d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aebfb-f20f-4eb5-bc1f-5d3249475c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00804df-ffa5-46a4-9080-62c65e81a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc305283-c3d6-4f40-9385-f5075a7827af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c51bb-f659-454d-9108-14c72ae994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4f40c-8d91-4595-a92e-42f47de5c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dcee4-1d51-4b8f-a00a-12854420205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5668-7bf0-4634-9ff9-7bf8fc355b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
