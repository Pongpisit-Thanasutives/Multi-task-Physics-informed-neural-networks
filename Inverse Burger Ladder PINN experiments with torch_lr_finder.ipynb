{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC  # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 2000\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:, None]\n",
    "x = data['x'].flatten()[:, None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        print('Init using xavier')\n",
    "        self.model.apply(self.xavier_init)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def loss(self, data, y_input, include_unsup=False):\n",
    "        total_loss = []\n",
    "        \n",
    "        uf, unsup_loss = self.forward(data)\n",
    "        \n",
    "        total_loss.append(F.mse_loss(uf, y_input))\n",
    "        if include_unsup: # or if unsup_loss: ?, lets chk\n",
    "            total_loss.append(unsup_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        uf, _ = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0497dc39fa475caa92eac1a0a9b89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.70E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate finding')\n",
    "bs = 4000\n",
    "bs = N if bs>N else bs\n",
    "criterion = LadderLoss()\n",
    "tmp_optimizer = SGDGC(model.parameters(), lr=1e-7, use_gc=True, nesterov=True, momentum=0.9)\n",
    "trainloader = get_dataloader(X_u_train, u_train, bs=4000)\n",
    "lr_finder = LRFinder(model, optimizer=tmp_optimizer, criterion=criterion, device=\"cpu\")\n",
    "lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "_, suggested_lr = lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init using xavier\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "network = Network(model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(network.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=False)[0]\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the lookahead option\n",
      "1st Phase optimization using SGD/Adam with PCGrad gradient modification\n",
      "Epoch 0:  2.1012301445007324\n",
      "Epoch 10:  0.8478496074676514\n",
      "Epoch 20:  0.5352879762649536\n",
      "Epoch 30:  0.500372588634491\n",
      "Epoch 40:  0.48628103733062744\n",
      "Epoch 50:  0.47556111216545105\n",
      "Epoch 60:  0.4643290042877197\n",
      "Epoch 70:  0.4371986389160156\n",
      "Epoch 80:  0.38632768392562866\n",
      "Epoch 90:  0.34160158038139343\n",
      "Epoch 100:  0.33523377776145935\n",
      "Epoch 110:  0.33083048462867737\n",
      "Epoch 120:  0.3284667134284973\n",
      "Epoch 130:  0.32760608196258545\n",
      "Epoch 140:  0.3269312381744385\n",
      "Epoch 150:  0.3265267610549927\n",
      "Epoch 160:  0.3262159824371338\n",
      "Epoch 170:  0.32595470547676086\n",
      "Epoch 180:  0.32573333382606506\n",
      "Epoch 190:  0.3255413770675659\n",
      "Epoch 200:  0.32534563541412354\n",
      "Epoch 210:  0.32517942786216736\n",
      "Epoch 220:  0.32501786947250366\n",
      "Epoch 230:  0.32486218214035034\n",
      "Epoch 240:  0.3247106373310089\n",
      "Epoch 250:  0.3245622515678406\n",
      "Epoch 260:  0.32441291213035583\n",
      "Epoch 270:  0.3242732584476471\n",
      "Epoch 280:  0.3241284489631653\n",
      "Epoch 290:  0.3239840567111969\n",
      "Epoch 300:  0.32383406162261963\n",
      "Epoch 310:  0.32368484139442444\n",
      "Epoch 320:  0.3235248029232025\n",
      "Epoch 330:  0.32336798310279846\n",
      "Epoch 340:  0.32320284843444824\n",
      "Epoch 350:  0.3230254054069519\n",
      "Epoch 360:  0.3228415250778198\n",
      "Epoch 370:  0.3226432800292969\n",
      "Epoch 380:  0.32243940234184265\n",
      "Epoch 390:  0.32222265005111694\n",
      "Epoch 400:  0.32197660207748413\n",
      "Epoch 410:  0.32171884179115295\n",
      "Epoch 420:  0.3214438259601593\n",
      "Epoch 430:  0.3211146295070648\n",
      "Epoch 440:  0.3207379877567291\n",
      "Epoch 450:  0.3202844560146332\n",
      "Epoch 460:  0.3197009861469269\n",
      "Epoch 470:  0.3188701570034027\n",
      "Epoch 480:  0.31760555505752563\n",
      "Epoch 490:  0.3154023289680481\n",
      "Epoch 500:  0.3112751841545105\n",
      "Epoch 510:  0.30342334508895874\n",
      "Epoch 520:  0.2917258143424988\n",
      "Epoch 530:  0.3094642460346222\n",
      "Epoch 540:  0.2684014141559601\n",
      "Epoch 550:  0.2638707756996155\n",
      "Epoch 560:  0.258748859167099\n",
      "Epoch 570:  0.25499799847602844\n",
      "Epoch 580:  0.25266388058662415\n",
      "Epoch 590:  0.2506411671638489\n",
      "Epoch 600:  0.24881812930107117\n",
      "Epoch 610:  0.24727541208267212\n",
      "Epoch 620:  0.24587294459342957\n",
      "Epoch 630:  0.2445843517780304\n",
      "Epoch 640:  0.24340103566646576\n",
      "Epoch 650:  0.2422579824924469\n",
      "Epoch 660:  0.2411975860595703\n",
      "Epoch 670:  0.24018314480781555\n",
      "Epoch 680:  0.23920795321464539\n",
      "Epoch 690:  0.23825925588607788\n",
      "Epoch 700:  0.23737208545207977\n",
      "Epoch 710:  0.23647251725196838\n",
      "Epoch 720:  0.2356347292661667\n",
      "Epoch 730:  0.23480616509914398\n",
      "Epoch 740:  0.2340330332517624\n",
      "Epoch 750:  0.23327073454856873\n",
      "Epoch 760:  0.23253774642944336\n",
      "Epoch 770:  0.2318737506866455\n",
      "Epoch 780:  0.23118925094604492\n",
      "Epoch 790:  0.23055994510650635\n",
      "Epoch 800:  0.22996839880943298\n",
      "Epoch 810:  0.2293982356786728\n",
      "Epoch 820:  0.22885340452194214\n",
      "Epoch 830:  0.22835126519203186\n",
      "Epoch 840:  0.22786806523799896\n",
      "Epoch 850:  0.22740912437438965\n",
      "Epoch 860:  0.22696435451507568\n",
      "Epoch 870:  0.2265600860118866\n",
      "Epoch 880:  0.22617532312870026\n",
      "Epoch 890:  0.2257809340953827\n",
      "Epoch 900:  0.22543662786483765\n",
      "Epoch 910:  0.22508028149604797\n",
      "Epoch 920:  0.2247103750705719\n",
      "Epoch 930:  0.2243639975786209\n",
      "Epoch 940:  0.22403821349143982\n",
      "Epoch 950:  0.22373653948307037\n",
      "Epoch 960:  0.22341948747634888\n",
      "Epoch 970:  0.2231324315071106\n",
      "Epoch 980:  0.22286716103553772\n",
      "Epoch 990:  0.2226182520389557\n",
      "Epoch 999:  0.22236588597297668\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.018621811643242836\n",
      "Epoch 10:  1.617918132978957e-05\n",
      "Epoch 20:  1.9422159311943687e-06\n",
      "Epoch 30:  1.2330115168879274e-06\n",
      "Epoch 40:  9.298310601479898e-07\n",
      "Epoch 50:  8.824053452372027e-07\n",
      "Epoch 60:  8.216304081543058e-07\n",
      "Epoch 70:  8.213183946281788e-07\n",
      "Epoch 80:  8.213183946281788e-07\n",
      "Epoch 90:  8.213183946281788e-07\n",
      "Epoch 100:  8.213183946281788e-07\n",
      "Epoch 110:  8.213183946281788e-07\n",
      "Epoch 120:  8.213183946281788e-07\n",
      "Epoch 130:  8.213183946281788e-07\n",
      "Epoch 140:  8.213183946281788e-07\n",
      "Epoch 150:  8.213183946281788e-07\n",
      "Epoch 160:  8.213183946281788e-07\n",
      "Epoch 170:  8.213183946281788e-07\n",
      "Epoch 180:  8.213183946281788e-07\n",
      "Epoch 190:  8.213183946281788e-07\n",
      "Epoch 200:  8.213183946281788e-07\n",
      "Epoch 210:  8.213183946281788e-07\n",
      "Epoch 220:  8.213183946281788e-07\n",
      "Epoch 230:  8.213183946281788e-07\n",
      "Epoch 240:  8.213183946281788e-07\n",
      "Epoch 250:  8.213183946281788e-07\n",
      "Epoch 260:  8.213183946281788e-07\n",
      "Epoch 270:  8.213183946281788e-07\n",
      "Epoch 280:  8.213183946281788e-07\n",
      "Epoch 290:  8.213183946281788e-07\n",
      "Epoch 299:  8.213183946281788e-07\n"
     ]
    }
   ],
   "source": [
    "lookahead = False \n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=suggested_lr, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead:\n",
    "    print(\"Using the lookahead option\")\n",
    "    optimizer1 = Lookahead(optimizer1)\n",
    "else:\n",
    "    print(\"Not using the lookahead option\")\n",
    "    \n",
    "epochs1 = 1000 # How long this should be ??? (500 seems to be a good number.)\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGD/Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=80, max_eval=100, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0321635929576587e-06"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((network(X_star)[0].detach() - u_star)**2).mean().item()\n",
    "# BEST-full: 5.905130819883198e-07\n",
    "# BEST-2000: 2.4505434339516796e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
