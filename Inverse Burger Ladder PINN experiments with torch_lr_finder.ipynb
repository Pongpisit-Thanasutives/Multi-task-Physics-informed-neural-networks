{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01588b0a-2e22-4fc7-b853-2bdb0e7a02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC  # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f334226-14bc-4ba8-9f54-b313cfefdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1405080-bfff-4155-afdf-b5fe632bae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 2000\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:, None]\n",
    "x = data['x'].flatten()[:, None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0888bd-10cc-442b-9e97-9601fccc06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        print('Init using xavier')\n",
    "        self.model.apply(self.xavier_init)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def loss(self, data, y_input, include_unsup=False):\n",
    "        total_loss = []\n",
    "        \n",
    "        uf, unsup_loss = self.forward(data)\n",
    "        \n",
    "        total_loss.append(F.mse_loss(uf, y_input))\n",
    "        if include_unsup: # or if unsup_loss: ?, lets chk\n",
    "            total_loss.append(unsup_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        data = torch.cat([x, t], dim=-1)\n",
    "        uf, _ = self.forward(data)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f597beb8-deff-480c-8ec6-616bbe89f6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96666527-e474-4cb8-b116-5b4fbc57b240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b63c40b9624b2b814f30d4d1358dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.97E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate finding')\n",
    "bs = 4000\n",
    "bs = N if bs>N else bs\n",
    "criterion = LadderLoss()\n",
    "tmp_optimizer = SGDGC(model.parameters(), lr=1e-7, use_gc=True, nesterov=True, momentum=0.9)\n",
    "trainloader = get_dataloader(X_u_train, u_train, bs=4000)\n",
    "lr_finder = LRFinder(model, optimizer=tmp_optimizer, criterion=criterion, device=\"cpu\")\n",
    "lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "_, suggested_lr = lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ccfd3a-1353-43d1-a286-aeea221a50cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init using xavier\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "network = Network(model=model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973af118-178f-477e-b2a1-d42a9123a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(network.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "#         param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=False)[0]\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d31ae1-c3ae-4be7-9c0a-eb1279539ea5",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6797e381-a987-4838-817c-10c017d93c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the lookahead option\n",
      "1st Phase optimization using SGD/Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.7160319685935974\n",
      "Epoch 10:  0.5841265916824341\n",
      "Epoch 20:  0.5032790899276733\n",
      "Epoch 30:  0.5011416673660278\n",
      "Epoch 40:  0.48378312587738037\n",
      "Epoch 50:  0.4533221423625946\n",
      "Epoch 60:  0.3900904655456543\n",
      "Epoch 70:  0.3509014844894409\n",
      "Epoch 80:  0.34847292304039\n",
      "Epoch 90:  0.3429672122001648\n",
      "Epoch 100:  0.3414376974105835\n",
      "Epoch 110:  0.34009599685668945\n",
      "Epoch 120:  0.33908772468566895\n",
      "Epoch 130:  0.3381412625312805\n",
      "Epoch 140:  0.33713299036026\n",
      "Epoch 150:  0.3359459936618805\n",
      "Epoch 160:  0.33437633514404297\n",
      "Epoch 170:  0.33196237683296204\n",
      "Epoch 180:  0.32776692509651184\n",
      "Epoch 190:  0.32018330693244934\n",
      "Epoch 200:  0.3092633783817291\n",
      "Epoch 210:  0.2995707094669342\n",
      "Epoch 220:  0.2888162136077881\n",
      "Epoch 230:  0.27772507071495056\n",
      "Epoch 240:  0.2714713215827942\n",
      "Epoch 250:  0.2676730155944824\n",
      "Epoch 260:  0.26442527770996094\n",
      "Epoch 270:  0.2621324062347412\n",
      "Epoch 280:  0.260136216878891\n",
      "Epoch 290:  0.2583584189414978\n",
      "Epoch 300:  0.2567967474460602\n",
      "Epoch 310:  0.2553151845932007\n",
      "Epoch 320:  0.2539428174495697\n",
      "Epoch 330:  0.25263792276382446\n",
      "Epoch 340:  0.2513870596885681\n",
      "Epoch 350:  0.25020480155944824\n",
      "Epoch 360:  0.24902719259262085\n",
      "Epoch 370:  0.24789959192276\n",
      "Epoch 380:  0.2468116581439972\n",
      "Epoch 390:  0.2457711547613144\n",
      "Epoch 400:  0.24477016925811768\n",
      "Epoch 410:  0.24382591247558594\n",
      "Epoch 420:  0.24290494620800018\n",
      "Epoch 430:  0.24205201864242554\n",
      "Epoch 440:  0.24126404523849487\n",
      "Epoch 450:  0.24051377177238464\n",
      "Epoch 460:  0.23983612656593323\n",
      "Epoch 470:  0.23920652270317078\n",
      "Epoch 480:  0.23863384127616882\n",
      "Epoch 490:  0.2381465882062912\n",
      "Epoch 500:  0.23763920366764069\n",
      "Epoch 510:  0.2372356653213501\n",
      "Epoch 520:  0.23682382702827454\n",
      "Epoch 530:  0.23646140098571777\n",
      "Epoch 540:  0.2361413836479187\n",
      "Epoch 550:  0.23583343625068665\n",
      "Epoch 560:  0.23554956912994385\n",
      "Epoch 570:  0.23525077104568481\n",
      "Epoch 580:  0.2350328266620636\n",
      "Epoch 590:  0.23476329445838928\n",
      "Epoch 600:  0.2345360517501831\n",
      "Epoch 610:  0.2343364953994751\n",
      "Epoch 620:  0.23412710428237915\n",
      "Epoch 630:  0.23394912481307983\n",
      "Epoch 640:  0.2337484359741211\n",
      "Epoch 650:  0.2336125373840332\n",
      "Epoch 660:  0.23343525826931\n",
      "Epoch 670:  0.23334476351737976\n",
      "Epoch 680:  0.2331565022468567\n",
      "Epoch 690:  0.2330349087715149\n",
      "Epoch 700:  0.2329288125038147\n",
      "Epoch 710:  0.23281404376029968\n",
      "Epoch 720:  0.232692152261734\n",
      "Epoch 730:  0.23256349563598633\n",
      "Epoch 740:  0.23245322704315186\n",
      "Epoch 750:  0.23231284320354462\n",
      "Epoch 760:  0.23213693499565125\n",
      "Epoch 770:  0.23193475604057312\n",
      "Epoch 780:  0.2317172884941101\n",
      "Epoch 790:  0.23138833045959473\n",
      "Epoch 800:  0.23103806376457214\n",
      "Epoch 810:  0.2305978685617447\n",
      "Epoch 820:  0.23014190793037415\n",
      "Epoch 830:  0.22961929440498352\n",
      "Epoch 840:  0.22897475957870483\n",
      "Epoch 850:  0.22848746180534363\n",
      "Epoch 860:  0.22787190973758698\n",
      "Epoch 870:  0.22741428017616272\n",
      "Epoch 880:  0.22704938054084778\n",
      "Epoch 890:  0.2267051637172699\n",
      "Epoch 900:  0.2265230119228363\n",
      "Epoch 910:  0.22629907727241516\n",
      "Epoch 920:  0.2261040210723877\n",
      "Epoch 930:  0.22604337334632874\n",
      "Epoch 940:  0.22586381435394287\n",
      "Epoch 950:  0.2257930040359497\n",
      "Epoch 960:  0.22575393319129944\n",
      "Epoch 970:  0.22562086582183838\n",
      "Epoch 980:  0.22563603520393372\n",
      "Epoch 990:  0.2255631983280182\n",
      "Epoch 999:  0.22580662369728088\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.014392074197530746\n",
      "Epoch 10:  2.635919736349024e-05\n",
      "Epoch 20:  3.98295287595829e-06\n",
      "Epoch 30:  1.997875415327144e-06\n",
      "Epoch 40:  1.7689969808998285e-06\n",
      "Epoch 50:  1.6218950804613996e-06\n",
      "Epoch 60:  1.5381201592390426e-06\n",
      "Epoch 70:  1.4868953712721122e-06\n",
      "Epoch 80:  1.1953219427596196e-06\n",
      "Epoch 90:  1.143873873843404e-06\n",
      "Epoch 100:  1.1154539834024035e-06\n",
      "Epoch 110:  1.113085204451636e-06\n",
      "Epoch 120:  1.113085204451636e-06\n"
     ]
    }
   ],
   "source": [
    "lookahead = False \n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=suggested_lr, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead:\n",
    "    print(\"Using the lookahead option\")\n",
    "    optimizer1 = Lookahead(optimizer1)\n",
    "else:\n",
    "    print(\"Not using the lookahead option\")\n",
    "    \n",
    "epochs1 = 1000 # How long this should be ??? (500 seems to be a good number.)\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGD/Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=80, max_eval=100, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "cur_loss = 1e-6\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 100) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        \n",
    "        # To early stop from the loop\n",
    "        if cur_loss != l.item(): cur_loss = l.item()\n",
    "        else:\n",
    "            print(\"Duplicating training loss => Early stop\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8dcd3-04f5-400c-9ea1-6317eef36b94",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3802d3d9-a25d-4674-afd2-e0ef3176c2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.044602635782212e-06"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((network(X_star)[0].detach() - u_star)**2).mean().item()\n",
    "# BEST-full: 5.905130819883198e-07\n",
    "# BEST-2000: 2.4505434339516796e-06, 3.473501465123263e-06, 5.008192601962946e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "248d1dfc-6cb8-4a44-91bd-75cd6c5acea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uf', 'u_x', 'u_xx', 'u_tt', 'u_xt', 'u_tx'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Exporting derivatives and dynamics to .npy files ###\n",
    "\n",
    "grads_dict, dynamics = network.get_gradients_dict(*dimension_slicing(X_u_train))\n",
    "index2features = grads_dict.keys()\n",
    "derivatives = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "dynamics = torch.squeeze(dynamics).detach().numpy()\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-2000.npy\", derivatives)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-2000.npy\", dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eaf7dd-3eff-4799-bfbb-c5c3efd08c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64c7d5-7cfb-4d36-b99b-c3586ac44d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0add1f9-9cd7-4bdf-b5a1-55bf750a403c",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7dc94-8253-4ee6-9781-c17e2728b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07367905-4fd8-437c-a273-f7336fa04cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05d873-7549-446c-a74d-54d6aa9a414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d61c0-ef35-49fd-ad92-bea99bea8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54aa2a5-710e-46f5-8911-9c11ec91d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9a904-9c5d-4357-a948-4030d36bc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fc4d74-3c7e-4ad7-ac86-b8661e9fe4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db15e257-03d1-4028-8cf0-5117623cca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c927fc-f993-40b7-85d6-41a94ee58a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69daf27d-ae44-497e-af18-99571086c455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d481e-f32e-4374-90b2-fd4558e3d5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef6e68-d6c9-49cf-924a-ca2e6af4e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a456d-1e99-48be-8b3f-8615ac00df0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "377ac2df-6c6a-4665-998b-5a15520afe5f",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee654cf1-39f3-4eb4-b53a-91f34986d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aebfb-f20f-4eb5-bc1f-5d3249475c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00804df-ffa5-46a4-9080-62c65e81a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc305283-c3d6-4f40-9385-f5075a7827af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c51bb-f659-454d-9108-14c72ae994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4f40c-8d91-4595-a92e-42f47de5c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4dcee4-1d51-4b8f-a00a-12854420205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5668-7bf0-4634-9ff9-7bf8fc355b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
