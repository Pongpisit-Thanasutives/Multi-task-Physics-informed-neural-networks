{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import scipy\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from pysr import pysr, best, best_callable\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "from utils import *\n",
    "import pcgrad\n",
    "from ladder import LadderNetwork\n",
    "\n",
    "# AdamGC (Gradient centrailization) optimizer\n",
    "# Please also try learning finder. (Doesn't have to be included in the paper)\n",
    "from optimizers import Lookahead, AdamGC, SGDGC  # Not have to report Lookahead and GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples: 25600\n"
     ]
    }
   ],
   "source": [
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:, None]\n",
    "x = data['x'].flatten()[:, None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:, None], T.flatten()[:, None]))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 25600\n",
    "print('The number of training samples:', str(N))\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx, :]\n",
    "\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, lambda_1_init, lambda_2_init):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        print('Init using xavier')\n",
    "        self.model.apply(self.xavier_init)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def loss(self, data, y_input, include_unsup=False):\n",
    "        total_loss = []\n",
    "        \n",
    "        uf, unsup_loss = self.forward(data)\n",
    "        \n",
    "        total_loss.append(F.mse_loss(uf, y_input))\n",
    "        if unsup_loss: \n",
    "            total_loss.append(unsup_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def get_gradients_dict(self, x, t):\n",
    "        self.eval()\n",
    "        \n",
    "        uf, _ = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        return {'uf':uf, 'u_x':u_x, 'u_xx':u_xx, 'u_tt':u_tt, 'u_xt':u_xt, 'u_tx':u_tx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LadderNetwork(\n",
       "  (encoder): Encoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): LinearLayer(\n",
       "        (linear): Linear(in_features=2, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): LinearLayer(\n",
       "        (linear): Linear(in_features=50, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (stacked_layers): Sequential(\n",
       "      (layer_0): DecoderLayer(\n",
       "        (V): Linear(in_features=1, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_1): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_2): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_3): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (layer_4): DecoderLayer(\n",
       "        (V): Linear(in_features=50, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (bottom_decoder): DecoderLayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in, hidden_nodes, d_out = 2, 50, 1\n",
    "bias = True, True\n",
    "n_layers = 4\n",
    "activation_function = torch.tanh\n",
    "noise_std = 0.01\n",
    "\n",
    "model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                      d_out=d_out, bias=bias, activation_function=activation_function, \n",
    "                      noise_std=noise_std)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ab5f9df5a743ce883bd0439dec8911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 3.97E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEKCAYAAADXdbjqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAv6ElEQVR4nO3deXgV5fn/8fd9su+BJEQIW8CA7FtkB7GKIFpRq2gEd4lLwVotrbZa17b+vlq1WhdcEEUBETcELFg3BNmCiGyyCAghIGEJIWRP7t8f50hjhJCQTCbL/bquc5GZ88w595ND8snMMzOPqCrGGGNMdXjcLsAYY0z9Z2FijDGm2ixMjDHGVJuFiTHGmGqzMDHGGFNtFibGGGOqzd/tAmpKbGystm3b1u0yjDGmXlm1atV+VY2r7us0mDBp27YtaWlpbpdhjDH1ioj8UBOvY4e5jDHGVJuFiTHGmGqzMDHGGFNtDWbMxBhTdUVFRaSnp5Ofn+92KcZhwcHBtGzZkoCAAEde38LEmEYsPT2diIgI2rZti4i4XY5xiKpy4MAB0tPTSUxMdOQ97DCXMY1Yfn4+MTExFiQNnIgQExPj6B5oo98zUVW+23uEFtEhRAb7cyi3iB+z89l3pIDC4lI8AlEhAUSHBhAdGkhUSAABfp6fbZ+VW8SBowUUlyr+Hg/RoQG/aFf+PYtKFEUJ8HjweOwH2bjHgqRxcPpzbvRhcii3iPP/9SUAfh6hpPTk87uEB/kTFRKACOzLLqCwpPS47SKC/IkOCyA6JJDo0AAKikvZ5wuq3MKSY+/ZxBdUTUMDaRIWQNOwQCJDAjiSX0zmkQIyjxSQV1iCCESGBBATFkhMeCAxYUHH/o0I9icrr4iDOQUczC2ipLSUQD8/YsIDaRYRRLPIYOIjg4gND/pFGOYWluDvJwT6eewXi6mYKixfDnv2QPPm0K8fOPB/5qmnniI1NZXQ0NAaf+3KysrKYvr06dx222218n4/XSsXGxvLwIED+eqrr07pdaZOncp5551HixYtarjCijX6MAkO8PDvq3qRkZVHVm4RcRFBxEcGExcRRLC/HyWqZOcVcSi3kMN5RRw6WkRWXiGHc4soVSU+Kpj4iGBiwgMJ9PNQWFJ6rN2h3EKycgs5lFtEVm4hQQF+dE2IIj4ymCahAYgIeYUlHMwt5NDRQg7lFrJjfy5f78zicG4REcH+xEUE+WoKQhUO5xWxdV8Oy7d72x9vbjMR8Mjxg1EEmoYGEhrkx5H8YrLzivipWaC/hxZRwSQ0CSEhOoSE6FBaRHuXwwL9ySsqIb+ohFJVIoIDiI/wPudne1aNw/z5cPPNkJUFHg+UlkJ0NEyeDKNG1ehbPfXUU4wbN871MHnuueeqFSbFxcX4+1f91+ypBgl4w6Rr164WJrUtNNCfC7vX7je9phSXlHLId4jtSH4x0SHevZro0ED8PEJxSSn7cwrZdySffdkF7DtScOwQXl5hMZEhAUQGBxAe7E9JqTc0d2flsTsrj883ZbLvSMFJawj099AuNoz2ceG0jwujfbNw2sWGExHsj0cEjwfCAv2J9oWnqafmz4fLLoO8vJ+vz8nxrp89+5QC5ejRo4wZM4b09HRKSkq47777+PHHH8nIyODss88mNjaWzz77jIULF3L//fdTUFBA+/btefXVVwkPD2fVqlXceeed5OTkEBsby9SpU2nevDnDhg2jR48efPHFFxQXFzNlyhT69u3L0aNHmThxIuvWraOoqIgHHniA0aNHs379eq6//noKCwspLS3lnXfe4b777uP777+nZ8+eDB8+nMcee+xntT/88MO88cYbxMXF0apVK/r06cMf/vAHhg0bRs+ePVm8eDEpKSl06NCBRx55hMLCQmJiYnjzzTeJj4/nwIEDpKSksHv3bgYMGEDZWW/Dw8PJyckB4LHHHmPWrFkUFBRwySWX8OCDD7Jjxw7OP/98Bg8ezFdffUVCQgIffPAB8+bNIy0tjbFjxxISEsLSpUsJCQmp+ud9KlS1QTz69OmjpmblFxXr9swcXbwlU/+7Ya8u2ZKpaTsO6uqdh3TR5n361oqd+vd5G/TGqSt02GOfaeLdc7XNn47/OOPej/TCp7/UP81eo68v3aFf/3BQ8wqL3e5io7dhw4aTNyotVU1IUPUe5Dr+o2VLb7sqmj17tt50003HlrOyslRVtU2bNpqZmamqqpmZmTpkyBDNyclRVdVHH31UH3zwQS0sLNQBAwbovn37VFV15syZev3116uq6llnnXXsdb/44gvt0qWLqqrec889Om3aNFVVPXTokCYlJWlOTo5OmDBB33jjDVVVLSgo0NzcXN2+ffux7cpbsWKF9ujRQ/Py8jQ7O1tPP/10feyxx46996233nqs7cGDB7XU97156aWX9M4771RV1YkTJ+qDDz6oqqpz585V4Fifw8LCVFV1wYIFOn78eC0tLdWSkhK94IIL9IsvvtDt27ern5+frl69WlVVL7/88mP9Ouuss3TlypXHrft4nzeQpjXwO7jR75mYEwvy96NtbBhtY8Mq1b6guISdB3L5PvMouYXFlCqUqnIkv5iMrDw27T3Cf9bvZebKXQB4BLomRDGgfQwJ0SE0DQukaVggbWPCaB4VbHsydcXy5XD4cMVtsrJgxQrvGEoVdOvWjbvuuos//elPXHjhhQwZMuQXbZYtW8aGDRsYNGgQAIWFhQwYMIBNmzaxbt06hg8fDkBJSQnNmzc/tl1KSgoAQ4cOJTs7m6ysLBYuXMicOXN4/PHHAe/ZbDt37mTAgAH87W9/Iz09nUsvvZSkpKQK616yZAmjR48mODiY4OBgfv3rX//s+SuuuOLY1+np6VxxxRXs2bOHwsLCY6fmLlq0iHfffReACy64gCZNmvzifRYuXMjChQvp1asXADk5OWzZsoXWrVuTmJhIz549AejTpw87duyosGanWZiYGhPk70dSfARJ8REnbKOqZBzOZ93uw6zbfZhl2w7wypfbKS43vtMkNIAB7WMYmhTH0A5xtIiupV1180t79njHSCri8UBGRpVfukOHDnz99dfMnz+fe++9l3POOYe//vWvP2ujqgwfPpwZM2b8bP3atWvp0qULS5cuPe5rl/9jRERQVd555x06duz4s+c6depEv379mDdvHqNGjWLy5Mm0a9euyv35SVjY//4AmzhxInfeeScXXXQRn3/+OQ888EClX0dVueeee7j55pt/tn7Hjh0EBQUdW/bz8yOv/CHIWmbXmZhaJSIkRIcwostp3HVeR96+ZSAbHx7Jir+cw4I7hjL9pn48PLoL53SK5+sfsrj73bUMfPRTzn3iCx6eu4FVPxz82bFlUwuaN/cOtlektBROYcA3IyOD0NBQxo0bx6RJk/j6668BiIiI4MiRIwD079+fJUuWsHXrVsA7zrJ582Y6duxIZmbmsTApKipi/fr1x177rbfeAmDx4sVERUURFRXFiBEjeOaZZ479H1q9ejUA27Zto127dtx+++2MHj2ab7/99mc1lDdo0CA+/PBD8vPzycnJYe7cuSfs4+HDh0lISADgtddeO7Z+6NChTJ8+HYCPPvqIQ4cO/WLbESNGMGXKlGPjJ7t372bfvn0Vfk8rqttJtmdiXBfg56FZRDDNIoKBCAaeHsvVeP8q2/xjDos2Z7JoSybTlv3AK4u30zwqmG4JUXRpEUWXFpH0bdeUyGBnbhFh8B66ioryDrafSHQ09O1b5Zdeu3YtkyZNwuPxEBAQwPPPPw9AamoqI0eOpEWLFnz22WdMnTqVlJQUCgq8J4U88sgjdOjQgdmzZ3P77bdz+PBhiouLueOOO+jSpQvgvX1Ir169KCoqYsqUKQDcd9993HHHHXTv3p3S0lISExOZO3cus2bNYtq0aQQEBHDaaafx5z//maZNmzJo0CC6du3K+eef/7MB+DPPPJOLLrqI7t27Ex8fT7du3YiKijpuHx944AEuv/xymjRpwq9+9Su2b98OwP33309KSgpdunRh4MCBtG7d+hfbnnfeeWzcuJEBAwYA3oH5N954Az8/vxN+T6+77jpuueWWWh+Al4byV15ycrLafCYN25H8Ihas/5HPNu1jY0Y22w8cRRX8PcKA9jGMSW7FiC6nEehvO9yVtXHjRjp16nTyhic6mwsgJOSUz+ZyyrBhw3j88cdJTk527D1ycnIIDw8nNzeXoUOH8uKLL9K7d2/H3q8mHO/zFpFVqlrtb5TtmZh6IyI4gMv6tOSyPi0ByCkoZt3uw3y+KZMP12QwccZqYsICuSy5JSlntq70iQOmEkaN8gZGLV1nUh+kpqayYcMG8vPzufbaa+t8kDjN9kxMg1BaqizaksmMFTv578Z9lJQqQ5JiueWs9gxsb/eeOpFK75n8RNV71lZGhneMpG9fR66AN86wPRNjTsLjEYZ1bMawjs34MTufWSt3MW3ZD4x9eTldWkQyumcLLuzews4Kqy6RKp/+axoHO7hsGpz4yGAmnpPEoj+ezSMXd8XfI/x9/ncMfPRTxrywlI/W7qnUPdgai4ZydMJUzOnP2dEwEZGRIrJJRLaKyN3Heb61iHwmIqtF5FsRGeVb31ZE8kTkG9/jBSfrNA1TcIAf4/q34YMJg/li0jAmjejIj0fyufXNrxnx1CI+21TxKZaNQXBwMAcOHLBAaeDUN59JcHCwY+/h2JiJiPgBm4HhQDqwEkhR1Q1l2rwIrFbV50WkMzBfVduKSFtgrqp2rez72ZiJqYySUuU/6/by+MJNbN9/lGEd47j3gk6c3uzEF1o2ZDbTYuNxopkW68OYSV9gq6puAxCRmcBoYEOZNgpE+r6OAqp+Ca0xVeDnES7o3pzhneN5fekO/vXJFkY89SVX92/DHecmER0a6HaJtSogIMCxmfdM4+LkYa4EYFeZ5XTfurIeAMaJSDowH5hY5rlE3+GvL0TklzfsMaYaAv093DSkHZ//YRgpfVvx+tIdnP3458xZk2GHfIw5BW4PwKcAU1W1JTAKmCYiHmAP0FpVewF3AtNFJLL8xiKSKiJpIpKWmZlZq4WbhiEmPIhHLu7G/N8NoU1MGLfPWM01U1awcU+226UZU684GSa7gVZlllv61pV1IzALQFWXAsFArKoWqOoB3/pVwPdAh/JvoKovqmqyqibHxcU50AXTWJxxWiTv3DqQv17YmW/TDzPq6S/5w9tryMhy9+Z5xtQXTobJSiBJRBJFJBC4EphTrs1O4BwAEemEN0wyRSTON4CPiLQDkoBtDtZqDH4e4YbBiSyadDbjh7RjzjcZnPvEF7y+dAeldiqxMRVyLExUtRiYACwANgKzVHW9iDwkIhf5mt0FjBeRNcAM4DrfZC1DgW9F5BtgNnCLqh50qlZjyooKDeDPozrxyV1nkdy2KX/9YD3XTV3JoaOFbpdmTJ1lt1MxpgKqypvLd/LQhxuIiwjiubG96dEq2u2yjKkxNXVqsNsD8MbUaSLCuP5tmH2r9xbgl7+wlOnLd9oZX8aUY2FiTCV0bxnN3ImD6d8+hj+/t5ZJs78lv6jE7bKMqTMsTIyppCZhgbx63Zncfk4Ss1elc/kLS9mXbVeOGwMWJsZUiZ9HuHN4B16+JpnvM3O4+NklrNt92O2yjHGdhYkxp+DczvG8fcsAFLj0+a+YlbbrpNsY05BZmBhzirq0iGLuxMH0bduUP87+lnvetXEU03hZmBhTDTHhQbx2Q19+e3Z7ZqzYRcpLy8jKtetRTONjYWJMNfl5hEkjzuD5sb1ZvzubMZOXsvewDcybxsXCxJgacn635ky94UwysvL5zfNfsX3/UbdLMqbWWJgYU4MGto9lZmp/8otKuPyFr/hur9192DQOFibG1LCuCVG8fcsA/D0eUl5cxvoMO3XYNHwWJsY4oF1cOG/d3J/QQH+uemk5a9MtUEzDZmFijEPaxIQxM7U/EcH+XPXyMlbvPOR2ScY4xsLEGAe1ahrKWzcPoEloINe8soKvLVBMA2VhYozDEqJDmJnan6bh3kBZ9YMFiml4LEyMqQUtokN4K3UAcRFBXPPKctJ22FxvpmGxMDGmlpwWFczM1P7ERwZz7ZQVrLRAMQ2IhYkxtSg+0hcoUd5AWbHdAsU0DBYmxtSyZpHBzBzfn+ZRwVz36gqWbzvgdknGVJuFiTEuaBYZzIzU/rSIDuG6V1faGIqp9yxMjHFJs4hgpo/vR/OoYK5/daVNsmXqNQsTY1zULCKYaTf1IzIkgGumrGDrvhy3SzLmlFiYGOOyhOgQ3ripHx4Rxr28nF0Hc90uyZgqszAxpg5IjA1j2o19ySsqYdwry9mXbfOhmPrF0TARkZEisklEtorI3cd5vrWIfCYiq0XkWxEZVea5e3zbbRKREU7WaUxd0Kl5JFOvP5PMIwWMe2U5h47ajI2m/nAsTETED3gWOB/oDKSISOdyze4FZqlqL+BK4Dnftp19y12AkcBzvtczpkHr1boJL1+bzI4DuVz76gqO5Be5XZIxleLknklfYKuqblPVQmAmMLpcGwUifV9HARm+r0cDM1W1QFW3A1t9r2dMgzewfSzPXdWbDRnZ3PhaGrmFxW6XZMxJORkmCcCuMsvpvnVlPQCME5F0YD4wsQrbGtNgnds5niev6EnajoNc/+pKCxRT57k9AJ8CTFXVlsAoYJqIVLomEUkVkTQRScvMzHSsSGPc8OseLXjyip6s3HGQG6ZaoJi6zckw2Q20KrPc0reurBuBWQCquhQIBmIruS2q+qKqJqtqclxcXA2WbkzdMLpnAk9e0ZMV2w9y49Q08gpL3C7JmONyMkxWAkkikigigXgH1OeUa7MTOAdARDrhDZNMX7srRSRIRBKBJGCFg7UaU2eN7pnAE2N6snz7AW6YutICxdRJjoWJqhYDE4AFwEa8Z22tF5GHROQiX7O7gPEisgaYAVynXuvx7rFsAP4D/FZV7SfINFoX90rgn2N6sHz7AW58zQLF1D2iqm7XUCOSk5M1LS3N7TKMcdR7q9O5c9YaBraP4eVrziQk0M6YN9UjIqtUNbm6r+P2ALwxpgou6dWSf17eg6++P8D1U1eQU2CD8qZusDAxpp65tHdLnhzTk5U7DjH2pWV2pbypEyxMjKmHLu6VwORxfdi49whjJi9l72G7l5dxl4WJMfXUuZ3jee36vmRk5XHZC1/xw4GjbpdkGjELE2PqsQHtY5iR2p+jBcVc9sJSvtub7XZJppGyMDGmnuveMppZNw/AT4TLn1/Kl1vsbhCm9lmYGNMAJMVH8O5tA0lo4p1TfsaKnW6XZBoZCxNjGogW0SHMvnUgQ5Jiuefdtfxj/kZKShvGdWSm7rMwMaYBCQ/y5+Vrkrm6fxsmL9rGja+t5HCezYlinGdhYkwD4+/n4eGLu/K3S7qyeMt+Ln52CVt+POJ2WaaBszAxpoEa268NM1L7cyS/mIufXcLC9XvdLsk0YBYmxjRgZ7ZtyocTB9G+WTip01bx5MebbRzFOMLCxJgGrnlUCLNuHsBverfkX59s4dopK9h3xK6YNzXLwsSYRiA4wI/HL+/Oo5d2Y+WOg4z612IWb9nvdlmmAbEwMaaREBGu7NuaORMGEx0awNVTlvPPhZsoLil1uzTTAFiYGNPIdDwtgjkTBnFZ75Y88+lWrnppOemHct0uy9RzFibGNEKhgf48dnkPnhjTg/UZhxn51Je8nbaLhjJZnql9FibGNGKX9m7Jf+4YSucWkUya/S2p01axP6fA7bJMPWRhYkwj16ppKDPH9+cvozrxxaZMRj61iE82/uh2WaaesTAxxuDxCOOHtuPDiYOJDQ/ixtfS+Mt7a8kttGmBTeVYmBhjjul4WgQfTBhE6tB2TF+xkwueXszqnYfcLsvUAxYmxpifCfL348+jOvHmTf0oKCrhsheW8sTCTRTZKcSmAhYmxpjjGtg+lv/8fiije7bg6U+3culzX7F1n90w0hyfhYkx5oQigwN4YkxPnh/bm/RDuVzw9GJeWbzd7u9lfsHRMBGRkSKySUS2isjdx3n+SRH5xvfYLCJZZZ4rKfPcHCfrNMZU7PxuzVnw+6EMPj2Wh+du4PIXbC/F/Jw4dZGSiPgBm4HhQDqwEkhR1Q0naD8R6KWqN/iWc1Q1vLLvl5ycrGlpadUv3BhzQqrK+9/s5sEPN5BbUMLt55zOzWe1J8DPDnLUVyKySlWTq/s6Tv4P6AtsVdVtqloIzARGV9A+BZjhYD3GmGoSES7p1ZL/3nkWw7vE8/jCzVz2wlK27z/qdmnGZU6GSQKwq8xyum/dL4hIGyAR+LTM6mARSRORZSJysWNVGmOqLDY8iGev6s2/r+rFjv1HGfWvL5m+fKfdjqURqyv7plcCs1W1pMy6Nr5dr6uAp0SkffmNRCTVFzhpmZmZtVWrMcbnwu4tWHDHUPq0acKf31vL+NfT7HYsjZSTYbIbaFVmuaVv3fFcSblDXKq62/fvNuBzoFf5jVT1RVVNVtXkuLi4mqjZGFNFp0UF8/oNfbnvws4s2rKfkU8tsimCGyEnw2QlkCQiiSISiDcwfnFWloicATQBlpZZ10REgnxfxwKDgOMO3Btj3OfxCDcOTuTDCYNpFhFM6rRV3DFzNVm5hW6XZmqJY2GiqsXABGABsBGYparrReQhEbmoTNMrgZn684OtnYA0EVkDfAY8eqKzwIwxdUfH0yJ4/7eD+N05Scz9dg/Dn7SbRjYWjp0aXNvs1GBj6pZ1uw/zh7fX8N3eI1zaO4EHLupCZHCA22WZcurDqcHGmEasa0IUcyYMZuKvTueDbzK48OnFfJue5XZZxiEWJsYYxwT6e7jrvI7Murk/xSWl/Ob5r3h1yXY7hbgBsjAxxjiuT5umzLt9CGd1iOPBDzdw6xtfk51f5HZZpgZVKkxEJExEPL6vO4jIRSJiBz+NMZXWJCyQl65J5t4LOvHxxh8Z/e8lfLc32+2yTA2p7J7JIrxXpCcAC4GrgalOFWWMaZhEhJuGtGPG+P4cLSjm4meX8N7qdLfLMjWgsmEiqpoLXAo8p6qXA12cK8sY05D1TWzK3NsH06NlNL9/aw33vr+WguKSk29o6qxKh4mIDADGAvN86/ycKckY0xg0iwjmzZv6cfNZ7Xhj2U7GvLCU9EO5bpdlTlFlw+QO4B7gPd+Fh+3wXkxojDGnzN/Pwz3nd+KFcX3YlnmUC59ZzBeb7T579VGVL1r0DcSHq2qdGjmzixaNqd+27z/KrW+sYtOPR/jdOUnc/qskPB5xu6wGr1YvWhSR6SISKSJhwDpgg4hMqu6bG2PMTxJjw3jvtkFc0jOBp/67hYkzV5NfZOMo9UVlD3N19u2JXAx8hHfukaudKsoY0ziFBPrxzzE9uOf8M5j37R5SXlpmt7SvJyobJgG+60ouBuaoahFgl7AaY2qciHDzWe15fmxvNmRkc8lzS2y++XqgsmEyGdgBhAGLfDMj1qkxE2NMw3J+t+bMTO1PXmEJlz73FV9t3e92SaYClQoTVX1aVRNUdZR6/QCc7XBtxphGrlfrJrx32yDiI4O5ZsoK3l99ovn1jNsqOwAfJSJP/DRFroj8E+9eijHGOKpV01DeuW0gyW2bcMdb3/Dyl9vcLskcR2UPc00BjgBjfI9s4FWnijLGmLIigwN47Ya+jOp2Go/M28jf5m2gtNSGbesS/0q2a6+qvymz/KCIfONAPcYYc1xB/n48k9Kb2PD1vPTldvbnFPJ/l3UnwM9ufl4XVDZM8kRksKouBhCRQUCec2UZY8wv+XmEBy/qQnxkMI8t2MSBo4U8P7Y3YUGV/VVmnFLZSL8FeFZEdojIDuDfwM2OVWWMMScgIvz27NP5v990Z8nW/XYtSh1R2bO51qhqD6A70F1VewG/crQyY4ypwJgzWzF5XB827T3CZc9/xc4DdpNIN1XpYKOqZpe5J9edDtRjjDGVdm7neKaP78eh3CIuff4rNmTY5W9uqc7Ild2BzRjjuj5tmvLOrQMI8BOuenkZ6zMOu11So1SdMLHz8owxdcLpzSJ4K3UAoQF+jH15ue2huKDCMBGRIyKSfZzHEaBFLdVojDEn1TomlBmp/QkJ8GPsy8ssUGpZhWGiqhGqGnmcR4SqnvRcPBEZKSKbRGSriNx9nOefFJFvfI/NIpJV5rlrRWSL73HtKfXOGNOotIkJY2Zqf4J9gbJxjwVKbXHsah8R8QOeBc4HOgMpItK5bBtV/b2q9lTVnsAzwLu+bZsC9wP9gL7A/SLSxKlajTENR5uYMGaM70+Qv/eQ13d7LVBqg5OXjvYFtqrqNlUtBGYCoytonwLM8H09AvhYVQ+q6iHgY2Ckg7UaYxqQtrHePZRAPw9XvWSBUhucDJMEYFeZ5XTful/w3dI+Efi0qtsaY8zxtI0NY0Zqf+9ZXi8ttzlRHFZXbmpzJTBbVas0R6eIpP50J+PMzEyHSjPG1FeJsWHMTB2AR4RrXlnBnsN2FyinOBkmu4FWZZZb+tYdz5X87xBXpbdV1RdVNVlVk+Pi4qpZrjGmIUqMDWPq9WeSnV/M1a+sYN+RfLdLapCcDJOVQJKIJIpIIN7AmFO+kYicATQBlpZZvQA4T0Sa+Abez/OtM8aYKuuaEMVL1ySz+1AeV05eZnsoDnAsTFS1GJiANwQ2ArNUdb2IPCQiF5VpeiUwU1W1zLYHgYfxBtJK4CHfOmOMOSUD2sfw+o192XekgLEvL+fQ0UK3S2pQpMzv8HotOTlZ09LS3C7DGFPHrdh+kHGvLKdLi0im39SfkEA/t0tylYisUtXk6r5OXRmAN8aYWtE3sSlPX9mLNbuymDD9a4pLSt0uqUGwMDHGNDoju57GQ6O78sl3+/jze2tpKEdo3GTTkxljGqVx/duwLzufpz/dSpuYMH579ulul1SvWZgYYxqt3w/vwA8Hc3lswSbaxIRyYXe7f+2pssNcxphGS0T4f7/pTnKbJtw1aw2rdx5yu6R6y8LEGNOoBQf4MfnqPsRHBjP+9TR2HbTpf0+FhYkxptGLCQ9iynVnUlBcyo2vrSQ7v8jtkuodCxNjjAFObxbOC+P6sC3zKBOmr7ZThqvIwsQYY3wGnR7LIxd3ZdHmTB74cL2dMlwFdjaXMcaUcWXf1mw/cJTJX2yjXWw4NwxOdLukesHCxBhjyvnTiDP4YX8uD8/bQJuYUM7pFO92SXWeHeYyxphyPB7hySt60i0hiokzVrMhw2ZqPBkLE2OMOY6QQD9eviaZqJAAbnxtJT9m2zwoFbEwMcaYE2gWGcwr155Jdl4RN72WRn5RlSaDbVQsTIwxpgKdW0TydEov1u4+zIMfbnC7nDrLwsQYY07inE7x3DqsPTNW7GTOmgy3y6mTLEyMMaYS7hregeQ2TbjnnW/Zvv+o2+XUORYmxhhTCf5+Hp5O6UWAv4fb3vzaxk/KsTAxxphKahEdwhNjerBxTzaPzLPxk7IsTIwxpgp+dUY844ck8saynXy0do/b5dQZFibGGFNFk0acQY9W0fzxnW/tlvU+FibGGFNFgf4e/p3SC4AJM1ZTWGx3GLYwMcaYU9CqaSj/7zfdWbMri38u3OR2Oa6zMDHGmFM0qltzUvq25sUvt7Hqh4Nul+MqR8NEREaKyCYR2Soid5+gzRgR2SAi60Vkepn1JSLyje8xx8k6jTHmVP3lgk60iAph0uxvG/Xpwo6FiYj4Ac8C5wOdgRQR6VyuTRJwDzBIVbsAd5R5Ok9Ve/oeFzlVpzHGVEd4kD//uLQb2zKPNurDXU7umfQFtqrqNlUtBGYCo8u1GQ88q6qHAFR1n4P1GGOMI4Z2iOPq/m146cvtfLE50+1yXOFkmCQAu8osp/vWldUB6CAiS0RkmYiMLPNcsIik+dZf7GCdxhhTbX+5oBMd4yO4a9Y37M8pcLucWuf2ALw/kAQMA1KAl0Qk2vdcG1VNBq4CnhKR9uU3FpFUX+CkZWY2zr8GjDF1Q3CAH0+n9OJwXhF/n7fR7XJqnZNhshtoVWa5pW9dWenAHFUtUtXtwGa84YKq7vb9uw34HOhV/g1U9UVVTVbV5Li4uJrvgTHGVEHH0yK45az2vLt6N0u27ne7nFrlZJisBJJEJFFEAoErgfJnZb2Pd68EEYnFe9hrm4g0EZGgMusHAXYjHGNMnffbs0+nbUwo976/rlGd3eVYmKhqMTABWABsBGap6noReUhEfjo7awFwQEQ2AJ8Bk1T1ANAJSBORNb71j6qqhYkxps4LDvDjkYu7sX3/UZ77/Hu3y6k1oqpu11AjkpOTNS0tze0yjDEGgDtmrmbe2j189LuhnN4s3O1yTkhEVvnGp6vF7QF4Y4xpkO69sDMhAX7cP2cdDeWP9opYmBhjjANiw4O4c3gHlmw9wMcbfnS7HMdZmBhjjEPG9m9DUrNwHpm3kYLihj0Yb2FijDEOCfDz8Ndfd2bnwVymLN7hdjmOsjAxxhgHDUmK49xO8fz70y3sy853uxzHWJgYY4zD7r2gE4UlpfzfgoZ7I0gLE2OMcVjb2DBuGJzI7FXprNmV5XY5jrAwMcaYWjDh7NOJDQ/igQ/XN8hThS1MjDGmFkQEB/DHkR1ZvTOLD77JcLucGmdhYowxteSy3i3pmhDJYws2NbhThS1MjDGmlng8wh9HnMHurDxmrth18g3qEQsTY4ypRUOSYunfrinPfLqVowXFbpdTYyxMjDGmFokIfxx5BvtzCpi8aJvb5dQYCxNjjKllvVs34cLuzZn8xffszspzu5waYWFijDEuuGdUJwAe/eg7lyupGRYmxhjjgoToEMYPaceHazLYkJHtdjnVZmFijDEuGT+0HRHB/jz9yRa3S6k2CxNjjHFJVEgA1w9K5D/r97JxT/3eO7EwMcYYF904KJGIoPq/d2JhYowxLooKDeD6wYl8tK5+751YmBhjjMsawt6JhYkxxrgsKjSA6we15aN1e/lub/3cO7EwMcaYOuCGwfV778TCxBhj6oDo0ECuH9SW+Wvr596Jo2EiIiNFZJOIbBWRu0/QZoyIbBCR9SIyvcz6a0Vki+9xrZN1GmNMXXDD4ETCg/x55pOtbpdSZY6FiYj4Ac8C5wOdgRQR6VyuTRJwDzBIVbsAd/jWNwXuB/oBfYH7RaSJU7UaY0xd8NPeyby1e+rdmV1O7pn0Bbaq6jZVLQRmAqPLtRkPPKuqhwBUdZ9v/QjgY1U96HvuY2Ckg7UaY0ydcNPgdkQE+fOv/9avsRMnwyQBKDv7S7pvXVkdgA4iskRElonIyCpsa4wxDU5UaAA3DvFeFb8uPQuWLYP33vP+W4fnjvevA++fBAwDWgKLRKRbZTcWkVQgFaB169ZO1GeMMbXuhsGJbH71LVp0vwGKcsHjgdJSiI6GyZNh1Ci3S/wFJ/dMdgOtyiy39K0rKx2Yo6pFqrod2Iw3XCqzLar6oqomq2pyXFxcjRZvjDFuifz0Y/71zt9pemgf5ORAdrb33/R0uOwymD/f7RJ/wckwWQkkiUiiiAQCVwJzyrV5H+9eCSISi/ew1zZgAXCeiDTxDbyf51tnjDENmyqkphJQkH/85/Py4Oab69whL8fCRFWLgQl4Q2AjMEtV14vIQyJyka/ZAuCAiGwAPgMmqeoBVT0IPIw3kFYCD/nWGWNMw7Z8ORw+XHGbrCxYsaJWyqksR8dMVHU+ML/cur+W+VqBO32P8ttOAaY4WZ8xxtQ5e/Z4x0gq4vFARkbt1FNJdgW8McbUJc2bewfbK1JaCi1a1E49lWRhYowxdUm/fhAVVXGb6Gjo27dWyqksCxNjjKlLRODFFyEk5PjPh4R4Tw8Wqd26TsLCxBhj6ppRo2D2bGjZEsLD0chIjgaGsL9JM/Ttt+vkdSZuX7RojDHmeEaNgp07YcUKJCODz/fDb7cG8GbHfgxyu7bjsDAxxpi6SsQ7hgKcU1RC88c/5/GFmxjYPgbxHeba8uMRmoYFEhMe5GaldpjLGGPqg+AAP24/J4nVO7P478Z9x9bf98E6rnhxmYuVeVmYGGNMPXF5n5Ykxobx2ILvKClVdh3MZdm2g1zUw/3ThC1MjDGmnvD383Dn8A5s/jGHt1bu4v3V3lsWXtLL/Zuq25iJMcbUIxd0a86MFTt5eO4GokIC6N+uKa2ahrpdlu2ZGGNMfeLxCE9e0ZPgAA97s/O5tHdLt0sCLEyMMabeiY8M5umUXpzbqRmjujV3uxzADnMZY0y9NCQpjiFJdWceJ9szMcYYU20WJsYYY6rNwsQYY0y1WZgYY4ypNgsTY4wx1WZhYowxptosTIwxxlSbhYkxxphqE1V1u4YaISKZwA/VeIko4HA12lV2fUXLJ/o6FthfidoqUpn+VdSmLvevup9dRc9Vt3+19dmdrN3xnqvMOutf/e3fifpa/mcvTFWrf/WjqtrDG6gvVqddZddXtFzB12m10b+K2tTl/lX3s3Oyf7X12Z1K/yqzzvpXf/t3or7W9O+Wnx52mOt/Pqxmu8qur2j5RF/XhMq8XkVt6nL/qvvZVfRcQ+5fZdZZ/6rPrf6dqK81/bsFaECHuRoyEUlT1WS363BKQ+5fQ+4bWP/qu5rsn+2Z1A8vul2Awxpy/xpy38D6V9/VWP9sz8QYY0y12Z6JMcaYarMwMcYYU20WJsYYY6rNwqQeExGPiPxNRJ4RkWvdrqemicgwEflSRF4QkWFu1+MEEQkTkTQRudDtWmqaiHTyfXazReRWt+upaSJysYi8JCJvich5btdT00SknYi8IiKzK9PewsQlIjJFRPaJyLpy60eKyCYR2Soid5/kZUYDLYEiIN2pWk9FDfVPgRwgmIbZP4A/AbOcqfLU1UT/VHWjqt4CjAEGOVlvVdVQ/95X1fHALcAVTtZbVTXUv22qemOl39PO5nKHiAzF+4vydVXt6lvnB2wGhuP95bkSSAH8gH+Ue4kbfI9DqjpZRGar6mW1Vf/J1FD/9qtqqYjEA0+o6tjaqv9kaqh/PYAYvGG5X1Xn1k71J1cT/VPVfSJyEXArME1Vp9dW/SdTU/3zbfdP4E1V/bqWyj+pGu5fpX63+Ndc+aYqVHWRiLQtt7ovsFVVtwGIyExgtKr+A/jFYRARSQcKfYslDpZbZTXRvzIOAUGOFHqKaujzGwaEAZ2BPBGZr6qlTtZdWTX1+anqHGCOiMwD6kyY1NDnJ8CjwEd1KUigxn/+KsXCpG5JAHaVWU4H+lXQ/l3gGREZAixysrAaUqX+icilwAggGvi3o5XVjCr1T1X/AiAi1+HbC3O0uuqr6uc3DLgU7x8C850srIZU9edvInAuECUip6vqC04WVwOq+vnFAH8DeonIPb7QOSELk3pMVXOBSh/TrG9U9V28gdmgqepUt2twgqp+DnzuchmOUdWngafdrsMpqnoA73hQpdgAfN2yG2hVZrmlb11DYf2r36x/9Zuj/bMwqVtWAkkikigigcCVwByXa6pJ1r/6zfpXvznaPwsTl4jIDGAp0FFE0kXkRlUtBiYAC4CNwCxVXe9mnafK+mf9q8usfzXfPzs12BhjTLXZnokxxphqszAxxhhTbRYmxhhjqs3CxBhjTLVZmBhjjKk2CxNjjDHVZmFiGjwRyanl9/uqlt8vWkRuq833NKY8CxNjqkhEKrynnaoOrOX3jAYsTIyrLExMoyQi7UXkPyKySryzOZ7hW/9rEVkuIqtF5L++uVQQkQdEZJqILAGm+ZaniMjnIrJNRG4v89o5vn+H+Z6fLSLficibvtuWIyKjfOtWicjTIvKLuUxE5DoRmSMinwKfiEi4iHwiIl+LyFoRGe1r+ijQXkS+EZHHfNtOEpGVIvKtiDzo5PfSGLC7BpvG60XgFlXdIiL9gOeAXwGLgf6qqiJyE/BH4C7fNp2BwaqaJyIPAGcAZwMRwCYReV5Vi8q9Ty+gC5ABLAEGiUgaMBkYqqrbfbe+OJHeQHdVPejbO7lEVbNFJBZYJiJzgLuBrqraE0C8U8gm4Z2/QvDOJzJUVevDNAWmnrIwMY2OiIQDA4G3fTsK8L/Jt1oCb4lIcyAQ2F5m0zmqmldmeZ6qFgAFIrIPiOeX0wuvUNV03/t+A7TFOwPeNlX96bVnAKknKPdjVT34U+nA38U7i14p3vkp4o+zzXm+x2rfcjjecLEwMY6xMDGNkQfI+ukv+XKewTtF8Bzf5E4PlHnuaLm2BWW+LuH4P0+VaVORsu85FogD+qhqkYjswDvlb3kC/ENVJ1fxvYw5ZTZmYhodVc0GtovI5eCdflVEeviejuJ/czxc61AJm4B2ZaZVvaKS20UB+3xBcjbQxrf+CN5DbT9ZANzg2wNDRBJEpFn1yzbmxGzPxDQGoSJS9vDTE3j/yn9eRO4FAoCZwBq8eyJvi8gh4FMgsaaL8Y253Ab8R0SO4p1nojLeBD4UkbVAGvCd7/UOiMgSEVmHdz7ySSLSCVjqO4yXA4wD9tV0X4z5id2C3hgXiEi4qub4zu56Ftiiqk+6XZcxp8oOcxnjjvG+Afn1eA9f2fiGqddsz8QYY0y12Z6JMcaYarMwMcYYU20WJsYYY6rNwsQYY0y1WZgYY4ypNgsTY4wx1fb/AeX8KrE0kpS8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Learning rate finding')\n",
    "criterion = LadderLoss()\n",
    "tmp_optimizer = SGDGC(model.parameters(), lr=1e-7, use_gc=True, nesterov=True, momentum=0.9)\n",
    "trainloader = get_dataloader(X_u_train, u_train, bs=4000)\n",
    "lr_finder = LRFinder(model, optimizer=tmp_optimizer, criterion=criterion, device=\"cpu\")\n",
    "lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "_, suggested_lr = lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init using xavier\n"
     ]
    }
   ],
   "source": [
    "# Doesn't matter, can be anything.\n",
    "lambda_1_init = 0.0\n",
    "lambda_2_init = 0.0\n",
    "\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = network.loss(X_u_train, u_train, include_unsup=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in network.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(network.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx]).requires_grad_(True)\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    l = network.loss(X_u_train, u_train, include_unsup=False)[0]\n",
    "    if l.requires_grad:\n",
    "        l.backward()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy weights from network.model.encoder and build a new feedforward model!\n",
    "### Change a model architecture? (ResNet, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using the lookahead option\n",
      "1st Phase optimization using SGD/Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.8142675161361694\n",
      "Epoch 10:  0.5815798044204712\n",
      "Epoch 20:  0.4952782690525055\n",
      "Epoch 30:  0.49043548107147217\n",
      "Epoch 40:  0.47902101278305054\n",
      "Epoch 50:  0.4710279107093811\n",
      "Epoch 60:  0.4521389603614807\n",
      "Epoch 70:  0.4057096838951111\n",
      "Epoch 80:  0.34631744027137756\n",
      "Epoch 90:  0.3283379077911377\n",
      "Epoch 100:  0.32037219405174255\n",
      "Epoch 110:  0.30951255559921265\n",
      "Epoch 120:  0.2997129559516907\n",
      "Epoch 130:  0.2911298871040344\n",
      "Epoch 140:  0.28089380264282227\n",
      "Epoch 150:  0.27101579308509827\n",
      "Epoch 160:  0.26485371589660645\n",
      "Epoch 170:  0.26070165634155273\n",
      "Epoch 180:  0.25719940662384033\n",
      "Epoch 190:  0.2545551657676697\n",
      "Epoch 200:  0.2524283230304718\n",
      "Epoch 210:  0.25054705142974854\n",
      "Epoch 220:  0.24894201755523682\n",
      "Epoch 230:  0.24751614034175873\n",
      "Epoch 240:  0.24622085690498352\n",
      "Epoch 250:  0.24503739178180695\n",
      "Epoch 260:  0.2439347803592682\n",
      "Epoch 270:  0.24290482699871063\n",
      "Epoch 280:  0.24192242324352264\n",
      "Epoch 290:  0.24099402129650116\n",
      "Epoch 300:  0.24010945856571198\n",
      "Epoch 310:  0.23924757540225983\n",
      "Epoch 320:  0.23841895163059235\n",
      "Epoch 330:  0.23762169480323792\n",
      "Epoch 340:  0.23684458434581757\n",
      "Epoch 350:  0.23608867824077606\n",
      "Epoch 360:  0.235354483127594\n",
      "Epoch 370:  0.2346464842557907\n",
      "Epoch 380:  0.23395104706287384\n",
      "Epoch 390:  0.2332892119884491\n",
      "Epoch 400:  0.2326432466506958\n",
      "Epoch 410:  0.23203063011169434\n",
      "Epoch 420:  0.23144224286079407\n",
      "Epoch 430:  0.23088553547859192\n",
      "Epoch 440:  0.2303588092327118\n",
      "Epoch 450:  0.22986848652362823\n",
      "Epoch 460:  0.22941046953201294\n",
      "Epoch 470:  0.22898048162460327\n",
      "Epoch 480:  0.22858187556266785\n",
      "Epoch 490:  0.22821560502052307\n",
      "Epoch 500:  0.22787660360336304\n",
      "Epoch 510:  0.22755439579486847\n",
      "Epoch 520:  0.22726012766361237\n",
      "Epoch 530:  0.22698655724525452\n",
      "Epoch 540:  0.2267264872789383\n",
      "Epoch 550:  0.22648346424102783\n",
      "Epoch 560:  0.2262502908706665\n",
      "Epoch 570:  0.2260276973247528\n",
      "Epoch 580:  0.2258148491382599\n",
      "Epoch 590:  0.2256051003932953\n",
      "Epoch 600:  0.22539934515953064\n",
      "Epoch 610:  0.22520068287849426\n",
      "Epoch 620:  0.22499659657478333\n",
      "Epoch 630:  0.22480052709579468\n",
      "Epoch 640:  0.22458767890930176\n",
      "Epoch 650:  0.2243802398443222\n",
      "Epoch 660:  0.22416052222251892\n",
      "Epoch 670:  0.22392722964286804\n",
      "Epoch 680:  0.2236950695514679\n",
      "Epoch 690:  0.2234431952238083\n",
      "Epoch 700:  0.22318518161773682\n",
      "Epoch 710:  0.22292456030845642\n",
      "Epoch 720:  0.22267122566699982\n",
      "Epoch 730:  0.22241437435150146\n",
      "Epoch 740:  0.22216098010540009\n",
      "Epoch 750:  0.22192978858947754\n",
      "Epoch 760:  0.2216903567314148\n",
      "Epoch 770:  0.22148479521274567\n",
      "Epoch 780:  0.22128760814666748\n",
      "Epoch 790:  0.22109980881214142\n",
      "Epoch 800:  0.22093632817268372\n",
      "Epoch 810:  0.22077220678329468\n",
      "Epoch 820:  0.2206377387046814\n",
      "Epoch 830:  0.220509871840477\n",
      "Epoch 840:  0.22040125727653503\n",
      "Epoch 850:  0.22028590738773346\n",
      "Epoch 860:  0.22020265460014343\n",
      "Epoch 870:  0.22010700404644012\n",
      "Epoch 880:  0.2200358808040619\n",
      "Epoch 890:  0.21997031569480896\n",
      "Epoch 900:  0.2199121117591858\n",
      "Epoch 910:  0.2198624312877655\n",
      "Epoch 920:  0.21982616186141968\n",
      "Epoch 930:  0.21979279816150665\n",
      "Epoch 940:  0.21976780891418457\n",
      "Epoch 950:  0.2197471559047699\n",
      "Epoch 960:  0.21974460780620575\n",
      "Epoch 970:  0.21975074708461761\n",
      "Epoch 980:  0.21975308656692505\n",
      "Epoch 990:  0.21977737545967102\n",
      "Epoch 999:  0.2198726236820221\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.015829050913453102\n",
      "Epoch 10:  1.8889993953052908e-05\n",
      "Epoch 20:  2.02682144845312e-06\n",
      "Epoch 30:  1.417717726326373e-06\n",
      "Epoch 40:  1.2641116882150527e-06\n",
      "Epoch 50:  9.744420594870462e-07\n",
      "Epoch 60:  8.515083891325048e-07\n",
      "Epoch 70:  8.071123147601611e-07\n",
      "Epoch 80:  7.892443818491301e-07\n",
      "Epoch 90:  7.889057656029763e-07\n",
      "Epoch 100:  7.889057656029763e-07\n",
      "Epoch 110:  7.889057656029763e-07\n",
      "Epoch 120:  7.889057656029763e-07\n",
      "Epoch 130:  7.889057656029763e-07\n",
      "Epoch 140:  7.889057656029763e-07\n",
      "Epoch 150:  7.889057656029763e-07\n",
      "Epoch 160:  7.889057656029763e-07\n",
      "Epoch 170:  7.889057656029763e-07\n",
      "Epoch 180:  7.889057656029763e-07\n",
      "Epoch 190:  7.889057656029763e-07\n",
      "Epoch 200:  7.889057656029763e-07\n",
      "Epoch 210:  7.889057656029763e-07\n",
      "Epoch 220:  7.889057656029763e-07\n",
      "Epoch 230:  7.889057656029763e-07\n",
      "Epoch 240:  7.889057656029763e-07\n",
      "Epoch 250:  7.889057656029763e-07\n",
      "Epoch 260:  7.889057656029763e-07\n",
      "Epoch 270:  7.889057656029763e-07\n",
      "Epoch 280:  7.889057656029763e-07\n",
      "Epoch 290:  7.889057656029763e-07\n",
      "Epoch 299:  7.889057656029763e-07\n"
     ]
    }
   ],
   "source": [
    "lookahead = False \n",
    "\n",
    "# optimizer1 = torch.optim.Adam(network.parameters(), lr=5e-3, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "# optimizer1 = torch.optim.SGD(network.parameters(), lr=5e-3)\n",
    "optimizer1 = SGDGC(network.parameters(), lr=suggested_lr, use_gc=True, nesterov=True, momentum=0.9)\n",
    "if lookahead:\n",
    "    print(\"Using the lookahead option\")\n",
    "    optimizer1 = Lookahead(optimizer1)\n",
    "else:\n",
    "    print(\"Not using the lookahead option\")\n",
    "    \n",
    "epochs1 = 1000 # How long this should be ??? (500 seems to be a good number.)\n",
    "network.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using SGD/Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    \n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "\n",
    "if not bias[0]:\n",
    "    print('Adding encoder biases.')\n",
    "    # Loading weights to a new encoder model with biases\n",
    "    # The bias for decoder could be whatever you want, it doesn't matter.\n",
    "    model = LadderNetwork(d_in=d_in, hidden_dims=hidden_nodes, n_layers=n_layers,\n",
    "                          d_out=d_out, bias=(True, False), activation_function=activation_function, \n",
    "                          noise_std=noise_std)\n",
    "\n",
    "    # Reinit the biases as 0.01\n",
    "    model.load_state_dict(network.model.state_dict(), strict=False)\n",
    "\n",
    "    # delete the old one and create the new network\n",
    "    del network\n",
    "    network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "\n",
    "# 2nd Phase optimizer\n",
    "optimizer2 = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=80, max_eval=100, history_size=120, line_search_fn='strong_wolfe')\n",
    "epochs2 = 300\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the MSE loss comparing btw with & without the sparsity (Average the results from 5 evaluations?)\n",
    "### The better one would benefit the Symbolic regression process to recover PDE relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.889057656029763e-07"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((network(X_star)[0].detach() - u_star)**2).mean().item() # BEST: 5.905130819883198e-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precise pde parameters recovery using the PINN technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_1_init, lambda_2_init = network.get_theta(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "# network.set_lambdas(lambda_1_init, lambda_2_init)\n",
    "\n",
    "lambda_1_init = 0.6860763\n",
    "lambda_2_init = np.log(0.0020577204)\n",
    "\n",
    "### Choosing btw reset model weights or pretraining ###\n",
    "network = Network(model=model, lambda_1_init=lambda_1_init, lambda_2_init=lambda_2_init)\n",
    "optimizer = torch.optim.LBFGS(network.parameters(), lr=5e-2, max_iter=50, max_eval=50, line_search_fn='strong_wolfe')\n",
    "\n",
    "network.train(); best_train_loss = 1e6\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        l = network.loss(X_u_train[:, 0:1], X_u_train[:, 1:2], u_train, is_pde_parameters_update=True)\n",
    "        if l.requires_grad:\n",
    "            l.backward()\n",
    "        return l\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if i > 400 and float(l.item()) < best_train_loss:\n",
    "        torch.save(network.state_dict(), 'nn_with_physical_reg_from_symreg.pth')\n",
    "        best_train_loss = float(l.item())\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading the best weights ###\n",
    "network.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01 / np.pi\n",
    "\n",
    "error_lambda_1 = np.abs(network.lambda_1.detach().item() - 1.0)*100\n",
    "error_lambda_2 = np.abs(torch.exp(network.lambda_2).detach().item() - nu) / nu * 100\n",
    "\n",
    "error_lambda_1, error_lambda_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0, network.lambda_1.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu, torch.exp(network.lambda_2).detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_dict, target = network.get_gradients_dict(X_u_train[:, 0:1], X_u_train[:, 1:2])\n",
    "index2features = grads_dict.keys()\n",
    "print(index2features)\n",
    "\n",
    "G = torch.cat(list(grads_dict.values()), dim=1).detach().numpy()\n",
    "target = torch.squeeze(target).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equations = pysr(G, target, niterations=20, binary_operators=[\"plus\", \"sub\", \"mult\"], unary_operators=[], batching=True, procs=4, populations=10, npop=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the one with best score => might be overfitting (the lowest loss)\n",
    "print(best(equations))\n",
    "# fn = best_callable(equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = equations.drop(labels='lambda_format', axis=1)\n",
    "df.to_pickle('./saved_path_inverse_burger/equations_from_pysr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The one config that I used, and it was giving a good approx symbolic representation of the data. ###\n",
    "\n",
    "# (1)\n",
    "# est_gp = SymbolicRegressor(population_size=50000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=2)\n",
    "\n",
    "# (2)\n",
    "# est_gp = SymbolicRegressor(population_size=60000, generations=20, function_set=('add', 'sub', 'mul'),\n",
    "#                            p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "#                            p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "#                            verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "# const_range=(-1. float(G.shape[1])) ?\n",
    "\n",
    "### Current experiment ###\n",
    "est_gp = SymbolicRegressor(population_size=60000, generations=25, function_set=('add', 'sub', 'mul'),\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1, p_hoist_mutation=0.05,\n",
    "                           p_point_mutation=0.1, max_samples=0.9, parsimony_coefficient=0.001,\n",
    "                           verbose=1, low_memory=True, n_jobs=-1)\n",
    "\n",
    "est_gp.fit(G, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_exp\n",
    "program = est_gp._program\n",
    "print(build_exp(program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import pickle_save\n",
    "# pickle_save(est_gp, './data/gp_symreg_with_noisy_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exreacted equation (for further fine-tuning)\n",
    "# u_t + 0.6860763*uf*u_x - 0.0020577204*u_xx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
