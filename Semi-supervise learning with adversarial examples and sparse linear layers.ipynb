{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "from tqdm import trange\n",
    "from sparse_linear import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = N\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "# lb and ub are used in adversarial training\n",
    "lb = to_tensor(lb, False)\n",
    "ub = to_tensor(ub, False)\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchSparseMLP(dimensions=[X_train_dim, 100, 50, 1], sparsity=0.2, activation_function=nn.Tanh, bn=bn, dropout=None, inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e406d1e6c05949a6bf58af0f4a839a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 4.85E-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n"
     ]
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'Adam'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None\n",
    "    \n",
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "# suggested_lr = 1e-4\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "    \n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "        \n",
    "epochs1 = 2500; epochs2 = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = 500\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training the generator for \")\n",
    "# best_generator_loss = 1000; best_generator_state_dict = None\n",
    "# generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-5, momentum=0.95)\n",
    "# for i in trange(1000):\n",
    "#     semisup_model.eval()\n",
    "#     generator.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#     unsup_loss = semisup_model(X_gen)[1]\n",
    "#     generator_loss = sinkhorn_loss(X_gen, X_u_train)\n",
    "# #     generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "#     generator_loss.backward(retain_graph=True)\n",
    "#     generator_optimizer.step()\n",
    "    \n",
    "#     if generator_loss.item() < best_generator_loss:\n",
    "#         best_generator_loss = generator_loss.item()\n",
    "#         best_generator_state_dict = generator.state_dict()\n",
    "#         print(best_generator_loss)\n",
    "        \n",
    "#     if i%100==0:\n",
    "#         print(generator_loss.item())\n",
    "\n",
    "# print(\"The best generator loss:\", best_generator_loss)\n",
    "# generator.load_state_dict(best_generator_state_dict)\n",
    "# generator.eval()\n",
    "# X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 46/300 [00:53<04:56,  1.17s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4b7089cd201b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0munsup_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msemisup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_u_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munsup_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1013\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1014\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/samples_loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m# Run --------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         values = routines[self.loss][backend](\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mα\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py\u001b[0m in \u001b[0;36msinkhorn_tensorized\u001b[0;34m(α, x, β, y, p, blur, reach, diameter, scaling, cost, debias, potentials, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mε_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mρ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     a_x, b_y, a_y, b_x = sinkhorn_loop(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0msoftmin_tensorized\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mlog_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mα\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_divergence.py\u001b[0m in \u001b[0;36msinkhorn_loop\u001b[0;34m(softmin, α_logs, β_logs, C_xxs, C_yys, C_xys, C_yxs, ε_s, ρ, jumps, kernel_truncation, truncate, cost, extrapolate, debias, last_extrapolation)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdebias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mat_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_xx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mα_log\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(α,α)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mbt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_yy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ_log\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_y\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(β,β)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mat_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_yx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mα_log\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_x\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(α,β) wrt. a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mbt_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoftmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_xy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mβ_log\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ma_y\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# OT(α,β) wrt. b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/geomloss/sinkhorn_samples.py\u001b[0m in \u001b[0;36msoftmin_tensorized\u001b[0;34m(ε, C, f)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msoftmin_tensorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mε\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mε\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mε\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1000; best_generator_state_dict = None\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "            generator_loss = sinkhorn_loss(X_gen, X_u_train)-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        generator.load_state_dict(best_generator_state_dict)\n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "    \n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.000323352258419618\n",
      "Epoch 10:  1.0414360076538287e-06\n",
      "Epoch 20:  6.391771876224084e-07\n",
      "Epoch 30:  5.342531039786991e-07\n",
      "Epoch 40:  5.265144977784075e-07\n",
      "Epoch 50:  4.881994186689553e-07\n",
      "Finishing the second stage\n",
      "Testing\n",
      "Test MSE: 3.4343975130468607e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 10) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2366e-06, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "# semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_burger/running_exp.pth\"), strict=False)\n",
    "# semisup_model.eval()\n",
    "# F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-4000-V1-with-2000unlabledadversarialsamples.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-4000-V1-with-2000unlabledadversarialsamples.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-2000unlabledadversarialsamples.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-25600-V1-with-2000unlabledadversarialsamples.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
