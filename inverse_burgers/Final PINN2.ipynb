{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "from models import RobustPCANN\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "\n",
    "from pytorch_robust_pca import *\n",
    "\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange\n",
    "\n",
    "# Symbolics\n",
    "import sympy\n",
    "import sympytorch\n",
    "\n",
    "# BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt import Optimizer\n",
    "\n",
    "# hyperopt\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Exact\n",
      "Clean (x, t)\n",
      "Training with 2000 samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../experimental_data/burgers_shock.mat\"\n",
    "data = loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.0\n",
    "noisy_xt = False\n",
    "\n",
    "if noise_intensity>0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "else: print(\"Clean Exact\")\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "if noisy_xt and noise_intensity>0.0:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names, base on the symbolic regression results\n",
    "feature_names = ('uf', 'u_x', 'u_xx'); feature2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.998*u_x*uf + 0.0031*u_xx {u_xx, uf, u_x}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymPyModule(expressions=(-0.998*u_x*uf + 0.0031*u_xx,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noiseless program\n",
    "program = '''\n",
    "-0.9980*uf*u_x+0.0031*u_xx\n",
    "'''\n",
    "pde_expr, variables = build_exp(program); print(pde_expr, variables)\n",
    "mod = sympytorch.SymPyModule(expressions=[pde_expr]); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, pretrained=False, noiseless_mode=True, init_cs=(0.5, 0.5), init_betas=(0.0, 0.0)):\n",
    "        super(RobustPINN, self).__init__()\n",
    "        self.model = model\n",
    "        if not pretrained: self.model.apply(self.xavier_init)\n",
    "        \n",
    "        self.noiseless_mode = noiseless_mode\n",
    "        self.in_fft_nn = None; self.out_fft_nn = None\n",
    "        self.inp_rpca = None; self.out_rpca = None\n",
    "        if not self.noiseless_mode:\n",
    "            # FFTNN\n",
    "            self.in_fft_nn = FFTTh(c=init_cs[0])\n",
    "            self.out_fft_nn = FFTTh(c=init_cs[1])\n",
    "\n",
    "            # Robust Beta-PCA\n",
    "            self.inp_rpca = RobustPCANN(beta=0.0, is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "            self.out_rpca = RobustPCANN(beta=0.0, is_beta_trainable=True, inp_dims=1, hidden_dims=32)\n",
    "        \n",
    "#         self.callable_loss_fn = loss_fn\n",
    "        self.p0 = torch.log(list(loss_fn.parameters())[0])\n",
    "        self.p1 = list(loss_fn.parameters())[1]\n",
    "        \n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        H = torch.cat([x, t], dim=1)\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, X_input, X_input_noise, y_input, y_input_noise, update_network_params=True, update_pde_params=True):        \n",
    "        # Denoising process\n",
    "        if not self.noiseless_mode:\n",
    "            # (1) Denoising FFT on (x, t)\n",
    "            # This line returns the approx. recon.\n",
    "            X_input_noise = cat(torch.fft.ifft(self.in_fft_nn(X_input_noise[1])*X_input_noise[0]).real.reshape(-1, 1), \n",
    "                                torch.fft.ifft(self.in_fft_nn(X_input_noise[3])*X_input_noise[2]).real.reshape(-1, 1))\n",
    "            X_input_noise = X_input-X_input_noise\n",
    "            X_input = self.inp_rpca(X_input, X_input_noise, normalize=True)\n",
    "            \n",
    "            # (2)D enoising FFT on y_input\n",
    "            y_input_noise = y_input-torch.fft.ifft(self.out_fft_nn(y_input_noise[1])*y_input_noise[0]).real.reshape(-1, 1)\n",
    "            y_input = self.out_rpca(y_input, y_input_noise, normalize=True)\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(X_input[:, 0:1], X_input[:, 1:2])\n",
    "        \n",
    "        total_loss = []\n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            mse_loss = F.mse_loss(grads_dict[\"uf\"], y_input)\n",
    "            total_loss.append(mse_loss)\n",
    "            \n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            # l_eq = F.mse_loss(self.callable_loss_fn(**grads_dict).squeeze(-1), u_t)\n",
    "            l_eq = F.mse_loss((self.p0)*grads_dict[\"u_xx\"]+self.p1*grads_dict[\"uf\"]*grads_dict[\"u_x\"], u_t)\n",
    "            total_loss.append(l_eq)\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        \n",
    "        return {\"uf\":uf, \"u_x\":u_x, \"u_xx\":u_xx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp): \n",
    "        return -1.0+2.0*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], \n",
    "         activation_function=nn.Tanh, bn=None, # nn.LayerNorm\n",
    "         dropout=None)\n",
    "\n",
    "### TODO: How to load weights without using bn ###\n",
    "\n",
    "# Pretrained model\n",
    "semisup_model_state_dict = torch.load(\"./weights_nobn/semisup_model_nobn_2000_2000_finetuned.pth\")\n",
    "parameters = OrderedDict()\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "\n",
    "model.load_state_dict(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2226e-05, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(model(X_u_train), u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISELESS_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fft, x_PSD = fft1d_denoise(X_u_train[:, 0:1], c=-5, return_real=True)\n",
    "_, t_fft, t_PSD = fft1d_denoise(X_u_train[:, 1:2], c=-5, return_real=True)\n",
    "_, u_train_fft, u_train_PSD = fft1d_denoise(u_train, c=-5, return_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global NOISELESS_MODE\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global NOISELESS_MODE\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = updated_grads[0][idx]+updated_grads[1][idx]\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in noiseless mode.\n"
     ]
    }
   ],
   "source": [
    "if not NOISELESS_MODE:\n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=False)\n",
    "\n",
    "    def inference(args):\n",
    "        global pinn\n",
    "        c1, c2 = args\n",
    "        pinn.in_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c1)]), requires_grad=False)\n",
    "        pinn.out_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c2)]), requires_grad=False)\n",
    "        losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "        return sum(losses).item()\n",
    "\n",
    "    pinn.eval()\n",
    "    space = [hp.uniform('c1', 0, 1), hp.uniform('c2', 0, 1)]\n",
    "    res = fmin(fn=inference, space=space, algo=tpe.suggest, max_evals=200)\n",
    "\n",
    "    print(res)\n",
    "    if 'pinn' in globals(): del pinn\n",
    "\n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=False,\n",
    "                      init_cs=(res['c1'], res['c2']))\n",
    "    \n",
    "else: \n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=True)\n",
    "    print(\"You are in noiseless mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model's weights properly\n"
     ]
    }
   ],
   "source": [
    "pinn = load_weights(pinn, \"pinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn.p0 = nn.Parameter(data=(torch.tensor(0.0031)))\n",
    "pinn.p1 = nn.Parameter(data=(torch.tensor(-0.9980)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1, epochs2 = 10000, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  8.926064765546471e-05\n",
      "-0.9980010390281677 0.0030990000814199448\n",
      "Epoch 100:  1.1977813301200513e-05\n",
      "-0.9980091452598572 0.003092702478170395\n",
      "Epoch 200:  1.2105851965316106e-05\n",
      "-0.9980151057243347 0.0030927201732993126\n",
      "Epoch 300:  1.2645186870940961e-05\n",
      "-0.9980226159095764 0.0030934198293834925\n",
      "Epoch 400:  1.329218503087759e-05\n",
      "-0.9980345368385315 0.003094780258834362\n",
      "Epoch 500:  1.378819524688879e-05\n",
      "-0.9980511665344238 0.0030966929625719786\n",
      "Epoch 600:  1.4067552911001258e-05\n",
      "-0.9980708956718445 0.0030990384984761477\n",
      "Epoch 700:  1.4201335034158546e-05\n",
      "-0.9980947375297546 0.003101689973846078\n",
      "Epoch 800:  1.4278224625741132e-05\n",
      "-0.9981224536895752 0.0031045342329889536\n",
      "Epoch 900:  1.4346717762236949e-05\n",
      "-0.9981522560119629 0.003107505850493908\n",
      "Epoch 1000:  1.4422708773054183e-05\n",
      "-0.9981867671012878 0.003110583871603012\n",
      "Epoch 1100:  1.4518091120407917e-05\n",
      "-0.9982225298881531 0.0031137478072196245\n",
      "Epoch 1200:  1.4627801647293381e-05\n",
      "-0.9982637166976929 0.003116993233561516\n",
      "Epoch 1300:  1.4751736671314575e-05\n",
      "-0.9983054399490356 0.0031202936079353094\n",
      "Epoch 1400:  1.488167981733568e-05\n",
      "-0.9983527064323425 0.003123636357486248\n",
      "Epoch 1500:  1.5039242498460226e-05\n",
      "-0.9984003901481628 0.003126988187432289\n",
      "Epoch 1600:  1.5195128071354702e-05\n",
      "-0.9984529614448547 0.003130342811346054\n",
      "Epoch 1700:  1.5357851225417107e-05\n",
      "-0.9985066056251526 0.003133656457066536\n",
      "Epoch 1800:  1.544975930301007e-05\n",
      "-0.9985602498054504 0.0031368553172796965\n",
      "Epoch 1900:  1.5211779100354761e-05\n",
      "-0.9986174702644348 0.0031397987622767687\n",
      "Epoch 2000:  1.4986741007305682e-05\n",
      "-0.9986711144447327 0.003142127301543951\n",
      "Epoch 2100:  1.4729681424796581e-05\n",
      "-0.9987247586250305 0.003144293325021863\n",
      "Epoch 2200:  1.4526657651003916e-05\n",
      "-0.9987776875495911 0.003146190196275711\n",
      "Epoch 2300:  1.3994586879562121e-05\n",
      "-0.9988265633583069 0.0031478169839829206\n",
      "Epoch 2400:  1.3806368770019617e-05\n",
      "-0.9988712072372437 0.0031490144319832325\n",
      "Epoch 2500:  1.353152583760675e-05\n",
      "-0.998918890953064 0.003150360891595483\n",
      "Epoch 2600:  1.3264297194837127e-05\n",
      "-0.9989598989486694 0.0031513036228716373\n",
      "Epoch 2700:  1.25757478599553e-05\n",
      "-0.9989808797836304 0.0031512088607996702\n",
      "Epoch 2800:  1.2932152458233759e-05\n",
      "-0.999029815196991 0.003152539487928152\n",
      "Epoch 2900:  1.3085657883493695e-05\n",
      "-0.9990834593772888 0.0031539935152977705\n",
      "Epoch 3000:  1.3110886357026175e-05\n",
      "-0.9991371035575867 0.0031553583685308695\n",
      "Epoch 3100:  1.3051727364654653e-05\n",
      "-0.9991907477378845 0.003156611928716302\n",
      "Epoch 3200:  1.2920243534608744e-05\n",
      "-0.999244213104248 0.003157722996547818\n",
      "Epoch 3300:  1.290036288992269e-05\n",
      "-0.9992966651916504 0.0031587788835167885\n",
      "Epoch 3400:  1.288645034946967e-05\n",
      "-0.9993503093719482 0.0031598699279129505\n",
      "Epoch 3500:  1.2843767763115466e-05\n",
      "-0.9994039535522461 0.003160970052704215\n",
      "Epoch 3600:  1.2891450751340017e-05\n",
      "-0.999457597732544 0.0031620461959391832\n",
      "Epoch 3700:  1.2786199476977345e-05\n",
      "-0.9995112419128418 0.0031630622688680887\n",
      "Epoch 3800:  1.2763067388732452e-05\n",
      "-0.9995648860931396 0.003164016641676426\n",
      "Epoch 3900:  1.2659706044360064e-05\n",
      "-0.9996157288551331 0.0031648522708564997\n",
      "Epoch 4000:  1.2554114618978929e-05\n",
      "-0.9996645450592041 0.0031656469218432903\n",
      "Epoch 4100:  1.2283202522667125e-05\n",
      "-0.9997119307518005 0.003166384529322386\n",
      "Epoch 4200:  1.1635335795290302e-05\n",
      "-0.9997296333312988 0.003166376380249858\n",
      "Epoch 4300:  1.1690288374666125e-05\n",
      "-0.999748945236206 0.0031663936097174883\n",
      "Epoch 4400:  1.183629865408875e-05\n",
      "-0.9997803568840027 0.0031668071169406176\n",
      "Epoch 4500:  1.1943945537495892e-05\n",
      "-0.9998161196708679 0.003167362418025732\n",
      "Epoch 4600:  1.1926344086532481e-05\n",
      "-0.9998518824577332 0.003167875576764345\n",
      "Epoch 4700:  1.1969164916081354e-05\n",
      "-0.9998884797096252 0.0031683833803981543\n",
      "Epoch 4800:  1.1980583622062113e-05\n",
      "-0.9999242424964905 0.003168876515701413\n",
      "Epoch 4900:  1.2050014447595458e-05\n",
      "-0.9999648928642273 0.0031694392673671246\n",
      "Epoch 5000:  1.2025126125081442e-05\n",
      "-1.0000032186508179 0.003169964998960495\n",
      "Epoch 5100:  1.2041076843161136e-05\n",
      "-1.000038981437683 0.0031704402063041925\n",
      "Epoch 5200:  1.2187577340228017e-05\n",
      "-1.0000815391540527 0.0031710637267678976\n",
      "Epoch 5300:  1.2185071682324633e-05\n",
      "-1.0001174211502075 0.0031715782824903727\n",
      "Epoch 5400:  1.2247318409208674e-05\n",
      "-1.0001531839370728 0.003172106109559536\n",
      "Epoch 5500:  1.2349315511528403e-05\n",
      "-1.0001918077468872 0.0031726614106446505\n",
      "Epoch 5600:  1.2036566658935044e-05\n",
      "-1.0002236366271973 0.0031729773618280888\n",
      "Epoch 5700:  1.205519674840616e-05\n",
      "-1.0002565383911133 0.0031733824871480465\n",
      "Epoch 5800:  1.1822296073660254e-05\n",
      "-1.000280737876892 0.0031735121738165617\n",
      "Epoch 5900:  1.1842024832731113e-05\n",
      "-1.0003094673156738 0.003173845587298274\n",
      "Epoch 6000:  1.1643036486930214e-05\n",
      "-1.0003317594528198 0.0031740288250148296\n",
      "Epoch 6100:  1.1304990948701743e-05\n",
      "-1.0003471374511719 0.003174015786498785\n",
      "Epoch 6200:  1.110043467633659e-05\n",
      "-1.000355839729309 0.0031739696860313416\n",
      "Epoch 6300:  1.08199365058681e-05\n",
      "-1.0003653764724731 0.0031738081015646458\n",
      "Epoch 6400:  1.075235013558995e-05\n",
      "-1.0003688335418701 0.0031738942489027977\n",
      "Epoch 6500:  1.0970567927870434e-05\n",
      "-1.0003862380981445 0.0031742185819894075\n",
      "Epoch 6600:  1.1054954484279733e-05\n",
      "-1.0004048347473145 0.0031745545566082\n",
      "Epoch 6700:  1.110036282625515e-05\n",
      "-1.0004239082336426 0.003174916608259082\n",
      "Epoch 6800:  1.1140993592562154e-05\n",
      "-1.000439167022705 0.0031752376817166805\n",
      "Epoch 6900:  1.1058304153266363e-05\n",
      "-1.0004510879516602 0.0031754658557474613\n",
      "Epoch 7000:  1.0858047971851192e-05\n",
      "-1.0004605054855347 0.003175550140440464\n",
      "Epoch 7100:  1.0933362318610307e-05\n",
      "-1.0004719495773315 0.0031757436227053404\n",
      "Epoch 7200:  1.0754384675237816e-05\n",
      "-1.0004805326461792 0.0031758900731801987\n",
      "Epoch 7300:  1.057754343491979e-05\n",
      "-1.0004844665527344 0.0031758565455675125\n",
      "Epoch 7400:  1.0492608453205321e-05\n",
      "-1.0004913806915283 0.003175977151840925\n",
      "Epoch 7500:  1.022844844555948e-05\n",
      "-1.000490665435791 0.0031758672557771206\n",
      "Epoch 7600:  9.96372000372503e-06\n",
      "-1.000484824180603 0.0031757086981087923\n",
      "Epoch 7700:  9.852758921624627e-06\n",
      "-1.0004816055297852 0.0031757124233990908\n",
      "Epoch 7800:  9.970222890842706e-06\n",
      "-1.0004761219024658 0.003175480989739299\n",
      "Epoch 7900:  9.496578059042804e-06\n",
      "-1.0004730224609375 0.003175530582666397\n",
      "Epoch 8000:  9.688771569926757e-06\n",
      "-1.0004730224609375 0.0031756535172462463\n",
      "Epoch 8100:  9.820189916354138e-06\n",
      "-1.000479817390442 0.003175906604155898\n",
      "Epoch 8200:  9.95978552964516e-06\n",
      "-1.000491738319397 0.003176230238750577\n",
      "Epoch 8300:  1.0048808690044098e-05\n",
      "-1.000503659248352 0.003176567144691944\n",
      "Epoch 8400:  1.009897550829919e-05\n",
      "-1.0005155801773071 0.003176875878125429\n",
      "Epoch 8500:  1.0094957360706758e-05\n",
      "-1.0005266666412354 0.003177118953317404\n",
      "Epoch 8600:  1.0052672223537229e-05\n",
      "-1.0005292892456055 0.003177262842655182\n",
      "Epoch 8700:  9.93944559013471e-06\n",
      "-1.0005300045013428 0.0031773350201547146\n",
      "Epoch 8800:  9.803268767427653e-06\n",
      "-1.0005301237106323 0.003177397884428501\n",
      "Epoch 8900:  9.741892426973209e-06\n",
      "-1.0005297660827637 0.0031774030067026615\n",
      "Epoch 9000:  9.606061212252825e-06\n",
      "-1.0005297660827637 0.0031773915980011225\n",
      "Epoch 9100:  9.519788363832049e-06\n",
      "-1.0005251169204712 0.003177410224452615\n",
      "Epoch 9200:  9.264056643587537e-06\n",
      "-1.0005199909210205 0.0031772556249052286\n",
      "Epoch 9300:  8.975155651569366e-06\n",
      "-1.0005079507827759 0.003177082631736994\n",
      "Epoch 9400:  8.92837761057308e-06\n",
      "-1.0004997253417969 0.0031769750639796257\n",
      "Epoch 9500:  8.675159733684268e-06\n",
      "-1.0004900693893433 0.003176799276843667\n",
      "Epoch 9600:  8.861992682795972e-06\n",
      "-1.0004862546920776 0.003176859114319086\n",
      "Epoch 9700:  9.037726158567239e-06\n",
      "-1.0004972219467163 0.003177190199494362\n",
      "Epoch 9800:  9.039603355631698e-06\n",
      "-1.0004987716674805 0.003177458653226495\n",
      "Epoch 9900:  8.946749403548893e-06\n",
      "-1.000500202178955 0.0031775913666933775\n",
      "Epoch 9999:  8.911359145713504e-06\n",
      "-1.0005000829696655 0.003177677746862173\n"
     ]
    }
   ],
   "source": [
    "# optimizer1 = MADGRAD(pinn.parameters(), lr=1e-7, momentum=0.95)\n",
    "optimizer1 = AdamGC(pinn.parameters(), lr=1e-6, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 100) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(float(pinn.p1.detach().numpy()), float((pinn.p0.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  3.71758574146952e-06\n",
      "Epoch 10:  3.71758574146952e-06\n",
      "Epoch 20:  3.71758574146952e-06\n",
      "Epoch 30:  3.71758574146952e-06\n",
      "Epoch 40:  3.71758574146952e-06\n",
      "Epoch 49:  3.71758574146952e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=500, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.00022292137146, 0.0031830661464482546)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(pinn.p1.detach().numpy()), float((pinn.p0.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011659960711956706, 0.01063217643403939)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const = 0.01/np.pi\n",
    "errs = 100*np.abs(npar([(1.00022292137146-1), (const-0.0031830661464482546)/const]))\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(pinn, \"pinn2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
