{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n",
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "# Model selection\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Exact\n",
      "Training with 2000 samples\n",
      "Clean (x, t)\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../experimental_data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.0\n",
    "noisy_xt = False\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "if noise_intensity>0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "else: print(\"Clean Exact\")\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000; include_unsup = True\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "if noisy_xt and noise_intensity>0.0:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_u_train = perturb(X_u_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Unsup data\n",
    "N_res = N\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "if include_unsup:\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        \n",
    "        return cat(uf, u_x, u_xx, u_xxx), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=1e-3):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])-(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 3.0])\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn, bi=False, th=False):\n",
    "        if bi: att = binarize(self.weighted_features(inn), self.th)\n",
    "        elif th: att = F.threshold(self.weighted_features(inn), self.th, 0.0)\n",
    "        else: att = self.weighted_features(inn)\n",
    "        return self.nonlinear_model(inn*att)\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input, bi=False, th=False)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        \n",
    "        l1 = mse_loss\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        \n",
    "        return l1+self.reg_intensity*(l2)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load('../saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained250labeledsamples_trained0unlabeledsamples_2.2e-03.pth')\n",
    "pretrained_state_dict = torch.load(\"./weights_nobn/pretrained_nobn.pth\")\n",
    "network_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=None, dropout=None),\n",
    "                                    index2features=feature_names),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=False,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "    network_state_dict = semisup_model.network.state_dict()\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining...\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.train()\n",
    "    for i in range(200):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        if (i % 100) == 0:\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9360, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.7256, 0.2138, 0.2376, 0.2344], grad_fn=<MeanBackward1>)\n",
      "tensor(0.9355, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.7189, 0.2138, 0.2394, 0.2329], grad_fn=<MeanBackward1>)\n",
      "tensor(0.9332, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.7554, 0.2138, 0.2275, 0.2208], grad_fn=<MeanBackward1>)\n",
      "tensor(0.9359, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.7886, 0.2138, 0.2392, 0.2423], grad_fn=<MeanBackward1>)\n",
      "tensor(0.9348, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.7839, 0.2138, 0.2394, 0.2489], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "f_opt = LBFGSNew(semisup_model.selector.parameters(), \n",
    "                 lr=1e-1, max_iter=300, \n",
    "                 max_eval=int(300*1.25), history_size=300, \n",
    "                 line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=500, max_eval=int(1.25*500), history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = F.mse_loss(semisup_model.selector(X_selector, bi=False, th=True), y_selector)\n",
    "#     loss = semisup_model.selector.loss(X_selector, y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "best_loss = 1000; best_att = None\n",
    "\n",
    "max_it = 5\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%1==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        att = semisup_model.selector.latest_weighted_features\n",
    "        print(loss); print(att)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_att = att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    predictions, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(predictions[:N, :], u_train[:N, :]), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(2.5465e-06, grad_fn=<MseLossBackward0>), tensor(0.9140, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7835, 0.2140, 0.2395, 0.2491], grad_fn=<MeanBackward1>)\n",
      "tensor([1, 2, 3, 0])\n",
      "[tensor(0.0001, grad_fn=<MseLossBackward0>), tensor(0.4164, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7934, 0.2152, 0.2514, 0.2187], grad_fn=<MeanBackward1>)\n",
      "tensor([1, 3, 2, 0])\n",
      "[tensor(0.0002, grad_fn=<MseLossBackward0>), tensor(0.2171, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8048, 0.2377, 0.2461, 0.1939], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 1, 2, 0])\n",
      "[tensor(0.0003, grad_fn=<MseLossBackward0>), tensor(0.1499, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8151, 0.3482, 0.2466, 0.1795], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.1208, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8263, 0.5318, 0.2471, 0.1672], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.1063, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8318, 0.5984, 0.2484, 0.1607], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0919, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8413, 0.6454, 0.2494, 0.1516], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0834, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8432, 0.6984, 0.2526, 0.1496], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0784, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8434, 0.7339, 0.2539, 0.1489], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0702, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8429, 0.7549, 0.2545, 0.1486], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0665, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8422, 0.7659, 0.2574, 0.1485], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0004, grad_fn=<MseLossBackward0>), tensor(0.0606, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8420, 0.7735, 0.2555, 0.1475], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0005, grad_fn=<MseLossBackward0>), tensor(0.0572, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8429, 0.7791, 0.2524, 0.1461], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0505, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8478, 0.7877, 0.2516, 0.1408], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0483, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8547, 0.7979, 0.2498, 0.1338], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0445, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8591, 0.8019, 0.2470, 0.1287], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0436, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8624, 0.8067, 0.2435, 0.1249], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0424, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8634, 0.8091, 0.2417, 0.1229], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0398, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8665, 0.8138, 0.2420, 0.1193], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0404, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8684, 0.8171, 0.2386, 0.1171], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0402, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8682, 0.8193, 0.2382, 0.1169], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0396, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8680, 0.8207, 0.2346, 0.1164], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0369, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8670, 0.8228, 0.2328, 0.1168], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0360, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8684, 0.8255, 0.2295, 0.1151], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0006, grad_fn=<MseLossBackward0>), tensor(0.0358, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8665, 0.8251, 0.2298, 0.1164], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0367, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8665, 0.8273, 0.2288, 0.1160], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0350, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8663, 0.8289, 0.2289, 0.1155], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0332, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8678, 0.8305, 0.2275, 0.1132], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0332, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8675, 0.8306, 0.2263, 0.1132], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0341, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8682, 0.8327, 0.2247, 0.1120], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0320, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8703, 0.8349, 0.2269, 0.1097], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0326, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8703, 0.8362, 0.2267, 0.1093], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0309, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8704, 0.8378, 0.2241, 0.1087], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0305, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8679, 0.8360, 0.2263, 0.1107], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0322, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8695, 0.8376, 0.2241, 0.1086], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0296, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8710, 0.8405, 0.2238, 0.1069], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0306, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8720, 0.8420, 0.2251, 0.1060], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0291, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8725, 0.8431, 0.2253, 0.1053], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0296, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8721, 0.8436, 0.2257, 0.1055], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0294, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8731, 0.8451, 0.2283, 0.1046], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0277, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8731, 0.8455, 0.2274, 0.1038], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0277, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8728, 0.8460, 0.2252, 0.1037], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0007, grad_fn=<MseLossBackward0>), tensor(0.0269, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8724, 0.8466, 0.2268, 0.1039], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0291, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8729, 0.8470, 0.2284, 0.1035], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0283, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8755, 0.8491, 0.2304, 0.1013], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0259, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8753, 0.8502, 0.2285, 0.1009], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0259, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8754, 0.8513, 0.2308, 0.1004], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0254, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8765, 0.8530, 0.2307, 0.0993], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0258, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8764, 0.8544, 0.2328, 0.0991], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0245, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8785, 0.8558, 0.2332, 0.0972], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0245, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8776, 0.8566, 0.2348, 0.0971], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0241, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8793, 0.8579, 0.2363, 0.0964], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0233, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8787, 0.8586, 0.2378, 0.0961], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0253, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8785, 0.8591, 0.2422, 0.0957], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0255, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8791, 0.8598, 0.2591, 0.0959], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0249, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8783, 0.8604, 0.2572, 0.0961], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0239, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8800, 0.8626, 0.2546, 0.0950], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0245, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8803, 0.8635, 0.2623, 0.0936], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0243, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8824, 0.8657, 0.2700, 0.0923], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0236, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8818, 0.8664, 0.2699, 0.0923], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0230, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8831, 0.8685, 0.2698, 0.0913], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0214, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8825, 0.8689, 0.2679, 0.0912], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0214, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8834, 0.8699, 0.2718, 0.0907], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0233, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8843, 0.8713, 0.2806, 0.0901], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0210, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8845, 0.8726, 0.2894, 0.0893], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0008, grad_fn=<MseLossBackward0>), tensor(0.0211, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8835, 0.8717, 0.2712, 0.0902], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0200, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8846, 0.8735, 0.2910, 0.0896], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0226, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8859, 0.8754, 0.2793, 0.0883], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0206, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8827, 0.8723, 0.2939, 0.0922], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0262, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8841, 0.8749, 0.3056, 0.0903], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0010, grad_fn=<MseLossBackward0>), tensor(0.0209, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8851, 0.8755, 0.2934, 0.0911], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0010, grad_fn=<MseLossBackward0>), tensor(0.0210, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8870, 0.8784, 0.2981, 0.0877], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0199, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8902, 0.8807, 0.2898, 0.0870], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0182, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8899, 0.8811, 0.2775, 0.0863], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0216, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8903, 0.8816, 0.2767, 0.0860], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0226, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8923, 0.8835, 0.3047, 0.0852], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0010, grad_fn=<MseLossBackward0>), tensor(0.0200, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8918, 0.8837, 0.3029, 0.0853], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0010, grad_fn=<MseLossBackward0>), tensor(0.0187, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8915, 0.8836, 0.2802, 0.0853], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0194, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8928, 0.8846, 0.2841, 0.0846], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0009, grad_fn=<MseLossBackward0>), tensor(0.0182, grad_fn=<AddBackward0>)]\n",
      "tensor([0.8928, 0.8851, 0.2910, 0.0845], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Joint training | Do lambda comparison here\n",
    "optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "optimizer.param_groups[0]['lr'] = 1e-7\n",
    "optimizer.param_groups[1]['lr'] = 1e-2\n",
    "\n",
    "# Use ~idx to sample adversarial data points\n",
    "for i in range(2000):\n",
    "    semisup_model.train()\n",
    "    optimizer.step(pcgrad_closure)\n",
    "    if i%25==0:\n",
    "        loss = pcgrad_closure(return_list=True); print(loss)\n",
    "        fi = semisup_model.selector.latest_weighted_features\n",
    "        print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5071652721162536e-07\n",
      "4.297066027447727e-07\n",
      "4.293218580642133e-07\n",
      "4.293218580642133e-07\n",
      "4.293218580642133e-07\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(50):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train[:N, :]))\n",
    "semisup_model.selector.weighted_features(X_selector)\n",
    "semisup_model.selector.latest_weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.reg_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1477, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n",
      "tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "f_opt = LBFGSNew(semisup_model.selector.parameters(), \n",
    "                 lr=5e-2, max_iter=300, \n",
    "                 max_eval=int(300*1.25), history_size=300, \n",
    "                 line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=500, max_eval=int(1.25*500), history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = F.mse_loss(semisup_model.selector(X_selector, bi=True), y_selector)\n",
    "#     loss = semisup_model.selector.loss(X_selector, y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "best_loss = 1000; best_att = None\n",
    "\n",
    "max_it = 10\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%1==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        att = semisup_model.selector.latest_weighted_features\n",
    "        print(loss); print(att)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_att = att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.8588, 0.8457, 0.4146, 0.0906], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "print(att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = min(50000, X_star.shape[0])\n",
    "idx_test = np.arange(n_test)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.002697 +0.000000i)u_xx\n",
      "    + (-0.913424 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Set normalize=1\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, d_tol=10)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"semisup_model_nobn_2000_2000_finetuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No selector network (Not the same as setting lambda_1 to 0.0) \n",
    "# varying lambda_{1} | lambda_{0} = 1e-5 or 1e-6 (the same results anyway)\n",
    "# PDE derived using STRidge\n",
    "# u_t = (0.002543 +0.000000i)u_xx\n",
    "#     + (-0.832252 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-1\n",
    "# tensor([0.2735, 0.1883, 0.2060, 0.2133])\n",
    "# u_t = (0.002695 +0.000000i)u_xx\n",
    "#     + (-0.924525 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-3\n",
    "# tensor([0.8153, 0.8142, 0.4398, 0.1562])\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-5\n",
    "# tensor([0.6721, 0.7903, 0.7139, 0.3486])\n",
    "# u_t = (0.002594 +0.000000i)u_xx\n",
    "#     + (-0.885143 +0.000000i)uf*u_x\n",
    "\n",
    "# varying lambda_{0} | lambda_{1} = 1e-3\n",
    "# 1e-1\n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.066179 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-3\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-5\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goals!!!\n",
    "# 3000 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "# 1000 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "# 500 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "\n",
    "# 3000\n",
    "# eq found by STR \n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "# eq after PINN fine-tuning + mean error\n",
    "# (-0.9994307160377502, 0.0031862353649783444)\n",
    "# (0.07773227423228249, )\n",
    "\n",
    "# 1000\n",
    "# eq found by STR\n",
    "# u_t = (0.002631 +0.000000i)u_xx\n",
    "#     + (-0.860374 +0.000000i)uf*u_x\n",
    "# eq after PINN fine-tuning + mean error\n",
    "# (-0.9891335368156433, 0.0031485233921557665)\n",
    "# (1.0864333669569826, 0.0002129514786863851)\n",
    "\n",
    "# 500\n",
    "# eq found by STR (failed)\n",
    "# u_t = (-0.072116 +0.000000i)uf*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
