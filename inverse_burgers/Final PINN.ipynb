{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "from models import RobustPCANN\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "\n",
    "from pytorch_robust_pca import *\n",
    "\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange\n",
    "\n",
    "# Symbolics\n",
    "import sympy\n",
    "import sympytorch\n",
    "\n",
    "# BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt import Optimizer\n",
    "\n",
    "# hyperopt\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Exact\n",
      "Clean (x, t)\n",
      "Training with 1000 samples\n",
      "โหลดทับ\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../experimental_data/burgers_shock.mat\"\n",
    "data = loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.0\n",
    "noisy_xt = False\n",
    "sub = True\n",
    "\n",
    "if noise_intensity>0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "else: print(\"Clean Exact\")\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "if noisy_xt and noise_intensity>0.0:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 1000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "if sub:\n",
    "    print(\"โหลดทับ\")\n",
    "    X_u_train = np.load(\"./data_files/X_u_train_1000labeledsamples.npy\")[:N, :]\n",
    "    u_train = np.load(\"./data_files/u_train_1000labeledsamples.npy\")\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names, base on the symbolic regression results\n",
    "feature_names = ('uf', 'u_x', 'u_xx'); feature2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.860374*u_x*uf + 0.002631*u_xx {uf, u_x, u_xx}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymPyModule(expressions=(-0.860374*u_x*uf + 0.002631*u_xx,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if u_train.shape[0] == 2000:\n",
    "    ### Noiseless 2000 labeled samples program ###\n",
    "    program = '''\n",
    "    -0.970158*uf*u_x+0.003090*u_xx\n",
    "    '''\n",
    "elif u_train.shape[0] == 1000:\n",
    "    ### Noiseless 1000 labeled samples program ###\n",
    "    program = '''\n",
    "    -0.860374*uf*u_x+0.002631*u_xx\n",
    "    '''\n",
    "else: program = None\n",
    "\n",
    "pde_expr, variables = build_exp(program); print(pde_expr, variables)\n",
    "mod = sympytorch.SymPyModule(expressions=[pde_expr]); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0026, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(-0.8604, requires_grad=True))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mod.parameters())[0], list(mod.parameters())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, pretrained=False, noiseless_mode=True, init_cs=(0.5, 0.5), init_betas=(0.0, 0.0)):\n",
    "        super(RobustPINN, self).__init__()\n",
    "        self.model = model\n",
    "        if not pretrained: self.model.apply(self.xavier_init)\n",
    "        \n",
    "        self.noiseless_mode = noiseless_mode\n",
    "        self.in_fft_nn = None; self.out_fft_nn = None\n",
    "        self.inp_rpca = None; self.out_rpca = None\n",
    "        if not self.noiseless_mode:\n",
    "            # FFTNN\n",
    "            self.in_fft_nn = FFTTh(c=init_cs[0], func=lambda x:(torch.exp(-F.relu(x))))\n",
    "            self.out_fft_nn = FFTTh(c=init_cs[1], func=lambda x:(torch.exp(-F.relu(x))))\n",
    "\n",
    "            # Robust Beta-PCA\n",
    "            self.inp_rpca = RobustPCANN(beta=0.0, is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "            self.out_rpca = RobustPCANN(beta=0.0, is_beta_trainable=True, inp_dims=1, hidden_dims=32)\n",
    "        \n",
    "        self.p0 = torch.log(list(loss_fn.parameters())[0])\n",
    "        self.p1 = list(loss_fn.parameters())[1]\n",
    "        \n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        H = torch.cat([x, t], dim=1)\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, X_input, X_input_noise, y_input, y_input_noise, exp_p0=True, update_network_params=True, update_pde_params=True):\n",
    "        # Denoising process\n",
    "        if not self.noiseless_mode:\n",
    "            # (1) Denoising FFT on (x, t)\n",
    "            # This line returns the approx. recon.\n",
    "            X_input_noise = cat(torch.fft.ifft(self.in_fft_nn(X_input_noise[1])*X_input_noise[0]).real.reshape(-1, 1), \n",
    "                                torch.fft.ifft(self.in_fft_nn(X_input_noise[3])*X_input_noise[2]).real.reshape(-1, 1))\n",
    "            X_input_noise = X_input-X_input_noise\n",
    "            X_input = self.inp_rpca(X_input, X_input_noise, normalize=True)\n",
    "            \n",
    "            # (2)D enoising FFT on y_input\n",
    "            y_input_noise = y_input-torch.fft.ifft(self.out_fft_nn(y_input_noise[1])*y_input_noise[0]).real.reshape(-1, 1)\n",
    "            y_input = self.out_rpca(y_input, y_input_noise, normalize=True)\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(X_input[:, 0:1], X_input[:, 1:2])\n",
    "        \n",
    "        total_loss = []\n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            mse_loss = F.mse_loss(grads_dict[\"uf\"], y_input)\n",
    "            total_loss.append(mse_loss)\n",
    "            \n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            if exp_p0: p0_coeff = torch.exp(self.p0)\n",
    "            else: p0_coeff = self.p0\n",
    "            l_eq = F.mse_loss(p0_coeff*grads_dict[\"u_xx\"]+self.p1*grads_dict[\"uf\"]*grads_dict[\"u_x\"], u_t)\n",
    "            total_loss.append(l_eq)\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        \n",
    "        return {\"uf\":uf, \"u_x\":u_x, \"u_xx\":u_xx}, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp): \n",
    "        return -1.0+2.0*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], \n",
    "         activation_function=nn.Tanh, bn=None, # nn.LayerNorm\n",
    "         dropout=None)\n",
    "\n",
    "### TODO: How to load weights without using bn ###\n",
    "\n",
    "# Pretrained model\n",
    "semisup_model_state_dict = torch.load(\"./weights_nobn/semisup_model_nobn_2000_2000_finetuned.pth\")\n",
    "parameters = OrderedDict()\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "\n",
    "model.load_state_dict(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1039e-05, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(model(X_u_train), u_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISELESS_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fft, x_PSD = fft1d_denoise(X_u_train[:, 0:1], c=-5, return_real=True)\n",
    "_, t_fft, t_PSD = fft1d_denoise(X_u_train[:, 1:2], c=-5, return_real=True)\n",
    "_, u_train_fft, u_train_PSD = fft1d_denoise(u_train, c=-5, return_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global NOISELESS_MODE\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), exp_p0=True, update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global NOISELESS_MODE\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), exp_p0=True, update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = updated_grads[0][idx]+updated_grads[1][idx]\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in noisy mode.\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.82trial/s, best loss: 0.33631765842437744]\n",
      "{'c1': 0.7650368913450012, 'c2': 4.7917967471131915}\n"
     ]
    }
   ],
   "source": [
    "if not NOISELESS_MODE:\n",
    "    print(\"You are in noisy mode.\")\n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=NOISELESS_MODE)\n",
    "\n",
    "    def inference(args):\n",
    "        global pinn\n",
    "        c1, c2 = args\n",
    "        \n",
    "        pinn.in_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c1)]), requires_grad=False)\n",
    "        pinn.out_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c2)]), requires_grad=False)\n",
    "        \n",
    "        losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "        return sum(losses).item()\n",
    "\n",
    "    pinn.eval()\n",
    "    space = [hp.uniform('c1', 0, 5), hp.uniform('c2', 0, 5)]\n",
    "    res = fmin(fn=inference, space=space, algo=tpe.suggest, max_evals=200)\n",
    "\n",
    "    print(res)\n",
    "    if 'pinn' in globals(): del pinn\n",
    "\n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=NOISELESS_MODE,\n",
    "                      init_cs=(res['c1'], res['c2']))\n",
    "    \n",
    "else: \n",
    "    pinn = RobustPINN(model=model, loss_fn=mod, index2features=feature_names, \n",
    "                      scale=False, lb=None, ub=None, pretrained=True, noiseless_mode=NOISELESS_MODE)\n",
    "    print(\"You are in noiseless mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1, epochs2 = 10000, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  1.2596908807754517\n",
      "Epoch 1000:  0.0014860666124150157\n",
      "Epoch 2000:  0.0005661820759996772\n",
      "Epoch 3000:  0.0005258560995571315\n",
      "Epoch 4000:  0.0002725743979681283\n",
      "Epoch 5000:  0.0002066211891360581\n",
      "Epoch 6000:  0.0008864373667165637\n",
      "Epoch 7000:  0.000874482502695173\n",
      "Epoch 8000:  0.00013616340584121644\n",
      "Epoch 9000:  0.00013446251978166401\n",
      "Epoch 9999:  0.001238491851836443\n"
     ]
    }
   ],
   "source": [
    "# optimizer1 = MADGRAD(pinn.parameters(), lr=1e-7, momentum=0.95)\n",
    "optimizer1 = AdamGC(pinn.parameters(), lr=6e-4, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 1000) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  3.5253633541287854e-05\n",
      "Epoch 10:  3.0167830118443817e-05\n",
      "Epoch 20:  3.0167830118443817e-05\n",
      "Epoch 30:  3.0167830118443817e-05\n",
      "Epoch 40:  3.0167830118443817e-05\n",
      "Epoch 49:  3.0167830118443817e-05\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=500, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0026310004759579897, -0.9852237701416016)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(np.exp(pinn.p0.detach().numpy())), float(pinn.p1.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pinn, \"tmp.pth\")\n",
    "# pinn = load_weights(pinn, \"tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global NOISELESS_MODE\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), exp_p0=False, update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global NOISELESS_MODE\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), exp_p0=False, update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = updated_grads[0][idx]+updated_grads[1][idx]\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor(0.0026, requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor(-0.9852, requires_grad=True))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.p0 = nn.Parameter(data=torch.tensor(round(float(np.exp(pinn.p0.detach().numpy())), 4)))\n",
    "pinn.p1 = nn.Parameter(data=torch.tensor(round(float(pinn.p1.detach().numpy()), 4)))\n",
    "pinn.p0, pinn.p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del optimizer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = AdamGC(pinn.parameters(), lr=1e-6, use_gc=True, gc_conv_only=False, gc_loc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0007386051584035158\n",
      "-0.985198974609375 0.002601000014692545\n",
      "Epoch 100:  3.733452467713505e-05\n",
      "-0.9851832985877991 0.0026196553371846676\n",
      "Epoch 200:  3.6088509659748524e-05\n",
      "-0.9851832985877991 0.0026238122954964638\n",
      "Epoch 300:  3.820470374193974e-05\n",
      "-0.9851860404014587 0.002626896370202303\n",
      "Epoch 400:  4.187254671705887e-05\n",
      "-0.9851920008659363 0.0026295834686607122\n",
      "Epoch 500:  4.666307722800411e-05\n",
      "-0.9852027297019958 0.0026324379723519087\n",
      "Epoch 600:  5.2126233640592545e-05\n",
      "-0.9852146506309509 0.0026358412578701973\n",
      "Epoch 700:  5.7790337450569496e-05\n",
      "-0.985226571559906 0.002639970276504755\n",
      "Epoch 800:  6.30751674179919e-05\n",
      "-0.9852423071861267 0.0026448785793036222\n",
      "Epoch 900:  6.773779750801623e-05\n",
      "-0.9852601885795593 0.002650471171364188\n",
      "Epoch 999:  7.160598033806309e-05\n",
      "-0.9852778911590576 0.0026565406005829573\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "pinn.train()\n",
    "for i in range(epochs):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 100) == 0 or i == epochs-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(float(pinn.p1.detach().numpy()), float((pinn.p0.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  7.164247654145584e-05\n",
      "-0.9852780699729919 0.0026566041633486748\n",
      "Epoch 100:  7.485940295737237e-05\n",
      "-0.9852959513664246 0.0026631164364516735\n",
      "Epoch 200:  7.750182703603059e-05\n",
      "-0.9853138327598572 0.002669866429641843\n",
      "Epoch 300:  7.964611722854897e-05\n",
      "-0.9853317141532898 0.0026767603121697903\n",
      "Epoch 400:  8.135857933666557e-05\n",
      "-0.9853514432907104 0.0026837200857698917\n",
      "Epoch 500:  8.265350334113464e-05\n",
      "-0.9853752851486206 0.002690696157515049\n",
      "Epoch 600:  8.35633763927035e-05\n",
      "-0.9853991270065308 0.002697627991437912\n",
      "Epoch 700:  8.41427463456057e-05\n",
      "-0.9854229688644409 0.002704483224079013\n",
      "Epoch 800:  8.424053521594033e-05\n",
      "-0.9854468107223511 0.00271124136634171\n",
      "Epoch 900:  8.388216519961134e-05\n",
      "-0.9854711890220642 0.002717860508710146\n",
      "Epoch 1000:  8.315096056321636e-05\n",
      "-0.9855009913444519 0.0027243320364505053\n",
      "Epoch 1100:  8.213075489038602e-05\n",
      "-0.9855307936668396 0.002730624983087182\n",
      "Epoch 1200:  8.084473665803671e-05\n",
      "-0.9855605959892273 0.002736743539571762\n",
      "Epoch 1300:  7.931662548799068e-05\n",
      "-0.985590398311615 0.002742678625509143\n",
      "Epoch 1400:  7.76828674133867e-05\n",
      "-0.9856206774711609 0.0027484362944960594\n",
      "Epoch 1500:  7.595098577439785e-05\n",
      "-0.9856564402580261 0.0027540442533791065\n",
      "Epoch 1600:  7.412549894070253e-05\n",
      "-0.9856922030448914 0.0027594934217631817\n",
      "Epoch 1700:  7.226737216114998e-05\n",
      "-0.9857279658317566 0.0027647826354950666\n",
      "Epoch 1800:  7.042365905363113e-05\n",
      "-0.9857637286186218 0.002769935643300414\n",
      "Epoch 1900:  6.855784158688039e-05\n",
      "-0.9857994914054871 0.002774949884042144\n",
      "Epoch 2000:  6.668913556495681e-05\n",
      "-0.9858365654945374 0.0027798207011073828\n",
      "Epoch 2100:  6.486828351626173e-05\n",
      "-0.9858782887458801 0.002784567419439554\n",
      "Epoch 2200:  6.311314064078033e-05\n",
      "-0.9859200119972229 0.002789192134514451\n",
      "Epoch 2300:  6.145487714093179e-05\n",
      "-0.9859617352485657 0.0027937109116464853\n",
      "Epoch 2400:  5.9875041188206524e-05\n",
      "-0.9860034584999084 0.002798131899908185\n",
      "Epoch 2500:  5.8395286032464355e-05\n",
      "-0.9860451817512512 0.0028024751227349043\n",
      "Epoch 2600:  5.696840162272565e-05\n",
      "-0.986086905002594 0.002806756179779768\n",
      "Epoch 2700:  5.555844836635515e-05\n",
      "-0.9861286282539368 0.0028109706472605467\n",
      "Epoch 2800:  5.411021993495524e-05\n",
      "-0.9861703515052795 0.0028150908183306456\n",
      "Epoch 2900:  5.267802407615818e-05\n",
      "-0.9862120747566223 0.0028191078454256058\n",
      "Epoch 3000:  5.126493124407716e-05\n",
      "-0.9862537980079651 0.0028230277821421623\n",
      "Epoch 3100:  4.9895177653525025e-05\n",
      "-0.9862957000732422 0.0028268678579479456\n",
      "Epoch 3200:  4.860719491261989e-05\n",
      "-0.9863405823707581 0.0028306441381573677\n",
      "Epoch 3300:  4.739569340017624e-05\n",
      "-0.9863868951797485 0.0028343843296170235\n",
      "Epoch 3400:  4.633882053894922e-05\n",
      "-0.9864315986633301 0.0028381054289638996\n",
      "Epoch 3500:  4.546541822492145e-05\n",
      "-0.9864749312400818 0.0028419019654393196\n",
      "Epoch 3600:  4.457444811123423e-05\n",
      "-0.9865166544914246 0.0028457751031965017\n",
      "Epoch 3700:  4.367451401776634e-05\n",
      "-0.9865583777427673 0.0028496673330664635\n",
      "Epoch 3800:  4.229055775795132e-05\n",
      "-0.9866001009941101 0.002853446640074253\n",
      "Epoch 3900:  4.086554326931946e-05\n",
      "-0.9866418242454529 0.002856909530237317\n",
      "Epoch 4000:  3.984740033047274e-05\n",
      "-0.9866835474967957 0.002860206877812743\n",
      "Epoch 4100:  3.933850530302152e-05\n",
      "-0.9867252707481384 0.002863730536773801\n",
      "Epoch 4200:  3.919413575204089e-05\n",
      "-0.9867669939994812 0.002867571311071515\n",
      "Epoch 4300:  3.8645765016553923e-05\n",
      "-0.986808717250824 0.0028717017266899347\n",
      "Epoch 4400:  3.802888750215061e-05\n",
      "-0.9868504405021667 0.002875733654946089\n",
      "Epoch 4500:  3.7803569284733385e-05\n",
      "-0.9868910312652588 0.002880007028579712\n",
      "Epoch 4600:  3.759702303796075e-05\n",
      "-0.9869276285171509 0.0028848014771938324\n",
      "Epoch 4700:  3.7200061342446133e-05\n",
      "-0.9869633913040161 0.0028893486596643925\n",
      "Epoch 4800:  3.677210406749509e-05\n",
      "-0.9869991540908813 0.0028938683681190014\n",
      "Epoch 4900:  3.6599591112462804e-05\n",
      "-0.987034261226654 0.002898491220548749\n",
      "Epoch 5000:  3.6044388252776116e-05\n",
      "-0.9870665073394775 0.002903380896896124\n",
      "Epoch 5100:  3.6066656321054325e-05\n",
      "-0.9870963096618652 0.0029080903623253107\n",
      "Epoch 5200:  3.53943178197369e-05\n",
      "-0.9871261119842529 0.002912884345278144\n",
      "Epoch 5300:  3.490945528028533e-05\n",
      "-0.9871551990509033 0.002917428268119693\n",
      "Epoch 5400:  3.4732718631858006e-05\n",
      "-0.9871811866760254 0.002922152169048786\n",
      "Epoch 5500:  3.4642103855730966e-05\n",
      "-0.9872040748596191 0.002927063964307308\n",
      "Epoch 5600:  3.5472454328555614e-05\n",
      "-0.9872246384620667 0.002932138741016388\n",
      "Epoch 5700:  3.460213338257745e-05\n",
      "-0.9872413873672485 0.002937648445367813\n",
      "Epoch 5800:  3.509826274239458e-05\n",
      "-0.987254798412323 0.0029433167073875666\n",
      "Epoch 5900:  3.517401637509465e-05\n",
      "-0.9872658848762512 0.002948675537481904\n",
      "Epoch 6000:  3.459587969700806e-05\n",
      "-0.987274706363678 0.002954006427899003\n",
      "Epoch 6100:  3.353204010636546e-05\n",
      "-0.9872806668281555 0.002958762925118208\n",
      "Epoch 6200:  3.543092680047266e-05\n",
      "-0.9872872233390808 0.0029637436382472515\n",
      "Epoch 6300:  3.4861099265981466e-05\n",
      "-0.9872894287109375 0.002968437736853957\n",
      "Epoch 6400:  3.5042066883761436e-05\n",
      "-0.9872864484786987 0.00297389249317348\n",
      "Epoch 6500:  3.448939241934568e-05\n",
      "-0.9872801303863525 0.0029792713467031717\n",
      "Epoch 6600:  3.478059443295933e-05\n",
      "-0.9872729778289795 0.002984351245686412\n",
      "Epoch 6700:  3.2920575904427096e-05\n",
      "-0.9872632026672363 0.0029886304400861263\n",
      "Epoch 6800:  3.412975638639182e-05\n",
      "-0.9872581362724304 0.0029932816978543997\n",
      "Epoch 6900:  3.381901842658408e-05\n",
      "-0.9872525930404663 0.0029965657740831375\n",
      "Epoch 7000:  3.342885975143872e-05\n",
      "-0.9872452616691589 0.0030006524175405502\n",
      "Epoch 7100:  3.291381653980352e-05\n",
      "-0.9872323274612427 0.0030055451206862926\n",
      "Epoch 7200:  3.291193934273906e-05\n",
      "-0.9872201681137085 0.0030096431728452444\n",
      "Epoch 7300:  3.311309774289839e-05\n",
      "-0.987204372882843 0.003014151705428958\n",
      "Epoch 7400:  3.256226045778021e-05\n",
      "-0.9871900677680969 0.0030177931766957045\n",
      "Epoch 7500:  3.109343015239574e-05\n",
      "-0.9871810674667358 0.0030200458131730556\n",
      "Epoch 7600:  3.180981002515182e-05\n",
      "-0.9871715903282166 0.0030243562068790197\n",
      "Epoch 7700:  3.101986658293754e-05\n",
      "-0.9871581792831421 0.0030282556544989347\n",
      "Epoch 7800:  3.1579824280925095e-05\n",
      "-0.987148642539978 0.0030308072455227375\n",
      "Epoch 7900:  3.099651075899601e-05\n",
      "-0.9871360659599304 0.0030344941187649965\n",
      "Epoch 8000:  3.07359077851288e-05\n",
      "-0.9871211647987366 0.003038087859749794\n",
      "Epoch 8100:  2.99801704386482e-05\n",
      "-0.9871085286140442 0.003041307209059596\n",
      "Epoch 8200:  2.9907687348895706e-05\n",
      "-0.9870986938476562 0.0030440865084528923\n",
      "Epoch 8300:  2.9825730962329544e-05\n",
      "-0.9870883822441101 0.003047285368666053\n",
      "Epoch 8400:  2.9736846045125276e-05\n",
      "-0.9870777726173401 0.0030501082073897123\n",
      "Epoch 8500:  2.9237931812531315e-05\n",
      "-0.9870675802230835 0.003052955260500312\n",
      "Epoch 8600:  2.9038013963145204e-05\n",
      "-0.9870594143867493 0.003055723849684\n",
      "Epoch 8700:  2.849402153515257e-05\n",
      "-0.987053632736206 0.003057671943679452\n",
      "Epoch 8800:  2.810405749187339e-05\n",
      "-0.9870457649230957 0.003060709685087204\n",
      "Epoch 8900:  2.7929001589654945e-05\n",
      "-0.9870384335517883 0.003062954405322671\n",
      "Epoch 9000:  2.7326561394147575e-05\n",
      "-0.987031877040863 0.003065242664888501\n",
      "Epoch 9100:  2.706242594285868e-05\n",
      "-0.9870348572731018 0.003066382138058543\n",
      "Epoch 9200:  2.6808744223671965e-05\n",
      "-0.9870317578315735 0.0030689400155097246\n",
      "Epoch 9300:  2.6428100682096556e-05\n",
      "-0.987026572227478 0.003070840844884515\n",
      "Epoch 9400:  2.6370429623057134e-05\n",
      "-0.9870206117630005 0.0030732923187315464\n",
      "Epoch 9500:  2.5935625671991147e-05\n",
      "-0.9870127439498901 0.0030755368061363697\n",
      "Epoch 9600:  2.5603862013667822e-05\n",
      "-0.987005889415741 0.0030779081862419844\n",
      "Epoch 9700:  2.554271304688882e-05\n",
      "-0.9869999885559082 0.003079738700762391\n",
      "Epoch 9800:  2.5258939785999246e-05\n",
      "-0.9869975447654724 0.0030812397599220276\n",
      "Epoch 9900:  2.4741500965319574e-05\n",
      "-0.9869916439056396 0.003083054441958666\n",
      "Epoch 10000:  2.4126578864525072e-05\n",
      "-0.986985981464386 0.003084526164457202\n",
      "Epoch 10100:  2.40364606725052e-05\n",
      "-0.9869893193244934 0.0030866011511534452\n",
      "Epoch 10200:  2.4090075385174714e-05\n",
      "-0.986985981464386 0.0030883715953677893\n",
      "Epoch 10300:  2.3434686227119528e-05\n",
      "-0.9869809150695801 0.0030895511154085398\n",
      "Epoch 10400:  2.3938902813824825e-05\n",
      "-0.986980676651001 0.00309128244407475\n",
      "Epoch 10500:  2.3402946681017056e-05\n",
      "-0.9869762063026428 0.0030928414780646563\n",
      "Epoch 10600:  2.3192680600914173e-05\n",
      "-0.9869741201400757 0.003094542771577835\n",
      "Epoch 10700:  2.259343455079943e-05\n",
      "-0.9869717955589294 0.0030963346362113953\n",
      "Epoch 10800:  2.2405038180295378e-05\n",
      "-0.9869687557220459 0.0030978922732174397\n",
      "Epoch 10900:  2.2118443666840903e-05\n",
      "-0.9869665503501892 0.003099131630733609\n",
      "Epoch 11000:  2.217231667600572e-05\n",
      "-0.9869649410247803 0.003100205445662141\n",
      "Epoch 11100:  2.1881798602407798e-05\n",
      "-0.986961841583252 0.0031018618028610945\n",
      "Epoch 11200:  2.1726775230490603e-05\n",
      "-0.9869610071182251 0.0031032059341669083\n",
      "Epoch 11300:  2.1578753148787655e-05\n",
      "-0.9869561791419983 0.003104563569650054\n",
      "Epoch 11400:  2.1445535821840167e-05\n",
      "-0.9869520664215088 0.0031059589236974716\n",
      "Epoch 11500:  2.088800465571694e-05\n",
      "-0.9869518876075745 0.003106630640104413\n",
      "Epoch 11600:  2.095666241075378e-05\n",
      "-0.986956000328064 0.003107660450041294\n",
      "Epoch 11700:  2.0652320017688908e-05\n",
      "-0.9869530200958252 0.0031088977120816708\n",
      "Epoch 11800:  2.0628696802305058e-05\n",
      "-0.9869512319564819 0.003110162215307355\n",
      "Epoch 11900:  2.0350587874418125e-05\n",
      "-0.9869508147239685 0.0031111817806959152\n",
      "Epoch 12000:  2.0406709154485725e-05\n",
      "-0.9869489669799805 0.0031124833039939404\n",
      "Epoch 12100:  2.001304892473854e-05\n",
      "-0.9869454503059387 0.0031138118356466293\n",
      "Epoch 12200:  2.000279346248135e-05\n",
      "-0.9869435429573059 0.0031145999673753977\n",
      "Epoch 12300:  1.9708631953108124e-05\n",
      "-0.9869410395622253 0.003115413710474968\n",
      "Epoch 12400:  1.9488317775540054e-05\n",
      "-0.9869418144226074 0.0031163927633315325\n",
      "Epoch 12500:  1.9655839423649013e-05\n",
      "-0.9869425296783447 0.0031175045296549797\n",
      "Epoch 12600:  1.9304938177810982e-05\n",
      "-0.9869400262832642 0.0031185061670839787\n",
      "Epoch 12700:  1.912024526973255e-05\n",
      "-0.9869377017021179 0.0031192258466035128\n",
      "Epoch 12800:  1.904328746604733e-05\n",
      "-0.9869381785392761 0.0031202146783471107\n",
      "Epoch 12900:  1.9062399587710388e-05\n",
      "-0.9869380593299866 0.0031210724264383316\n",
      "Epoch 13000:  1.87858531717211e-05\n",
      "-0.9869349002838135 0.0031220007222145796\n",
      "Epoch 13100:  1.866415550466627e-05\n",
      "-0.9869351387023926 0.003122856607660651\n",
      "Epoch 13200:  1.8466032997821458e-05\n",
      "-0.9869331121444702 0.003123868489637971\n",
      "Epoch 13300:  1.8155917132389732e-05\n",
      "-0.986930251121521 0.0031244701240211725\n",
      "Epoch 13400:  1.8151275071431883e-05\n",
      "-0.9869309663772583 0.003125289920717478\n",
      "Epoch 13500:  1.8008526240009815e-05\n",
      "-0.9869303107261658 0.0031260333489626646\n",
      "Epoch 13600:  1.793705450836569e-05\n",
      "-0.9869311451911926 0.0031267160084098577\n",
      "Epoch 13700:  1.771966890373733e-05\n",
      "-0.986929714679718 0.0031274573411792517\n",
      "Epoch 13800:  1.7683249097899534e-05\n",
      "-0.9869294762611389 0.0031280722469091415\n",
      "Epoch 13900:  1.758041253197007e-05\n",
      "-0.9869285225868225 0.0031285465229302645\n",
      "Epoch 14000:  1.7528967873658985e-05\n",
      "-0.986928403377533 0.003129363991320133\n",
      "Epoch 14100:  1.7157359252450988e-05\n",
      "-0.9869265556335449 0.003129867836833\n",
      "Epoch 14200:  1.7131929780589417e-05\n",
      "-0.9869258999824524 0.0031308343168348074\n",
      "Epoch 14300:  1.71453102666419e-05\n",
      "-0.9869247078895569 0.0031314066145569086\n",
      "Epoch 14400:  1.7014202967402525e-05\n",
      "-0.9869253039360046 0.0031321044079959393\n",
      "Epoch 14500:  1.6954081729636528e-05\n",
      "-0.9869223833084106 0.0031325449235737324\n",
      "Epoch 14600:  1.6831361790536903e-05\n",
      "-0.9869225025177002 0.0031331840436905622\n",
      "Epoch 14700:  1.6729667549952865e-05\n",
      "-0.9869236350059509 0.0031336608808487654\n",
      "Epoch 14800:  1.6491041606059298e-05\n",
      "-0.9869219660758972 0.0031341721769422293\n",
      "Epoch 14900:  1.6526590115972795e-05\n",
      "-0.9869218468666077 0.003134771715849638\n",
      "Epoch 15000:  1.6354317267541774e-05\n",
      "-0.9869231581687927 0.0031354136299341917\n",
      "Epoch 15100:  1.615984001546167e-05\n",
      "-0.986921489238739 0.003136004088446498\n",
      "Epoch 15200:  1.608139973541256e-05\n",
      "-0.9869205355644226 0.003136374056339264\n",
      "Epoch 15300:  1.600189170858357e-05\n",
      "-0.9869202971458435 0.003136847633868456\n",
      "Epoch 15400:  1.6129783034557477e-05\n",
      "-0.9869212508201599 0.0031373684760183096\n",
      "Epoch 15500:  1.588913073646836e-05\n",
      "-0.9869207739830017 0.00313795474357903\n",
      "Epoch 15600:  1.5714102119090967e-05\n",
      "-0.9869182109832764 0.0031384839676320553\n",
      "Epoch 15700:  1.5755656932014972e-05\n",
      "-0.9869197010993958 0.0031389091163873672\n",
      "Epoch 15800:  1.571483153384179e-05\n",
      "-0.9869200587272644 0.003139265812933445\n",
      "Epoch 15900:  1.5484993127756752e-05\n",
      "-0.9869188070297241 0.0031396476551890373\n",
      "Epoch 16000:  1.5507073840126395e-05\n",
      "-0.986918568611145 0.003140035318210721\n",
      "Epoch 16100:  1.5504574548685923e-05\n",
      "-0.9869204163551331 0.003140421351417899\n",
      "Epoch 16200:  1.5281269952538423e-05\n",
      "-0.9869193434715271 0.003140922635793686\n",
      "Epoch 16300:  1.5276906196959317e-05\n",
      "-0.9869193434715271 0.0031414651311933994\n",
      "Epoch 16400:  1.5174036889220588e-05\n",
      "-0.9869195818901062 0.003141648368909955\n",
      "Epoch 16500:  1.4926917174307164e-05\n",
      "-0.9869183897972107 0.0031419615261256695\n",
      "Epoch 16600:  1.488807174609974e-05\n",
      "-0.9869192242622375 0.0031423987820744514\n",
      "Epoch 16700:  1.4788109183427878e-05\n",
      "-0.9869194626808167 0.0031426805071532726\n",
      "Epoch 16800:  1.4865650882711634e-05\n",
      "-0.9869200587272644 0.0031431205570697784\n",
      "Epoch 16900:  1.4684905181638896e-05\n",
      "-0.9869182705879211 0.003143265377730131\n",
      "Epoch 17000:  1.4664388800156303e-05\n",
      "-0.9869178533554077 0.0031435948330909014\n",
      "Epoch 17100:  1.4517390809487551e-05\n",
      "-0.9869158864021301 0.0031438779551535845\n",
      "Epoch 17200:  1.4326205928227864e-05\n",
      "-0.986914336681366 0.00314431544393301\n",
      "Epoch 17300:  1.441776748833945e-05\n",
      "-0.9869158864021301 0.0031444609630852938\n",
      "Epoch 17400:  1.4210289009497501e-05\n",
      "-0.9869145154953003 0.0031446851789951324\n",
      "Epoch 17500:  1.4267692677094601e-05\n",
      "-0.9869168400764465 0.003145122667774558\n",
      "Epoch 17600:  1.4242507859307807e-05\n",
      "-0.9869139194488525 0.003145338036119938\n",
      "Epoch 17700:  1.426731250830926e-05\n",
      "-0.9869137406349182 0.003145645372569561\n",
      "Epoch 17800:  1.4126168935035821e-05\n",
      "-0.9869131445884705 0.0031460411846637726\n",
      "Epoch 17900:  1.3934116395830642e-05\n",
      "-0.9869114756584167 0.0031462954357266426\n",
      "Epoch 18000:  1.4004785953147803e-05\n",
      "-0.9869117736816406 0.0031466393265873194\n",
      "Epoch 18100:  1.3883856809115969e-05\n",
      "-0.9869096279144287 0.003146550850942731\n",
      "Epoch 18200:  1.3767042219114956e-05\n",
      "-0.986910343170166 0.0031468160450458527\n",
      "Epoch 18300:  1.3697079339181073e-05\n",
      "-0.9869099259376526 0.0031473853159695864\n",
      "Epoch 18400:  1.3788822798233014e-05\n",
      "-0.9869101047515869 0.0031473746057599783\n",
      "Epoch 18500:  1.3692966604139656e-05\n",
      "-0.9869105219841003 0.0031477503944188356\n",
      "Epoch 18600:  1.3570139344665222e-05\n",
      "-0.9869105815887451 0.003148219082504511\n",
      "Epoch 18700:  1.3593306903203484e-05\n",
      "-0.9869065284729004 0.0031482994090765715\n",
      "Epoch 18800:  1.354487903881818e-05\n",
      "-0.9869080781936646 0.00314844842068851\n",
      "Epoch 18900:  1.336880995950196e-05\n",
      "-0.9869083166122437 0.003148663556203246\n",
      "Epoch 19000:  1.3302035767992493e-05\n",
      "-0.9869073033332825 0.0031488772947341204\n",
      "Epoch 19100:  1.3309766472957563e-05\n",
      "-0.986907958984375 0.003149173455312848\n",
      "Epoch 19200:  1.3340671102923807e-05\n",
      "-0.9869076013565063 0.003149417694658041\n",
      "Epoch 19300:  1.3229802789282985e-05\n",
      "-0.9869054555892944 0.003149704309180379\n",
      "Epoch 19400:  1.3199643035477493e-05\n",
      "-0.9869031310081482 0.0031496770679950714\n",
      "Epoch 19500:  1.3100739124638494e-05\n",
      "-0.9869054555892944 0.003149778349325061\n",
      "Epoch 19600:  1.2950475138495676e-05\n",
      "-0.9869047403335571 0.003150066826492548\n",
      "Epoch 19700:  1.2976955986232497e-05\n",
      "-0.9869064092636108 0.003150292206555605\n",
      "Epoch 19800:  1.290977161261253e-05\n",
      "-0.9869065880775452 0.0031504344660788774\n",
      "Epoch 19900:  1.2912413694721181e-05\n",
      "-0.9869062304496765 0.0031505711376667023\n",
      "Epoch 20000:  1.2816875823773444e-05\n",
      "-0.9869049191474915 0.00315079465508461\n",
      "Epoch 20100:  1.2761262041749433e-05\n",
      "-0.9869040846824646 0.0031508347019553185\n",
      "Epoch 20200:  1.2799409887520596e-05\n",
      "-0.9869049787521362 0.003150911768898368\n",
      "Epoch 20300:  1.2645726201299112e-05\n",
      "-0.9869030117988586 0.0031511266715824604\n",
      "Epoch 20400:  1.2621279893210158e-05\n",
      "-0.9869017004966736 0.003151258686557412\n",
      "Epoch 20500:  1.2684165994869545e-05\n",
      "-0.9869028925895691 0.003151302458718419\n",
      "Epoch 20600:  1.2487001185945701e-05\n",
      "-0.9869003295898438 0.003151794895529747\n",
      "Epoch 20700:  1.2453182534954976e-05\n",
      "-0.9869017004966736 0.003151653567329049\n",
      "Epoch 20800:  1.2452150258468464e-05\n",
      "-0.9869022369384766 0.00315166381187737\n",
      "Epoch 20900:  1.2347534720902331e-05\n",
      "-0.9869001507759094 0.0031518572941422462\n",
      "Epoch 21000:  1.2384693036437966e-05\n",
      "-0.9869011640548706 0.0031520435586571693\n",
      "Epoch 21100:  1.2333975064393599e-05\n",
      "-0.9869006872177124 0.0031521173659712076\n",
      "Epoch 21200:  1.2226450053276494e-05\n",
      "-0.986899733543396 0.003152156714349985\n",
      "Epoch 21300:  1.2241608601470944e-05\n",
      "-0.9869009256362915 0.0031524035148322582\n",
      "Epoch 21400:  1.2132071788073517e-05\n",
      "-0.9868996143341064 0.003152517369017005\n",
      "Epoch 21500:  1.2085665730410255e-05\n",
      "-0.9868975281715393 0.0031525602098554373\n",
      "Epoch 21600:  1.2032661288685631e-05\n",
      "-0.9868935942649841 0.0031527183018624783\n",
      "Epoch 21700:  1.2039912689942867e-05\n",
      "-0.9868960976600647 0.0031527660321444273\n",
      "Epoch 21800:  1.1936340342799667e-05\n",
      "-0.9868943095207214 0.003152959980070591\n",
      "Epoch 21900:  1.1972893844358623e-05\n",
      "-0.9868940114974976 0.003153004217892885\n",
      "Epoch 22000:  1.1920795259356964e-05\n",
      "-0.9868955016136169 0.0031531096901744604\n",
      "Epoch 22100:  1.1822870874311775e-05\n",
      "-0.9868941903114319 0.0031531681306660175\n",
      "Epoch 22200:  1.1840183105960023e-05\n",
      "-0.9868942499160767 0.003153126919642091\n",
      "Epoch 22300:  1.1780806744354777e-05\n",
      "-0.9868928790092468 0.0031532503198832273\n",
      "Epoch 22400:  1.1675842870317865e-05\n",
      "-0.986893892288208 0.0031534451991319656\n",
      "Epoch 22500:  1.1629639629973099e-05\n",
      "-0.9868907928466797 0.003153385827317834\n",
      "Epoch 22600:  1.1640004231594503e-05\n",
      "-0.9868919253349304 0.003153624013066292\n",
      "Epoch 22700:  1.1539613296918105e-05\n",
      "-0.9868907928466797 0.0031537183094769716\n",
      "Epoch 22800:  1.1525782610988244e-05\n",
      "-0.9868898391723633 0.003153775818645954\n",
      "Epoch 22900:  1.1512725905049592e-05\n",
      "-0.9868913292884827 0.0031539236661046743\n",
      "Epoch 23000:  1.1421278031775728e-05\n",
      "-0.9868890643119812 0.0031539485789835453\n",
      "Epoch 23100:  1.1384930076019373e-05\n",
      "-0.9868873953819275 0.003154006553813815\n",
      "Epoch 23200:  1.1415703738748562e-05\n",
      "-0.9868878722190857 0.003153980942443013\n",
      "Epoch 23300:  1.13594996946631e-05\n",
      "-0.9868862628936768 0.0031540312338620424\n",
      "Epoch 23400:  1.130186501541175e-05\n",
      "-0.9868831038475037 0.0031540016643702984\n",
      "Epoch 23500:  1.1243420885875821e-05\n",
      "-0.9868820309638977 0.003153901780024171\n",
      "Epoch 23600:  1.127523046307033e-05\n",
      "-0.9868836998939514 0.003154229139909148\n",
      "Epoch 23700:  1.1199937944184057e-05\n",
      "-0.986882746219635 0.0031544521916657686\n",
      "Epoch 23800:  1.1158695997437462e-05\n",
      "-0.9868819713592529 0.003154221922159195\n",
      "Epoch 23900:  1.1096290108980611e-05\n",
      "-0.986878514289856 0.003154387231916189\n",
      "Epoch 24000:  1.1074904250563122e-05\n",
      "-0.9868791103363037 0.0031545348465442657\n",
      "Epoch 24100:  1.098789471143391e-05\n",
      "-0.9868771433830261 0.0031544032972306013\n",
      "Epoch 24200:  1.0954559002129827e-05\n",
      "-0.9868760704994202 0.0031544817611575127\n",
      "Epoch 24300:  1.0951165677397512e-05\n",
      "-0.9868764877319336 0.0031544349621981382\n",
      "Epoch 24400:  1.0869957804970909e-05\n",
      "-0.9868767261505127 0.0031546580139547586\n",
      "Epoch 24500:  1.0889053555729333e-05\n",
      "-0.9868758320808411 0.0031547124963253736\n",
      "Epoch 24600:  1.0807946637214627e-05\n",
      "-0.9868758320808411 0.003154784208163619\n",
      "Epoch 24700:  1.0762358215288259e-05\n",
      "-0.9868764281272888 0.0031548873521387577\n",
      "Epoch 24800:  1.0769720574899111e-05\n",
      "-0.9868742227554321 0.003155000042170286\n",
      "Epoch 24900:  1.0739657227532007e-05\n",
      "-0.9868721961975098 0.00315496395342052\n",
      "Epoch 25000:  1.0710882634157315e-05\n",
      "-0.9868722558021545 0.0031551173888146877\n",
      "Epoch 25100:  1.0680570994736627e-05\n",
      "-0.9868701696395874 0.003154964419081807\n",
      "Epoch 25200:  1.065266587829683e-05\n",
      "-0.9868691563606262 0.003155064070597291\n",
      "Epoch 25300:  1.0606591786199715e-05\n",
      "-0.9868685007095337 0.003155084326863289\n",
      "Epoch 25400:  1.0521195690671448e-05\n",
      "-0.9868634343147278 0.0031551632564514875\n",
      "Epoch 25500:  1.0553676474955864e-05\n",
      "-0.9868653416633606 0.003155210055410862\n",
      "Epoch 25600:  1.0516288966755383e-05\n",
      "-0.9868661761283875 0.0031552815344184637\n",
      "Epoch 25700:  1.048070134856971e-05\n",
      "-0.9868667721748352 0.00315535394474864\n",
      "Epoch 25800:  1.0407665286038537e-05\n",
      "-0.9868658781051636 0.0031554955057799816\n",
      "Epoch 25900:  1.0368774383096024e-05\n",
      "-0.9868625998497009 0.0031553704757243395\n",
      "Epoch 26000:  1.0342506357119419e-05\n",
      "-0.986862063407898 0.0031552559230476618\n",
      "Epoch 26100:  1.0364468835177831e-05\n",
      "-0.9868612289428711 0.003155214712023735\n",
      "Epoch 26200:  1.0331300472898874e-05\n",
      "-0.9868602156639099 0.003155103651806712\n",
      "Epoch 26300:  1.0246329111396335e-05\n",
      "-0.986858606338501 0.0031553616281598806\n",
      "Epoch 26400:  1.0223949175269809e-05\n",
      "-0.9868581295013428 0.0031554417219012976\n",
      "Epoch 26500:  1.024339235300431e-05\n",
      "-0.9868583679199219 0.0031553925946354866\n",
      "Epoch 26600:  1.0289460988133214e-05\n",
      "-0.9868583083152771 0.003155550453811884\n",
      "Epoch 26700:  1.0186760846409015e-05\n",
      "-0.9868574738502502 0.003155598184093833\n",
      "Epoch 26800:  1.0163291335629765e-05\n",
      "-0.9868561625480652 0.0031556840986013412\n",
      "Epoch 26900:  1.0105591172759887e-05\n",
      "-0.9868557453155518 0.003155814716592431\n",
      "Epoch 27000:  1.0036051207862329e-05\n",
      "-0.9868540167808533 0.003155802609398961\n",
      "Epoch 27100:  1.0108828064403497e-05\n",
      "-0.9868524074554443 0.0031556894537061453\n",
      "Epoch 27200:  9.990005310100969e-06\n",
      "-0.9868520498275757 0.0031557262409478426\n",
      "Epoch 27300:  9.952163054549601e-06\n",
      "-0.9868494272232056 0.003155664075165987\n",
      "Epoch 27400:  9.967481673811562e-06\n",
      "-0.9868506193161011 0.0031557537149637938\n",
      "Epoch 27500:  9.923683137458283e-06\n",
      "-0.9868499636650085 0.003155753016471863\n",
      "Epoch 27600:  9.892893103824463e-06\n",
      "-0.9868483543395996 0.0031557606998831034\n",
      "Epoch 27700:  9.818520993576385e-06\n",
      "-0.9868472814559937 0.0031557190231978893\n",
      "Epoch 27800:  9.814588338485919e-06\n",
      "-0.9868466854095459 0.003155933925881982\n",
      "Epoch 27900:  9.841894097917248e-06\n",
      "-0.9868447780609131 0.003155952086672187\n",
      "Epoch 28000:  9.794240213523153e-06\n",
      "-0.9868448376655579 0.0031560035422444344\n",
      "Epoch 28100:  9.74013346421998e-06\n",
      "-0.9868444204330444 0.0031558903865516186\n",
      "Epoch 28200:  9.741700523591135e-06\n",
      "-0.9868437647819519 0.0031559993512928486\n",
      "Epoch 28300:  9.664434401202016e-06\n",
      "-0.9868417978286743 0.0031560384668409824\n",
      "Epoch 28400:  9.67217783909291e-06\n",
      "-0.9868412613868713 0.0031559071503579617\n",
      "Epoch 28500:  9.602127647667658e-06\n",
      "-0.9868409633636475 0.003155878046527505\n",
      "Epoch 28600:  9.599730219633784e-06\n",
      "-0.9868408441543579 0.0031559320632368326\n",
      "Epoch 28700:  9.595209121471271e-06\n",
      "-0.9868401288986206 0.0031559409108012915\n",
      "Epoch 28800:  9.526875146548264e-06\n",
      "-0.9868378639221191 0.0031560719944536686\n",
      "Epoch 28900:  9.583796781953424e-06\n",
      "-0.9868365526199341 0.003155894111841917\n",
      "Epoch 29000:  9.509925803286023e-06\n",
      "-0.9868327379226685 0.003155922284349799\n",
      "Epoch 29100:  9.434705134481192e-06\n",
      "-0.986832857131958 0.0031560512725263834\n",
      "Epoch 29200:  9.414967280463316e-06\n",
      "-0.9868322610855103 0.0031562079675495625\n",
      "Epoch 29300:  9.425743883184623e-06\n",
      "-0.9868301749229431 0.0031560093630105257\n",
      "Epoch 29400:  9.401010174769908e-06\n",
      "-0.986831545829773 0.003156225895509124\n",
      "Epoch 29500:  9.339294592791703e-06\n",
      "-0.9868295192718506 0.0031559725757688284\n",
      "Epoch 29600:  9.406056960870046e-06\n",
      "-0.9868301749229431 0.0031559965573251247\n",
      "Epoch 29700:  9.275522643292788e-06\n",
      "-0.9868237972259521 0.0031560601200908422\n",
      "Epoch 29800:  9.305085768573917e-06\n",
      "-0.9868254661560059 0.0031559611670672894\n",
      "Epoch 29900:  9.244953616871499e-06\n",
      "-0.9868233799934387 0.0031560934148728848\n",
      "Epoch 30000:  9.229983334080316e-06\n",
      "-0.9868232011795044 0.003156134393066168\n",
      "Epoch 30100:  9.18815567274578e-06\n",
      "-0.9868227243423462 0.0031561008654534817\n",
      "Epoch 30200:  9.162769856629893e-06\n",
      "-0.9868233799934387 0.003156163962557912\n",
      "Epoch 30300:  9.160800800600555e-06\n",
      "-0.9868195056915283 0.003156347433105111\n",
      "Epoch 30400:  9.130825674219523e-06\n",
      "-0.986817479133606 0.0031561905052512884\n",
      "Epoch 30500:  9.07904814084759e-06\n",
      "-0.9868168830871582 0.00315610459074378\n",
      "Epoch 30600:  9.059303920366801e-06\n",
      "-0.98681640625 0.003156231250613928\n",
      "Epoch 30700:  9.098948794417083e-06\n",
      "-0.9868152141571045 0.003156025893986225\n",
      "Epoch 30800:  9.027514352055732e-06\n",
      "-0.9868148565292358 0.0031562154181301594\n",
      "Epoch 30900:  9.024734026752412e-06\n",
      "-0.986814022064209 0.0031561932992190123\n",
      "Epoch 31000:  8.973353033070453e-06\n",
      "-0.9868142604827881 0.0031564715318381786\n",
      "Epoch 31100:  8.951896234066226e-06\n",
      "-0.9868117570877075 0.00315618934109807\n",
      "Epoch 31200:  8.925617294153199e-06\n",
      "-0.9868102669715881 0.003156239865347743\n",
      "Epoch 31300:  8.912867087929044e-06\n",
      "-0.9868108034133911 0.003156220307573676\n",
      "Epoch 31400:  8.923693712858949e-06\n",
      "-0.9868103861808777 0.0031561730429530144\n",
      "Epoch 31500:  8.82730364537565e-06\n",
      "-0.9868065714836121 0.0031563006341457367\n",
      "Epoch 31600:  8.871484169503674e-06\n",
      "-0.986806333065033 0.0031561932992190123\n",
      "Epoch 31700:  8.80848165252246e-06\n",
      "-0.9868058562278748 0.003156457096338272\n",
      "Epoch 31800:  8.80243896972388e-06\n",
      "-0.9868032932281494 0.0031563551165163517\n",
      "Epoch 31900:  8.782364602666348e-06\n",
      "-0.9868033528327942 0.0031561709474772215\n",
      "Epoch 32000:  8.778802111919504e-06\n",
      "-0.9868013262748718 0.0031563956290483475\n",
      "Epoch 32100:  8.713720490050036e-06\n",
      "-0.9867980480194092 0.0031562030781060457\n",
      "Epoch 32200:  8.726727173780091e-06\n",
      "-0.9867972731590271 0.0031561460345983505\n",
      "Epoch 32300:  8.707737833901774e-06\n",
      "-0.9867974519729614 0.0031562598887830973\n",
      "Epoch 32400:  8.665351742820349e-06\n",
      "-0.9867959022521973 0.0031562354415655136\n",
      "Epoch 32500:  8.651877578813583e-06\n",
      "-0.986795961856842 0.0031565723475068808\n",
      "Epoch 32600:  8.638547114969697e-06\n",
      "-0.9867954850196838 0.0031561877112835646\n",
      "Epoch 32700:  8.599235115980264e-06\n",
      "-0.9867918491363525 0.0031563835218548775\n",
      "Epoch 32800:  8.530661034455989e-06\n",
      "-0.9867887496948242 0.0031562934163957834\n",
      "Epoch 32900:  8.5257424871088e-06\n",
      "-0.9867889285087585 0.0031563814263790846\n",
      "Epoch 33000:  8.544443517166656e-06\n",
      "-0.9867882132530212 0.003156200284138322\n",
      "Epoch 33100:  8.47605406306684e-06\n",
      "-0.986786961555481 0.0031563560478389263\n",
      "Epoch 33200:  8.4665434769704e-06\n",
      "-0.9867871999740601 0.00315633462741971\n",
      "Epoch 33300:  8.447390428045765e-06\n",
      "-0.9867854118347168 0.0031561448704451323\n",
      "Epoch 33400:  8.408427675021812e-06\n",
      "-0.9867833256721497 0.0031562235672026873\n",
      "Epoch 33500:  8.441823410976212e-06\n",
      "-0.9867837429046631 0.0031564601231366396\n",
      "Epoch 33600:  8.413238901994191e-06\n",
      "-0.9867802262306213 0.0031563236843794584\n",
      "Epoch 33700:  8.377849553653505e-06\n",
      "-0.986780047416687 0.0031563814263790846\n",
      "Epoch 33800:  8.387720299651846e-06\n",
      "-0.9867790937423706 0.0031562624499201775\n",
      "Epoch 33900:  8.313062608067412e-06\n",
      "-0.9867769479751587 0.0031562417279928923\n",
      "Epoch 34000:  8.29216787678888e-06\n",
      "-0.9867768287658691 0.0031563357915729284\n",
      "Epoch 34100:  8.277591405203566e-06\n",
      "-0.9867766499519348 0.0031563176307827234\n",
      "Epoch 34200:  8.260031790996436e-06\n",
      "-0.9867740869522095 0.003156282240524888\n",
      "Epoch 34300:  8.217488357331604e-06\n",
      "-0.9867730736732483 0.003156310645863414\n",
      "Epoch 34400:  8.212613465730101e-06\n",
      "-0.986771285533905 0.0031562144868075848\n",
      "Epoch 34500:  8.215805792133324e-06\n",
      "-0.9867722392082214 0.003156149061396718\n",
      "Epoch 34600:  8.197599527193233e-06\n",
      "-0.9867704510688782 0.0031563742086291313\n",
      "Epoch 34700:  8.178421012416948e-06\n",
      "-0.9867693781852722 0.0031563921365886927\n",
      "Epoch 34800:  8.154724127962254e-06\n",
      "-0.9867698550224304 0.003156436840072274\n",
      "Epoch 34900:  8.156295734806918e-06\n",
      "-0.986767590045929 0.003156397957354784\n",
      "Epoch 35000:  8.107080248009879e-06\n",
      "-0.9867657423019409 0.003156314603984356\n",
      "Epoch 35100:  8.098533726297319e-06\n",
      "-0.9867653846740723 0.0031562806107103825\n",
      "Epoch 35200:  8.065996553341392e-06\n",
      "-0.986765444278717 0.003156406572088599\n",
      "Epoch 35300:  8.088147296803072e-06\n",
      "-0.9867626428604126 0.003156315302476287\n",
      "Epoch 35400:  8.059144420258235e-06\n",
      "-0.9867625832557678 0.003156273625791073\n",
      "Epoch 35500:  8.020199857128318e-06\n",
      "-0.9867613911628723 0.0031562349759042263\n",
      "Epoch 35600:  7.98746714281151e-06\n",
      "-0.9867594242095947 0.0031562901567667723\n",
      "Epoch 35700:  7.955598448461387e-06\n",
      "-0.9867581725120544 0.003156252671033144\n",
      "Epoch 35800:  7.972385901666712e-06\n",
      "-0.986755907535553 0.003156127408146858\n",
      "Epoch 35900:  7.925850695755798e-06\n",
      "-0.9867552518844604 0.0031562657095491886\n",
      "Epoch 36000:  7.935000212455634e-06\n",
      "-0.9867542386054993 0.0031560880597680807\n",
      "Epoch 36100:  7.925473255454563e-06\n",
      "-0.986752986907959 0.003156200982630253\n",
      "Epoch 36200:  7.872637979744468e-06\n",
      "-0.9867531061172485 0.0031563083175569773\n",
      "Epoch 36300:  7.843842467991635e-06\n",
      "-0.9867503046989441 0.0031562279909849167\n",
      "Epoch 36400:  7.872412425058428e-06\n",
      "-0.9867499470710754 0.0031562689691781998\n",
      "Epoch 36500:  7.844334504625294e-06\n",
      "-0.986748218536377 0.0031560640782117844\n",
      "Epoch 36600:  7.795430065016262e-06\n",
      "-0.9867448806762695 0.003156175371259451\n",
      "Epoch 36700:  7.807236215739977e-06\n",
      "-0.9867469072341919 0.0031562410295009613\n",
      "Epoch 36800:  7.764445399516262e-06\n",
      "-0.9867445230484009 0.0031563080847263336\n",
      "Epoch 36900:  7.73042756918585e-06\n",
      "-0.9867433309555054 0.0031561595387756824\n",
      "Epoch 37000:  7.721560905338265e-06\n",
      "-0.9867417216300964 0.0031563108786940575\n",
      "Epoch 37100:  7.706029464316089e-06\n",
      "-0.9867410659790039 0.003156213788315654\n",
      "Epoch 37200:  7.709130841249134e-06\n",
      "-0.986739993095398 0.003156177932396531\n",
      "Epoch 37300:  7.737945452390704e-06\n",
      "-0.9867396950721741 0.0031561916694045067\n",
      "Epoch 37400:  7.7041368058417e-06\n",
      "-0.9867386817932129 0.0031561204232275486\n",
      "Epoch 37500:  7.639496288902592e-06\n",
      "-0.986736536026001 0.0031562293879687786\n",
      "Epoch 37600:  7.65905588195892e-06\n",
      "-0.9867342114448547 0.00315618934109807\n",
      "Epoch 37700:  7.581069894513348e-06\n",
      "-0.9867321848869324 0.0031560135539621115\n",
      "Epoch 37800:  7.617043138452573e-06\n",
      "-0.9867298603057861 0.0031560540664941072\n",
      "Epoch 37900:  7.58567739467253e-06\n",
      "-0.9867270588874817 0.0031562503427267075\n",
      "Epoch 38000:  7.577185442642076e-06\n",
      "-0.9867255687713623 0.0031560221686959267\n",
      "Epoch 38100:  7.576640655315714e-06\n",
      "-0.986725389957428 0.0031560331117361784\n",
      "Epoch 38200:  7.510444447689224e-06\n",
      "-0.9867226481437683 0.003155983053147793\n",
      "Epoch 38300:  7.556293894595001e-06\n",
      "-0.9867229461669922 0.0031561150681227446\n",
      "Epoch 38400:  7.518915481341537e-06\n",
      "-0.9867234826087952 0.00315624987706542\n",
      "Epoch 38500:  7.504854238504777e-06\n",
      "-0.9867236018180847 0.0031561937648802996\n",
      "Epoch 38600:  7.475595339201391e-06\n",
      "-0.986723005771637 0.0031561346258968115\n",
      "Epoch 38700:  7.429810921166791e-06\n",
      "-0.9867194294929504 0.0031562638469040394\n",
      "Epoch 38800:  7.499964340240695e-06\n",
      "-0.9867178201675415 0.0031559846829622984\n",
      "Epoch 38900:  7.4055715231224895e-06\n",
      "-0.9867180585861206 0.0031560552306473255\n",
      "Epoch 39000:  7.406687473121565e-06\n",
      "-0.9867159724235535 0.0031560263596475124\n",
      "Epoch 39100:  7.436306532326853e-06\n",
      "-0.9867141246795654 0.0031560445204377174\n",
      "Epoch 39200:  7.372786967607681e-06\n",
      "-0.9867141246795654 0.003156000981107354\n",
      "Epoch 39300:  7.388852736767149e-06\n",
      "-0.9867114424705505 0.003156098537147045\n",
      "Epoch 39400:  7.3672845246619545e-06\n",
      "-0.9867111444473267 0.0031561364885419607\n",
      "Epoch 39500:  7.336542694247328e-06\n",
      "-0.9867106080055237 0.00315631041303277\n",
      "Epoch 39600:  7.328575520659797e-06\n",
      "-0.9867104887962341 0.003156168619170785\n",
      "Epoch 39700:  7.280054433067562e-06\n",
      "-0.9867068529129028 0.003156091319397092\n",
      "Epoch 39800:  7.2846346483856905e-06\n",
      "-0.9867085814476013 0.0031562354415655136\n",
      "Epoch 39900:  7.2570223892398644e-06\n",
      "-0.9867063164710999 0.003156109247356653\n",
      "Epoch 40000:  7.230305072880583e-06\n",
      "-0.9867038130760193 0.0031560915522277355\n",
      "Epoch 40100:  7.2539983193564694e-06\n",
      "-0.9867043495178223 0.0031561751384288073\n",
      "Epoch 40200:  7.242133506224491e-06\n",
      "-0.9867050051689148 0.0031562172807753086\n",
      "Epoch 40300:  7.249565442180028e-06\n",
      "-0.9867029190063477 0.0031560854986310005\n",
      "Epoch 40400:  7.203424047474982e-06\n",
      "-0.9867006540298462 0.003156176535412669\n",
      "Epoch 40500:  7.152183570724446e-06\n",
      "-0.9866985082626343 0.003156142309308052\n",
      "Epoch 40600:  7.1502199716633186e-06\n",
      "-0.9866998791694641 0.0031562242656946182\n",
      "Epoch 40700:  7.146539701352594e-06\n",
      "-0.9866971373558044 0.0031560894567519426\n",
      "Epoch 40800:  7.140209618228255e-06\n",
      "-0.986698567867279 0.0031562151852995157\n",
      "Epoch 40900:  7.14328007234144e-06\n",
      "-0.9866973757743835 0.003156112739816308\n",
      "Epoch 41000:  7.106396424205741e-06\n",
      "-0.9866942167282104 0.0031560701318085194\n",
      "Epoch 41100:  7.056302820274141e-06\n",
      "-0.9866916537284851 0.003156069666147232\n",
      "Epoch 41200:  7.0478640736837406e-06\n",
      "-0.9866915345191956 0.003156043589115143\n",
      "Epoch 41300:  7.06438186170999e-06\n",
      "-0.9866926074028015 0.003156175371259451\n",
      "Epoch 41400:  7.0618066274619196e-06\n",
      "-0.9866902828216553 0.0031559772323817015\n",
      "Epoch 41500:  7.038347121124389e-06\n",
      "-0.9866892695426941 0.0031560519710183144\n",
      "Epoch 41600:  6.985146228544181e-06\n",
      "-0.9866849780082703 0.003156081074848771\n",
      "Epoch 41700:  6.9797579271835275e-06\n",
      "-0.9866852164268494 0.003156072925776243\n",
      "Epoch 41800:  6.957876394153573e-06\n",
      "-0.9866828918457031 0.0031560894567519426\n",
      "Epoch 41900:  6.977815701247891e-06\n",
      "-0.9866839647293091 0.003156176768243313\n",
      "Epoch 42000:  6.967568424443016e-06\n",
      "-0.9866831302642822 0.003155977465212345\n",
      "Epoch 42100:  6.914740879437886e-06\n",
      "-0.9866804480552673 0.0031559059862047434\n",
      "Epoch 42200:  6.940582807146711e-06\n",
      "-0.9866817593574524 0.003156167222186923\n",
      "Epoch 42300:  6.901093911437783e-06\n",
      "-0.9866803884506226 0.0031560033094137907\n",
      "Epoch 42400:  6.867625415907241e-06\n",
      "-0.9866770505905151 0.003155982820317149\n",
      "Epoch 42500:  6.853790182503872e-06\n",
      "-0.9866743087768555 0.003155980259180069\n",
      "Epoch 42600:  6.869556273159105e-06\n",
      "-0.9866757392883301 0.003156118094921112\n",
      "Epoch 42700:  6.834564374003094e-06\n",
      "-0.9866746068000793 0.003156067803502083\n",
      "Epoch 42800:  6.8187000579200685e-06\n",
      "-0.9866726994514465 0.0031561225187033415\n",
      "Epoch 42900:  6.814320840931032e-06\n",
      "-0.9866707921028137 0.00315600517205894\n",
      "Epoch 43000:  6.796869001846062e-06\n",
      "-0.9866684079170227 0.0031559616327285767\n",
      "Epoch 43100:  6.785934147046646e-06\n",
      "-0.9866650700569153 0.003155861748382449\n",
      "Epoch 43200:  6.763089459127514e-06\n",
      "-0.9866651892662048 0.003155869198963046\n",
      "Epoch 43300:  6.740646767866565e-06\n",
      "-0.9866639971733093 0.003155883401632309\n",
      "Epoch 43400:  6.744534402969293e-06\n",
      "-0.9866631031036377 0.0031560093630105257\n",
      "Epoch 43500:  6.736549039487727e-06\n",
      "-0.9866628050804138 0.003155910177156329\n",
      "Epoch 43600:  6.750232842023252e-06\n",
      "-0.9866616129875183 0.0031557404436171055\n",
      "Epoch 43700:  6.721456884406507e-06\n",
      "-0.9866604804992676 0.0031559669878333807\n",
      "Epoch 43800:  6.679574198642513e-06\n",
      "-0.9866572618484497 0.003155819373205304\n",
      "Epoch 43900:  6.670083166682161e-06\n",
      "-0.9866573810577393 0.003155840327963233\n",
      "Epoch 44000:  6.701427537336713e-06\n",
      "-0.9866587519645691 0.0031558512710034847\n",
      "Epoch 44100:  6.654406661255052e-06\n",
      "-0.9866571426391602 0.0031560349743813276\n",
      "Epoch 44200:  6.646261226705974e-06\n",
      "-0.9866540431976318 0.0031557660549879074\n",
      "Epoch 44300:  6.655681318079587e-06\n",
      "-0.9866543412208557 0.0031558885239064693\n",
      "Epoch 44400:  6.597336778213503e-06\n",
      "-0.9866524934768677 0.0031558817718178034\n",
      "Epoch 44500:  6.6073116613551974e-06\n",
      "-0.9866503477096558 0.0031557963229715824\n",
      "Epoch 44600:  6.579317869181978e-06\n",
      "-0.9866487979888916 0.0031558689661324024\n",
      "Epoch 44700:  6.599282187380595e-06\n",
      "-0.9866480231285095 0.003155783051624894\n",
      "Epoch 44800:  6.563366696354933e-06\n",
      "-0.9866461753845215 0.0031558251939713955\n",
      "Epoch 44900:  6.568895514647011e-06\n",
      "-0.9866466522216797 0.0031558589544147253\n",
      "Epoch 45000:  6.612192464672262e-06\n",
      "-0.9866451621055603 0.0031556859612464905\n",
      "Epoch 45100:  6.5117646954604425e-06\n",
      "-0.9866412878036499 0.0031558992341160774\n",
      "Epoch 45200:  6.531576673296513e-06\n",
      "-0.9866412281990051 0.003155788639560342\n",
      "Epoch 45300:  6.526034212583909e-06\n",
      "-0.986642062664032 0.003155686892569065\n",
      "Epoch 45400:  6.501525604107883e-06\n",
      "-0.9866403341293335 0.0031558945775032043\n",
      "Epoch 45500:  6.453520200011553e-06\n",
      "-0.9866380095481873 0.0031557639595121145\n",
      "Epoch 45600:  6.469317213486647e-06\n",
      "-0.9866368174552917 0.0031556759495288134\n",
      "Epoch 45700:  6.490862688224297e-06\n",
      "-0.986636221408844 0.0031558640766888857\n",
      "Epoch 45800:  6.435880095523316e-06\n",
      "-0.9866347908973694 0.003155682235956192\n",
      "Epoch 45900:  6.441301138693234e-06\n",
      "-0.9866340756416321 0.003155770245939493\n",
      "Epoch 46000:  6.419140390789835e-06\n",
      "-0.986631453037262 0.0031558391638100147\n",
      "Epoch 46100:  6.426270374504384e-06\n",
      "-0.9866316914558411 0.003155922284349799\n",
      "Epoch 46200:  6.384008884197101e-06\n",
      "-0.9866304397583008 0.003155740676447749\n",
      "Epoch 46300:  6.442315680033062e-06\n",
      "-0.986629843711853 0.003155777230858803\n",
      "Epoch 46400:  6.381594175763894e-06\n",
      "-0.9866294264793396 0.003156030550599098\n",
      "Epoch 46500:  6.363840839185286e-06\n",
      "-0.9866260290145874 0.003155861748382449\n",
      "Epoch 46600:  6.32503997621825e-06\n",
      "-0.9866235852241516 0.003155795391649008\n",
      "Epoch 46700:  6.350190687953727e-06\n",
      "-0.9866227507591248 0.0031556652393192053\n",
      "Epoch 46800:  6.347200724121649e-06\n",
      "-0.9866243600845337 0.00315585033968091\n",
      "Epoch 46900:  6.3479710661340505e-06\n",
      "-0.9866249561309814 0.0031556941103190184\n",
      "Epoch 47000:  6.301659595919773e-06\n",
      "-0.9866224527359009 0.0031557600013911724\n",
      "Epoch 47100:  6.274107818171615e-06\n",
      "-0.986619234085083 0.0031557362526655197\n",
      "Epoch 47200:  6.267112894420279e-06\n",
      "-0.9866181015968323 0.0031558177433907986\n",
      "Epoch 47300:  6.247283181437524e-06\n",
      "-0.9866144061088562 0.003155551617965102\n",
      "Epoch 47400:  6.262182978389319e-06\n",
      "-0.9866151213645935 0.0031555891036987305\n",
      "Epoch 47500:  6.280482921283692e-06\n",
      "-0.9866166710853577 0.003155661514028907\n",
      "Epoch 47600:  6.228978691069642e-06\n",
      "-0.9866141676902771 0.0031556054018437862\n",
      "Epoch 47700:  6.234614374989178e-06\n",
      "-0.9866162538528442 0.003155727870762348\n",
      "Epoch 47800:  6.211496838659514e-06\n",
      "-0.9866143465042114 0.003155806567519903\n",
      "Epoch 47900:  6.1896889747004025e-06\n",
      "-0.9866117238998413 0.0031558217015117407\n",
      "Epoch 48000:  6.212669177330099e-06\n",
      "-0.9866111278533936 0.0031556133180856705\n",
      "Epoch 48100:  6.197944912855746e-06\n",
      "-0.9866083860397339 0.003155727172270417\n",
      "Epoch 48200:  6.183306595630711e-06\n",
      "-0.9866081476211548 0.00315576558932662\n",
      "Epoch 48300:  6.156626113806851e-06\n",
      "-0.9866069555282593 0.0031557248439639807\n",
      "Epoch 48400:  6.1690493566857185e-06\n",
      "-0.9866057634353638 0.003155691782012582\n",
      "Epoch 48500:  6.127603228378575e-06\n",
      "-0.9866057634353638 0.003155606798827648\n",
      "Epoch 48600:  6.134694103820948e-06\n",
      "-0.9866036176681519 0.003155621699988842\n",
      "Epoch 48700:  6.126137122919317e-06\n",
      "-0.9866026639938354 0.0031557190231978893\n",
      "Epoch 48800:  6.086981557018589e-06\n",
      "-0.986602246761322 0.003155656158924103\n",
      "Epoch 48900:  6.106992259446997e-06\n",
      "-0.986602783203125 0.0031558035407215357\n",
      "Epoch 48999:  6.077731541154208e-06\n",
      "-0.986600399017334 0.003155781188979745\n"
     ]
    }
   ],
   "source": [
    "epochs = 49000\n",
    "pinn.train()\n",
    "for i in range(epochs):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 100) == 0 or i == epochs-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(float(pinn.p1.detach().numpy()), float((pinn.p0.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pinn, \"pinn_expffth_cleanob_cleanlabel_1000labeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinn no ffth clean all\n",
    "# (-1.00022292137146, 0.0031830661464482546)\n",
    "# (0.011659960711956706, 0.01063217643403939)\n",
    "\n",
    "# pinn no ffth clean ob noisy label\n",
    "# (-1.0002213716506958, 0.003198811085894704)\n",
    "# (0.2578756208767374, 0.2357384558071573)\n",
    "\n",
    "# pinn no ffth noisy ob noisy label\n",
    "# (-1.009460687637329 0.0032692295499145985)\n",
    "# (1.8259720664182926, 0.8799033026853824)\n",
    "\n",
    "### ----- ###\n",
    "# pinn ffth clean all\n",
    "# (-0.9994307160377502, 0.0031862353649783444)\n",
    "# 0.07773227423228249\n",
    "\n",
    "# pinn ffth clean ob noisy label\n",
    "# (-1.0016363859176636 0.003176966216415167)\n",
    "# (0.17815066491893816, 0.014512073152580726)\n",
    "\n",
    "# pinn ffth noisy ob noisy label\n",
    "# (-1.0197374820709229 0.0031673626508563757)\n",
    "# (1.2340579276207333, 0.7396902794715519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sd = torch.load(\"...\")\n",
    "# est1 = abs(float(sd[\"p1\"].detach().numpy()))\n",
    "# est2 = float(sd[\"p0\"].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = abs(float((pinn.p1.detach().numpy())))\n",
    "est2 = (float((pinn.p0.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.986600399017334, 0.003155781188979745)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est1, est2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0990850509551593, 0.24087504731144238)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const = 0.01/np.pi\n",
    "errs = 100*npar([np.abs(est1-1), np.abs(est2-const)/const])\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
