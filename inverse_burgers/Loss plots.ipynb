{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n",
      "Running Python 3.9.9\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.style.use('science')\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "# Model selection\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Exact\n",
      "Training with 3000 samples\n",
      "Clean (x, t)\n",
      "Training with 3000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../experimental_data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.0\n",
    "noisy_xt = False\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "if noise_intensity>0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "else: print(\"Clean Exact\")\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 3000; include_unsup = True; load_idx = True\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "if load_idx: idx = np.load(\"./data_files/idx.npy\")\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "if noisy_xt and noise_intensity>0.0:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    # perturb2d(X_u_train, intensity=noise_intensity)\n",
    "    X_u_train = perturb(X_u_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Unsup data\n",
    "N_res = N\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "if load_idx: idx_res = np.load(\"./data_files/idx_res.npy\")\n",
    "X_res = X_star[idx_res, :]\n",
    "if include_unsup:\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        \n",
    "        return cat(uf, u_x, u_xx, u_xxx), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=1e-3):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])-(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 3.0])\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn, bi=False):\n",
    "        if bi: att = binarize(self.weighted_features(inn), self.th)\n",
    "        else: att = F.threshold(self.weighted_features(inn), self.th, 0.0)\n",
    "        return self.nonlinear_model(inn*att)\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        \n",
    "        l1 = mse_loss\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1+self.reg_intensity*(l2)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = torch.load('../saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained250labeledsamples_trained0unlabeledsamples_2.2e-03.pth')\n",
    "pretrained_state_dict = torch.load(\"./weights_nobn/pretrained_nobn.pth\")\n",
    "network_state_dict = None\n",
    "use_pretrained_weights = False\n",
    "lets_pretrain = True\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=None, dropout=None),\n",
    "                                    index2features=feature_names),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=False,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "    network_state_dict = semisup_model.network.state_dict()\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "Epoch 0:  2.022688750002999e-05\n",
      "Test MSE: 2.5e-05\n",
      "Computing derivatives features\n"
     ]
    }
   ],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining...\")\n",
    "    pretraining_epochs = 1 # 1, 5, 1\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(), \n",
    "                                     lr=1e-1, max_iter=500, \n",
    "                                     max_eval=int(500*1.25), history_size=300, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.train()\n",
    "    for i in range(pretraining_epochs):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        l = pretraining_closure()\n",
    "        if (i % 1) == 0:\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, _ = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    predictions, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(predictions[:N, :], u_train[:N, :]), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(2.4227e-05, grad_fn=<MseLossBackward0>), tensor(1.3096, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7075, 0.3541, 0.5176, 0.7357], grad_fn=<MeanBackward1>)\n",
      "tensor([1, 2, 0, 3])\n",
      "[tensor(0.0005, grad_fn=<MseLossBackward0>), tensor(0.6190, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7376, 0.4040, 0.6183, 0.7389], grad_fn=<MeanBackward1>)\n",
      "tensor([1, 2, 0, 3])\n",
      "[tensor(0.0011, grad_fn=<MseLossBackward0>), tensor(0.3179, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7464, 0.6887, 0.5571, 0.5990], grad_fn=<MeanBackward1>)\n",
      "tensor([2, 3, 1, 0])\n",
      "[tensor(0.0014, grad_fn=<MseLossBackward0>), tensor(0.1645, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7567, 0.7402, 0.5379, 0.3486], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0013, grad_fn=<MseLossBackward0>), tensor(0.1220, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7657, 0.7481, 0.4845, 0.2947], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n",
      "[tensor(0.0013, grad_fn=<MseLossBackward0>), tensor(0.0953, grad_fn=<AddBackward0>)]\n",
      "tensor([0.7765, 0.7466, 0.4558, 0.2779], grad_fn=<MeanBackward1>)\n",
      "tensor([3, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Joint training | Do lambda comparison here\n",
    "optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "optimizer.param_groups[0]['lr'] = 1e-7\n",
    "optimizer.param_groups[1]['lr'] = 1e-2\n",
    "\n",
    "# Use ~idx to sample adversarial data points\n",
    "for i in range(150):\n",
    "    semisup_model.train()\n",
    "    optimizer.step(pcgrad_closure)\n",
    "    if i%25==0:\n",
    "        loss = pcgrad_closure(return_list=True); print(loss)\n",
    "        fi = semisup_model.selector.latest_weighted_features\n",
    "        print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.725307917397004e-06\n",
      "2.0515221876848955e-06\n",
      "2.050543116638437e-06\n",
      "2.0497620880632894e-06\n",
      "2.049087242994574e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=500, max_eval=int(1.25*500), history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(5):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%1==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps = min(100, t.shape[0]) # 46, 100\n",
    "n_test = min(x.shape[0]*t_steps, X_star.shape[0])\n",
    "idx_test = np.arange(n_test)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003073 +0.000000i)u_xx\n",
      "    + (-0.969320 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Set normalize=1\n",
    "max_feature = 5\n",
    "w = TrainSTRidge(X_input[:, :max_feature], y_input, lam=1e-6, d_tol=2, l0_penalty=5, normalize=2) # d_tol=2, 5, 10\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:max_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_star = u_star.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 79.44it/s]\n"
     ]
    }
   ],
   "source": [
    "end = 100\n",
    "performances = []\n",
    "for i in trange(1, end+1):\n",
    "    NUMBER = min(x.shape[0]*i, X_star.shape[0])\n",
    "    xin, tin = X_star[:NUMBER, 0:1], X_star[:NUMBER, 1:2]\n",
    "    uf = semisup_model.network(xin, tin).detach().numpy()\n",
    "    performances.append(relative_l2_error(uf, u_star[:NUMBER].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x189 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(T[:, 0], performances)\n",
    "plt.axvline(x=0.45, color='black', linestyle='--', label=\"t = 0.45\")\n",
    "plt.ylabel(\"Relative $L_{2}$ error\")\n",
    "plt.xlabel('t')\n",
    "plt.title(\"Noiseless\")\n",
    "# plt.title(\"$u$ + Noise\")\n",
    "# plt.title(\"$u$ + Noise \\& $(x, t)$ + Noise\")\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"./vis_pics/loss_plots/loss_plot_cleanall.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"./weights_nobn/semisup_model_nobn_3000_3000_cleanall_unfinetuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l0bnb import fit_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Data.\n",
      "BnB Started.\n",
      "Iteration: 1. Number of non-zeros:  1\n",
      "Iteration: 2. Number of non-zeros:  3\n"
     ]
    }
   ],
   "source": [
    "sols = fit_path(X_input.astype(np.float64), \n",
    "             np.ravel(y_input).astype(np.float64), \n",
    "             lambda_2 = 1e-2, max_nonzeros = 3, normalize=True, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.92243960e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -9.29647500e-02,  0.00000000e+00,  0.00000000e+00, -2.78928646e-06,\n",
       "        0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sols[-1][\"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uf',\n",
       " 'u_x',\n",
       " 'u_xx',\n",
       " 'u_xxx',\n",
       " 'uf*u_x',\n",
       " 'uf*u_xx',\n",
       " 'uf*u_xxx',\n",
       " 'u_x*u_xx',\n",
       " 'u_x*u_xxx',\n",
       " 'u_xx*u_xxx']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abess import abessLm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abessLm(always_select=[2], support_size=range(0, 3))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abess_model = abessLm(always_select=[2], support_size=range(3))\n",
    "abess_model.fit(X_input, y_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.00307263,  0.        , -0.96933081,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abess_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pysindy as ps\n",
    "# from pysindy import SINDy\n",
    "# from pysindy.optimizers import STLSQ\n",
    "\n",
    "# fl = ps.feature_library.PolynomialLibrary(degree=1, include_interaction=False, include_bias=False)\n",
    "# opt = STLSQ(threshold=1e-4)\n",
    "# model = SINDy(optimizer=opt, feature_library=fl, differentiation_method=None, feature_names=[\"u_xx\", \"uu_x\"])\n",
    "\n",
    "# model.fit(x=X_input[:, [2, 4]], x_dot=y_input, t=0.01)\n",
    "# model.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No selector network (Not the same as setting lambda_1 to 0.0) \n",
    "# varying lambda_{1} | lambda_{0} = 1e-5 or 1e-6 (the same results anyway)\n",
    "# PDE derived using STRidge\n",
    "# u_t = (0.002543 +0.000000i)u_xx\n",
    "#     + (-0.832252 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-1\n",
    "# tensor([0.2735, 0.1883, 0.2060, 0.2133])\n",
    "# u_t = (0.002695 +0.000000i)u_xx\n",
    "#     + (-0.924525 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-3\n",
    "# tensor([0.8153, 0.8142, 0.4398, 0.1562])\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-5\n",
    "# tensor([0.6721, 0.7903, 0.7139, 0.3486])\n",
    "# u_t = (0.002594 +0.000000i)u_xx\n",
    "#     + (-0.885143 +0.000000i)uf*u_x\n",
    "\n",
    "# varying lambda_{0} | lambda_{1} = 1e-3\n",
    "# 1e-1\n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.066179 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-3\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "\n",
    "# 1e-5\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goals!!!\n",
    "# 3000 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "# 1000 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "# 500 | eq found by STR | eq after PINN fine-tuning + mean error\n",
    "\n",
    "# 3000\n",
    "# eq found by STR \n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "# eq after PINN fine-tuning + mean error\n",
    "# (-0.9994307160377502, 0.0031862353649783444)\n",
    "# (0.07773227423228249, )\n",
    "\n",
    "# 1000\n",
    "# eq found by STR\n",
    "# u_t = (0.002631 +0.000000i)u_xx\n",
    "#     + (-0.860374 +0.000000i)uf*u_x\n",
    "# eq after PINN fine-tuning + mean error\n",
    "# (-0.9891335368156433, 0.0031485233921557665)\n",
    "# (1.0864333669569826, 0.0002129514786863851)\n",
    "\n",
    "# 500\n",
    "# eq found by STR (failed)\n",
    "# u_t = (-0.072116 +0.000000i)uf*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discover under [-1, 1]x[0, 0.45]\n",
    "\n",
    "# Clean all\n",
    "# [0.8153, 0.8142, 0.4398, 0.1562]\n",
    "# u_t = (0.003090 +0.000000i)u_xx\n",
    "#     + (-0.970158 +0.000000i)uf*u_x\n",
    "# full domain\n",
    "# u_t = (0.003073 +0.000000i)u_xx\n",
    "#     + (-0.969320 +0.000000i)uf*u_x\n",
    "\n",
    "# Clean (x, t), and noisy labels\n",
    "# [0.7840, 0.7994, 0.4154, 0.2193]\n",
    "# u_t = (0.002942 +0.000000i)u_xx\n",
    "#     + (-0.961846 +0.000000i)uf*u_x\n",
    "# full domain\n",
    "# u_t = (0.002905 +0.000000i)u_xx\n",
    "#     + (-0.925471 +0.000000i)uf*u_x\n",
    "\n",
    "# Noisy (x, t), and noisy labels | relative l2 error = 0.06251628\n",
    "# [0.7423, 0.7245, 0.6249, 0.2471]\n",
    "# u_t = (0.005327 +0.000000i)u_xx\n",
    "#     + (-0.826977 +0.000000i)uf*u_x\n",
    "# full domain\n",
    "# u_t = (0.001513 +0.000000i)u_xx\n",
    "#     + (-0.506898 +0.000000i)uf*u_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
