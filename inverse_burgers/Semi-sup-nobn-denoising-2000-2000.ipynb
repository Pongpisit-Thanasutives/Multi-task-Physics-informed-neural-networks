{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n",
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "import sys; sys.path.insert(0, '../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "from models import RobustPCANN\n",
    "\n",
    "# Model selection\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from pytorch_robust_pca import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed Exact with intensity = 0.01\n",
      "Training with 2000 samples\n",
      "Noisy (x, t)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"../experimental_data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.01\n",
    "noisy_xt = True\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "if noise_intensity>0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "else: print(\"Clean Exact\")\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000; include_unsup = False\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "if noisy_xt and noise_intensity>0.0:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_u_train = perturb(X_u_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Unsup data\n",
    "N_res = N\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "if include_unsup:\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingNetwork(nn.Module):\n",
    "    def __init__(self, model, init_cs=(0.5, 0.5), init_betas=(0.0, 0.0)):\n",
    "        super(DenoisingNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        \n",
    "        # FFTNN\n",
    "        self.in_fft_nn = FFTTh(c=init_cs[0])\n",
    "        self.out_fft_nn = FFTTh(c=init_cs[1])\n",
    "\n",
    "        # Robust Beta-PCA\n",
    "        self.inp_rpca = RobustPCANN(beta=init_betas[0], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "        self.out_rpca = RobustPCANN(beta=init_betas[1], is_beta_trainable=True, inp_dims=1, hidden_dims=32)\n",
    "        \n",
    "    def forward(self, X_input, X_input_noise, y_input, y_input_noise):\n",
    "        X_input, y_input = self.denoise(X_input, X_input_noise, y_input, y_input_noise)\n",
    "        return self.model(X_input), y_input\n",
    "    \n",
    "    def denoise(self, X_input, X_input_noise, y_input, y_input_noise):\n",
    "        # Denoising process\n",
    "        \n",
    "        # (1) Denoising FFT on (x, t)\n",
    "        # This line returns the approx. recon.\n",
    "        X_input_noise = cat(torch.fft.ifft(self.in_fft_nn(X_input_noise[1])*X_input_noise[0]).real.reshape(-1, 1), \n",
    "                            torch.fft.ifft(self.in_fft_nn(X_input_noise[3])*X_input_noise[2]).real.reshape(-1, 1))\n",
    "        X_input_noise = X_input-X_input_noise\n",
    "        X_input = self.inp_rpca(X_input, X_input_noise, normalize=True)\n",
    "\n",
    "        # (2)Denoising FFT on y_input\n",
    "        y_input_noise = y_input-torch.fft.ifft(self.out_fft_nn(y_input_noise[1])*y_input_noise[0]).real.reshape(-1, 1)\n",
    "        y_input = self.out_rpca(y_input, y_input_noise, normalize=True)\n",
    "        \n",
    "        return X_input, y_input\n",
    "    \n",
    "    def loss(self, v1, v2):\n",
    "        return F.mse_loss(v1, v2, reduction='mean')\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.model(cat(x, t))\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        \n",
    "        return cat(uf, u_x, u_xx, u_xxx), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fft, x_PSD = fft1d_denoise(X_u_train[:, 0:1], c=-5, return_real=True)\n",
    "_, t_fft, t_PSD = fft1d_denoise(X_u_train[:, 1:2], c=-5, return_real=True)\n",
    "_, u_train_fft, u_train_PSD = fft1d_denoise(u_train, c=-5, return_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "solver = DenoisingNetwork(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=None, dropout=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n",
      "Epoch 0:  0.001017270958982408\n",
      "Epoch 100:  0.00010209782340098172\n"
     ]
    }
   ],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining...\")\n",
    "    pretraining_optimizer = LBFGSNew(solver.parameters(), \n",
    "                                     lr=1e-1, max_iter=300, \n",
    "                                     max_eval=int(300*1.25), history_size=150, \n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    solver.train()\n",
    "    for i in range(200):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled(): pretraining_optimizer.zero_grad()\n",
    "            preds, adjusted_grounds = solver(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), \n",
    "                                  u_train, (u_train_fft, u_train_PSD))\n",
    "            mse_loss = solver.loss(preds, adjusted_grounds)\n",
    "            if mse_loss.requires_grad: mse_loss.backward(retain_graph=True)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        if (i % 100) == 0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fine-tuning the solver network\n",
    "# f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300)\n",
    "\n",
    "# def finetuning_closure():\n",
    "#     global N, X_train, u_train\n",
    "#     if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "#     # the solver network only consider the first N samples.\n",
    "#     loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train[:N, :])), u_train[:N, :])\n",
    "#     if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "#     return loss\n",
    "\n",
    "# semisup_model.network.train()\n",
    "# semisup_model.selector.eval()\n",
    "\n",
    "# for i in range(50):\n",
    "#     f_opt.step(finetuning_closure)\n",
    "#     if i%10==0:\n",
    "#         loss = finetuning_closure()\n",
    "#         print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = min(50000, X_star.shape[0])\n",
    "idx_test = np.arange(n_test)\n",
    "referenced_derivatives, u_t = solver.get_selector_data(*dimension_slicing(X_star[idx_test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.000021 +0.000000i)u_xx\n",
      "    + (-0.011102 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Set normalize=1\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, d_tol=2)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
