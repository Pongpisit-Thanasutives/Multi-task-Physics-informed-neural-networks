{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "from models import RobustPCANN\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "\n",
    "from pytorch_robust_pca import *\n",
    "\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange\n",
    "\n",
    "import sympy\n",
    "import sympytorch\n",
    "\n",
    "from pde_diff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is arranged in an uniform grid\n",
      "Fine-tuning with 20000 samples\n",
      "Clean (x, t)\n",
      "Clean labels\n",
      "Not including N_res\n"
     ]
    }
   ],
   "source": [
    "include_N_res = False\n",
    "\n",
    "# DATA_PATH = '../deephpms_data/KS.mat'\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True)\n",
    "X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "N = 20000 #20000, 30000, 60000\n",
    "print(f\"Fine-tuning with {N} samples\")\n",
    "# idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "idx = np.arange(N)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "noise_intensity = 0.01\n",
    "noisy_xt = False; noisy_labels = False\n",
    "if noisy_xt: X_u_train = perturb(X_u_train, noise_intensity); print(\"Noisy (x, t)\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "if noisy_labels: u_train = perturb(u_train, noise_intensity); print(\"Noisy labels\")\n",
    "else: print(\"Clean labels\")\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Fine-tuning with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_u_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names, base on the symbolic regression results (only the important features)\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxxx'); feature2index = {}\n",
    "\n",
    "# del X_star, u_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.92*u_x*uf - 0.97*u_xx - 0.902*u_xxxx {u_xx, uf, u_xxxx, u_x}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymPyModule(expressions=(-0.92*u_x*uf - 0.97*u_xx - 0.902*u_xxxx,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noisy (x, t) and noisy labels\n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.912049 +0.000000i)u_xx\n",
    "#     + (-0.909050 +0.000000i)u_xxxx\n",
    "#     + (-0.951584 +0.000000i)uf*u_x\n",
    "\n",
    "# Clean (x, t) but noisy labels\n",
    "# PDE derived using STRidge\n",
    "# u_t = (-0.942656 +0.000000i)u_xx\n",
    "#     + (-0.900600 +0.000000i)u_xxxx\n",
    "#     + (-0.919862 +0.000000i)uf*u_x\n",
    "\n",
    "# Clean all\n",
    "program = '''\n",
    "-0.97*u_xx-0.902*u_xxxx-0.920*uf*u_x\n",
    "''' \n",
    "\n",
    "pde_expr, variables = build_exp(program); print(pde_expr, variables)\n",
    "mod = sympytorch.SymPyModule(expressions=[pde_expr]); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, pretrained=False, noiseless_mode=True, init_cs=(0.5, 0.5), init_betas=(0.0, 0.0)):\n",
    "        super(RobustPINN, self).__init__()\n",
    "        self.model = model\n",
    "        if not pretrained: self.model.apply(self.xavier_init)\n",
    "        \n",
    "        self.noiseless_mode = noiseless_mode\n",
    "        if self.noiseless_mode: print(\"No denoising\")\n",
    "        else: print(\"With denoising method\")\n",
    "        \n",
    "        self.in_fft_nn = None; self.out_fft_nn = None\n",
    "        self.inp_rpca = None; self.out_rpca = None\n",
    "        if not self.noiseless_mode:\n",
    "            # FFTNN\n",
    "            self.in_fft_nn = FFTTh(c=init_cs[0])\n",
    "            self.out_fft_nn = FFTTh(c=init_cs[1])\n",
    "\n",
    "            # Robust Beta-PCA\n",
    "            self.inp_rpca = RobustPCANN(beta=init_betas[0], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "            self.out_rpca = RobustPCANN(beta=init_betas[1], is_beta_trainable=True, inp_dims=1, hidden_dims=32)\n",
    "        \n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.init_parameters = [nn.Parameter(torch.tensor(x.item())) for x in loss_fn.parameters()]\n",
    "        self.param0 = self.init_parameters[0]\n",
    "        self.param1 = self.init_parameters[1]\n",
    "        self.param2 = self.init_parameters[2]\n",
    "        del self.callable_loss_fn, self.init_parameters\n",
    "        \n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        H = torch.cat([x, t], dim=1)\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, X_input, X_input_noise, y_input, y_input_noise, update_network_params=True, update_pde_params=True):\n",
    "        # Denoising process\n",
    "        if not self.noiseless_mode:\n",
    "            # (1) Denoising FFT on (x, t)\n",
    "            # This line returns the approx. recon.\n",
    "            X_input_noise = cat(torch.fft.ifft(self.in_fft_nn(X_input_noise[1])*X_input_noise[0]).real.reshape(-1, 1), \n",
    "                                torch.fft.ifft(self.in_fft_nn(X_input_noise[3])*X_input_noise[2]).real.reshape(-1, 1))\n",
    "            X_input_noise = X_input-X_input_noise\n",
    "            X_input = self.inp_rpca(X_input, X_input_noise, normalize=True)\n",
    "            \n",
    "            # (2) Denoising FFT on y_input\n",
    "            y_input_noise = y_input-torch.fft.ifft(self.out_fft_nn(y_input_noise[1])*y_input_noise[0]).real.reshape(-1, 1)\n",
    "            y_input = self.out_rpca(y_input, y_input_noise, normalize=True)\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(X_input[:, 0:1], X_input[:, 1:2])\n",
    "        \n",
    "        total_loss = []\n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            mse_loss = F.mse_loss(grads_dict[\"uf\"], y_input)\n",
    "            total_loss.append(mse_loss)\n",
    "            \n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            u_t_pred = (self.param0*grads_dict[\"uf\"]*grads_dict[\"u_x\"])+(self.param1*grads_dict[\"u_xx\"])+(self.param2*grads_dict[\"u_xxxx\"])\n",
    "            l_eq = F.mse_loss(u_t_pred, u_t)\n",
    "            total_loss.append(l_eq)\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]        \n",
    "        return {\"uf\":uf, \"u_x\":u_x, \"u_xx\":u_xx, \"u_xxxx\":u_xxxx}, u_t\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]\n",
    "        u_xxxxx = self.gradients(u_xxxx, x)[0]\n",
    "        derivatives = []\n",
    "        derivatives.append(uf)\n",
    "        derivatives.append(u_x)\n",
    "        derivatives.append(u_xx)\n",
    "        derivatives.append(u_xxx)\n",
    "        derivatives.append(u_xxxx)\n",
    "        derivatives.append(u_xxxxx)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp): \n",
    "        return -1.0+2.0*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "No denoising\n",
      "Loaded the model's weights properly\n",
      "No denoising\n"
     ]
    }
   ],
   "source": [
    "noiseless_mode = True\n",
    "model = TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)\n",
    "\n",
    "# Pretrained model\n",
    "semisup_model_state_dict = cpu_load(\"../inverse_KS/saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained30000labeledsamples_trained0unlabeledsamples.pth\")\n",
    "parameters = OrderedDict()\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "model.load_state_dict(parameters)\n",
    "\n",
    "pinn = RobustPINN(model=model, loss_fn=mod, \n",
    "                  index2features=feature_names, scale=True, lb=lb, ub=ub, \n",
    "                  pretrained=True, noiseless_mode=True)\n",
    "pinn = load_weights(pinn, \"./new_saved_path/KS_rudy_pretrained_pinn_2ndrun.pth\")\n",
    "\n",
    "# assigning the prefered loss_fn\n",
    "model = pinn.model\n",
    "pinn = RobustPINN(model=model, loss_fn=mod, \n",
    "                  index2features=feature_names, scale=True, lb=lb, ub=ub, \n",
    "                  pretrained=True, noiseless_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use STRidge to discover the hidden relation (on top of the pretrained solver net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_pretrain = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    xx, tt = X_u_train[:, 0:1], X_u_train[:, 1:2]\n",
    "\n",
    "    pretraining_optimizer = LBFGSNew(pinn.model.parameters(),\n",
    "                                     lr=1e-1, max_iter=500,\n",
    "                                     max_eval=int(500*1.25), history_size=300,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    model.train()\n",
    "    for i in range(5): # 1, 5, 200\n",
    "        def pretraining_closure():\n",
    "            global xx, tt, u_train\n",
    "            if torch.is_grad_enabled(): pretraining_optimizer.zero_grad()\n",
    "            mse_loss = F.mse_loss(pinn(xx, tt), u_train)\n",
    "            if mse_loss.requires_grad: mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "        if (i%1)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inn = u_star\n",
    "# Ut, R, rhs_des = build_linear_system(inn.reshape(x.shape[0], t.shape[0]), t[1]-t[0], x[2]-x[1], D=5, P=5, time_diff = 'FD', space_diff = 'FD')\n",
    "# w = TrainSTRidge(R,Ut,10**-6,50)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, rhs_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx, tt = X_u_train[:, 0:1], X_u_train[:, 1:2]\n",
    "\n",
    "NUMBER = 21000 # 20000, 21000\n",
    "NUMBER = min(NUMBER, X_star.shape[0])\n",
    "xx, tt = X_star[:NUMBER, 0:1], X_star[:NUMBER, 1:2]\n",
    "# xx, tt = torch.FloatTensor(xx).requires_grad_(True), torch.FloatTensor(tt).requires_grad_(True)\n",
    "\n",
    "uf = pinn(xx, tt)\n",
    "u_t = pinn.gradients(uf, tt)[0]\n",
    "\n",
    "### PDE Loss calculation ###\n",
    "# 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "u_x = pinn.gradients(uf, xx)[0]\n",
    "u_xx = pinn.gradients(u_x, xx)[0]\n",
    "u_xxx = pinn.gradients(u_xx, xx)[0]\n",
    "u_xxxx = pinn.gradients(u_xxx, xx)[0]\n",
    "# u_xxxxx = pinn.gradients(u_xxxx, xx)[0]\n",
    "\n",
    "derivatives = []\n",
    "derivatives.append(uf)\n",
    "derivatives.append(u_x)\n",
    "derivatives.append(u_xx)\n",
    "# derivatives.append(u_xxx)\n",
    "derivatives.append(u_xxxx)\n",
    "# derivatives.append(u_xxxxx)\n",
    "derivatives = torch.cat(derivatives, dim=1)\n",
    "\n",
    "derivatives = derivatives.detach().numpy()\n",
    "u_t = u_t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"uf\", \"u_x\", \"u_xx\", \"u_xxxx\"]\n",
    "\n",
    "X_input = derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names): poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.942890 +0.000000i)u_xx\n",
      "    + (-0.901222 +0.000000i)u_xxxx\n",
      "    + (-0.918244 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# # Set normalize=1\n",
    "# TrainSTRidge(X_input[:, :], y_input, 1e-6, 100, normalize=1) 1st run\n",
    "# TrainSTRidge(X_input[:, :], y_input, 1e-6, 10, normalize=1) 2nd run\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 10, normalize=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(pinn, \"./new_saved_path/KS_rudy_noisy_pretrained_pinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fft, x_PSD = fft1d_denoise(X_u_train[:, 0:1], c=-5, return_real=True)\n",
    "_, t_fft, t_PSD = fft1d_denoise(X_u_train[:, 1:2], c=-5, return_real=True)\n",
    "_, u_train_fft, u_train_PSD = fft1d_denoise(u_train, c=-5, return_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fft, x_PSD = x_fft.detach(), x_PSD.detach()\n",
    "t_fft, t_PSD = t_fft.detach(), t_PSD.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = updated_grads[0][idx]+updated_grads[1][idx]\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2.5395e-05, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.0003, grad_fn=<MseLossBackward0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.0006905767950229347\n",
      "Parameter containing:\n",
      "tensor(-1.0096, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0072, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0147, requires_grad=True)\n",
      "Epoch 10:  0.0003737722581718117\n",
      "Parameter containing:\n",
      "tensor(-1.0096, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0073, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0145, requires_grad=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7d1bf878b309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1st Phase optimization using Adam with PCGrad gradient modification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moptimizer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtl_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mepochs1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtl_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/madgrad/madgrad.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# step counter must be stored in state to ensure correct behavior under\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-4654108d707f>\u001b[0m in \u001b[0;36mmtl_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmtl_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpinn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_PSD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_PSD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu_train_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train_PSD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_network_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_pde_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mupdated_grads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0d0360506dec>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X_input, X_input_noise, y_input, y_input_noise, update_network_params, update_pde_params)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0my_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_rpca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mgrads_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0d0360506dec>\u001b[0m in \u001b[0;36mgrads_dict\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mu_xx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mu_xxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_xx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mu_xxxx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_xxx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"uf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u_x\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u_xx\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu_xx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"u_xxxx\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mu_xxxx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0d0360506dec>\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(self, func, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mneural_net_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_vmap_internals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_pass_through\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             allow_unused, accumulate_grad=False)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs1, epochs2 = 200, 20\n",
    "# TODO: Save best state dict and training for more epochs.\n",
    "optimizer1 = MADGRAD(pinn.parameters(), lr=1e-5, momentum=0.9)\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.param0, pinn.param1, pinn.param2)\n",
    "        \n",
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=300, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 5) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9700000286102295, -0.9020000100135803, -0.9200000166893005]\n"
     ]
    }
   ],
   "source": [
    "pred_params = [pinn.param0.item(), pinn.param1.item(), pinn.param2.item()]\n",
    "print(pred_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.933331489562988 2.876727299778854\n"
     ]
    }
   ],
   "source": [
    "errs = 100*np.abs(np.array(pred_params)+1)\n",
    "print(errs.mean(), errs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pinn, \"./new_saved_path/KS_rudy_final_finetuned_no_dft_pinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results ###\n",
    "# finetuned pinn on KS_new.pkl\n",
    "\n",
    "# w/o DFT\n",
    "# clean all\n",
    "# [-1.0038105249404907, -1.0389138460159302, -1.018654465675354]\n",
    "# 2.0459612210591636 1.4387603533171842\n",
    "\n",
    "# w/o DFT\n",
    "# clean (x, t) and noisy lables\n",
    "# 1st run\n",
    "# [-0.9552096724510193, -1.0218358039855957, -1.0091873407363892]\n",
    "# 2.527115742365519 1.4736449310642017\n",
    "# 2nd run\n",
    "# [-0.9824689626693726, -1.0259006023406982, -1.0224767923355103]\n",
    "# 2.1969477335611978 0.34356397750920376\n",
    "\n",
    "# w/o DFT\n",
    "# noisy (x, t) and noisy lables\n",
    "# [-0.9445462822914124, -1.0215760469436646, -1.0037590265274048]\n",
    "# 2.692959705988566 2.1441092139699642\n",
    "\n",
    "###############################\n",
    "# w/ DFT\n",
    "# clean all\n",
    "# [-0.9454907774925232, -1.0266990661621094, -1.0051020383834839]\n",
    "# 2.8770109017690024 2.0223490873202765\n",
    "\n",
    "# w/ DFT\n",
    "# clean (x, t) and noisy lables\n",
    "# [-0.965919554233551, -1.0234534740447998, -1.0168591737747192]\n",
    "# 2.479769786198934 0.7094516920660559\n",
    "\n",
    "# w/ DFT\n",
    "# noisy (x, t) and noisy lables\n",
    "# [-0.9662449955940247, -1.0259292125701904, -1.0150245428085327]\n",
    "# 2.4902919928232827 0.7681037488669546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results ###\n",
    "\n",
    "# w/o DFT\n",
    "# noisy labels\n",
    "# [-0.9948846697807312, -1.0065652132034302, -0.9950999617576599]\n",
    "# 0.5526860555013021 0.07394681988603513\n",
    "\n",
    "# noisy (x, t) and labels\n",
    "# [-0.9892916083335876, -1.011358618736267, -0.9886858463287354]\n",
    "# 1.1127054691314697 0.029659549537678777\n",
    "\n",
    "# /w DFT\n",
    "# clean | /w DFT\n",
    "# [-0.9948897361755371, -1.0049610137939453, -0.9951441287994385]\n",
    "# 0.4975716272989909 0.01043744028051521\n",
    "\n",
    "# noisy (x, t) and labels | /w DFT\n",
    "# [-0.9947568774223328, -1.0052711963653564, -0.9935149550437927]\n",
    "# 0.5666454633076986 0.05789442242742208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder ###\n",
    "\n",
    "# Clean Exact and (x, t)\n",
    "# [-0.999634325504303, -0.9994997382164001, -0.9995566010475159]\n",
    "# (0.04364450772603353, 0.005516461306387781)\n",
    "\n",
    "# Pretrained with final_finetuned_pinn_5000 (do not use / report)\n",
    "# [-0.9996052980422974, -0.9995099902153015, -0.9995319247245789]\n",
    "# 0.04509290059407552 0.0040754479422416435\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-0.9967969655990601, -0.9969800114631653, -0.9973703026771545]\n",
    "# (0.2950906753540039, 0.023910676954986623)\n",
    "\n",
    "# Noisy Exact and noisy (x, t)\n",
    "# [-0.9975059032440186, -0.9962050914764404, -0.9969104528427124]\n",
    "# 0.3126184145609538 0.05316856937153804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder & With Robust PCA ###\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-1.0003160238265991, -1.0005097389221191, -0.9997726678848267]\n",
    "# (0.035103162129720054, 0.011791963483533422)\n",
    "\n",
    "# Noisy Exact and noisy (x, t) + Robust PCA\n",
    "# [-1.000351071357727, -0.9991073608398438, -0.9980993866920471]\n",
    "# (0.08140603701273601 0.09209241474722876)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder & With Double Beta-Robust PCA ###\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-1.0013889074325562, -0.998389720916748, -1.0010501146316528]\n",
    "# 0.1349767049153646 0.023035484282371465\n",
    "\n",
    "# Noisy Exact and noisy (x, t) + Robust PCA\n",
    "# [-0.9980248212814331, -0.9982557892799377, -0.998869001865387]\n",
    "# 0.1616795857747396 0.03562172791384541"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
