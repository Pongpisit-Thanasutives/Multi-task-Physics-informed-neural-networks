{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from preprocess import *\n",
    "from models import RobustPCANN\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "\n",
    "from pytorch_robust_pca import *\n",
    "\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Tracking\n",
    "from tqdm import trange\n",
    "\n",
    "import sympy\n",
    "import sympytorch\n",
    "\n",
    "from pde_diff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../deephpms_data/KS_new2.pkl\n",
      "Clean (x, t)\n",
      "Total data points 29696\n",
      "Fine-tuning with 29696 samples\n"
     ]
    }
   ],
   "source": [
    "# Loading the KS sol\n",
    "DATA_PATH = \"../deephpms_data/KS_new2.pkl\"\n",
    "data = pickle_load(DATA_PATH)\n",
    "t = data['t']\n",
    "x = data['x']\n",
    "X, T = np.meshgrid(x, t)\n",
    "Exact = data['u'].T\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.0\n",
    "noisy_xt = False\n",
    "\n",
    "if noise_intensity > 0.0:\n",
    "    Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "    print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "\n",
    "x_star = X.T.flatten()[:, None]\n",
    "t_star = T.T.flatten()[:, None]\n",
    "X_star = np.hstack((x_star, t_star))\n",
    "u_star = Exact.flatten()[:, None]\n",
    "\n",
    "if noisy_xt: \n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "print(f\"Total data points {u_star.shape[0]}\")\n",
    "N = min(40000, u_star.shape[0])\n",
    "print(f\"Fine-tuning with {N} samples\")\n",
    "idx = np.random.choice(u_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = to_tensor(X_u_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names, base on the symbolic regression results\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxxx'); feature2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.014672*u_x*uf - 1.007216*u_xx - 1.00963*u_xxxx {u_xxxx, u_x, u_xx, uf}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymPyModule(expressions=(-1.014672*u_x*uf - 1.007216*u_xx - 1.00963*u_xxxx,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program = '''\n",
    "-1.048322*u_xx-1.055897*u_xxxx-0.997619*uf*u_x\n",
    "'''\n",
    "\n",
    "program = '''\n",
    "-0.41179*u_xx-0.348230*u_xxxx-0.332343*uf*u_x\n",
    "'''\n",
    "\n",
    "program = '''\n",
    "-1.007216*u_xx-1.0096300*u_xxxx-1.014672*uf*u_x\n",
    "''' \n",
    "\n",
    "pde_expr, variables = build_exp(program); print(pde_expr, variables)\n",
    "mod = sympytorch.SymPyModule(expressions=[pde_expr]); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, pretrained=False, noiseless_mode=True, init_cs=(0.5, 0.5), init_betas=(0.0, 0.0)):\n",
    "        super(RobustPINN, self).__init__()\n",
    "        self.model = model\n",
    "        if not pretrained: self.model.apply(self.xavier_init)\n",
    "        \n",
    "        self.noiseless_mode = noiseless_mode\n",
    "        if self.noiseless_mode: print(\"No denoising\")\n",
    "        else: print(\"With denoising method\")\n",
    "        \n",
    "        self.in_fft_nn = None; self.out_fft_nn = None\n",
    "        self.inp_rpca = None; self.out_rpca = None\n",
    "        if not self.noiseless_mode:\n",
    "            # FFTNN\n",
    "            self.in_fft_nn = FFTTh(c=init_cs[0])\n",
    "            self.out_fft_nn = FFTTh(c=init_cs[1])\n",
    "\n",
    "            # Robust Beta-PCA\n",
    "            self.inp_rpca = RobustPCANN(beta=init_betas[0], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "            self.out_rpca = RobustPCANN(beta=init_betas[1], is_beta_trainable=True, inp_dims=1, hidden_dims=32)\n",
    "        \n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.init_parameters = [nn.Parameter(torch.tensor(x.item())) for x in loss_fn.parameters()]\n",
    "        self.param0 = self.init_parameters[0]\n",
    "        self.param1 = self.init_parameters[1]\n",
    "        self.param2 = self.init_parameters[2]\n",
    "        del self.callable_loss_fn, self.init_parameters\n",
    "        \n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        H = torch.cat([x, t], dim=1)\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, X_input, X_input_noise, y_input, y_input_noise, update_network_params=True, update_pde_params=True):\n",
    "        # Denoising process\n",
    "        if not self.noiseless_mode:\n",
    "            # (1) Denoising FFT on (x, t)\n",
    "            # This line returns the approx. recon.\n",
    "            X_input_noise = cat(torch.fft.ifft(self.in_fft_nn(X_input_noise[1])*X_input_noise[0]).real.reshape(-1, 1), \n",
    "                                torch.fft.ifft(self.in_fft_nn(X_input_noise[3])*X_input_noise[2]).real.reshape(-1, 1))\n",
    "            X_input_noise = X_input-X_input_noise\n",
    "            X_input = self.inp_rpca(X_input, X_input_noise, normalize=True)\n",
    "            \n",
    "            # (2) Denoising FFT on y_input\n",
    "            y_input_noise = y_input-torch.fft.ifft(self.out_fft_nn(y_input_noise[1])*y_input_noise[0]).real.reshape(-1, 1)\n",
    "            y_input = self.out_rpca(y_input, y_input_noise, normalize=True)\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(X_input[:, 0:1], X_input[:, 1:2])\n",
    "        \n",
    "        total_loss = []\n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            mse_loss = F.mse_loss(grads_dict[\"uf\"], y_input)\n",
    "            total_loss.append(mse_loss)\n",
    "            \n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            u_t_pred = (self.param0*grads_dict[\"uf\"]*grads_dict[\"u_x\"])+(self.param1*grads_dict[\"u_xx\"])+(self.param2*grads_dict[\"u_xxxx\"])\n",
    "            l_eq = F.mse_loss(u_t_pred, u_t)\n",
    "            total_loss.append(l_eq)\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]        \n",
    "        return {\"uf\":uf, \"u_x\":u_x, \"u_xx\":u_xx, \"u_xxxx\":u_xxxx}, u_t\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]\n",
    "        u_xxxxx = self.gradients(u_xxxx, x)[0]\n",
    "        derivatives = []\n",
    "        derivatives.append(uf)\n",
    "        derivatives.append(u_x)\n",
    "        derivatives.append(u_xx)\n",
    "        derivatives.append(u_xxx)\n",
    "        derivatives.append(u_xxxx)\n",
    "        derivatives.append(u_xxxxx)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp): \n",
    "        return -1.0+2.0*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "No denoising\n",
      "Loaded the model's weights properly\n",
      "No denoising\n"
     ]
    }
   ],
   "source": [
    "model = TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)\n",
    "pinn = RobustPINN(model=model, loss_fn=mod, \n",
    "                  index2features=feature_names, scale=True, lb=lb, ub=ub, \n",
    "                  pretrained=True, noiseless_mode=True)\n",
    "pinn = load_weights(pinn, \"./new_saved_path/KS_new2_pretrained_pinn.pth\")\n",
    "\n",
    "# assigning the prefered loss_fn\n",
    "model = pinn.model\n",
    "pinn = RobustPINN(model=model, loss_fn=mod, \n",
    "                  index2features=feature_names, scale=True, lb=lb, ub=ub, \n",
    "                  pretrained=True, noiseless_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use STRidge to discover the hidden relation (on top of the pretrained solver net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  1.4179110621626023e-05\n",
      "Epoch 1:  1.417903695255518e-05\n",
      "Epoch 2:  1.4178962373989634e-05\n",
      "Epoch 3:  1.4178842320689e-05\n",
      "Epoch 4:  1.417844759998843e-05\n"
     ]
    }
   ],
   "source": [
    "xx, tt = X_u_train[:, 0:1], X_u_train[:, 1:2]\n",
    "\n",
    "pretraining_optimizer = LBFGSNew(pinn.model.parameters(),\n",
    "                                 lr=1e-1, max_iter=500,\n",
    "                                 max_eval=int(500*1.25), history_size=300,\n",
    "                                 line_search_fn=True, batch_mode=False)\n",
    "\n",
    "model.train()\n",
    "for i in range(5):\n",
    "    def pretraining_closure():\n",
    "        global xx, tt, u_train\n",
    "        if torch.is_grad_enabled(): pretraining_optimizer.zero_grad()\n",
    "        mse_loss = F.mse_loss(pinn(xx, tt), u_train)\n",
    "        if mse_loss.requires_grad: mse_loss.backward(retain_graph=False)\n",
    "        return mse_loss\n",
    "\n",
    "    pretraining_optimizer.step(pretraining_closure)\n",
    "\n",
    "    if (i%1)==0:\n",
    "        l = pretraining_closure()\n",
    "        curr_loss = l.item()\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-1.007216 +0.000000i)uu_{x}\n",
      "    + (-1.009630 +0.000000i)u_{xx}\n",
      "    + (-1.014672 +0.000000i)u_{xxxx}\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "inn = u_star\n",
    "Ut, R, rhs_des = build_linear_system(inn.reshape(x.shape[0], t.shape[0]), t[1]-t[0], x[2]-x[1], D=5, P=5, time_diff = 'FD', space_diff = 'FD')\n",
    "w = TrainSTRidge(R,Ut,10**-6,50)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, rhs_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # xx, tt = X_u_train[:, 0:1], X_u_train[:, 1:2]\n",
    "\n",
    "# NUMBER = 30000\n",
    "# xx, tt = X_star[:NUMBER, 0:1], X_star[:NUMBER, 1:2]\n",
    "# xx, tt = torch.FloatTensor(xx).requires_grad_(True), torch.FloatTensor(tt).requires_grad_(True)\n",
    "\n",
    "# uf = pinn(xx, tt)\n",
    "# u_t = pinn.gradients(uf, tt)[0]\n",
    "\n",
    "# ### PDE Loss calculation ###\n",
    "# # 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "# u_x = pinn.gradients(uf, xx)[0]\n",
    "# u_xx = pinn.gradients(u_x, xx)[0]\n",
    "# u_xxx = pinn.gradients(u_xx, xx)[0]\n",
    "# u_xxxx = pinn.gradients(u_xxx, xx)[0]\n",
    "# # u_xxxxx = pinn.gradients(u_xxxx, xx)[0]\n",
    "\n",
    "# derivatives = []\n",
    "# derivatives.append(uf)\n",
    "# derivatives.append(u_x)\n",
    "# derivatives.append(u_xx)\n",
    "# derivatives.append(u_xxx)\n",
    "# derivatives.append(u_xxxx)\n",
    "# # derivatives.append(u_xxxxx)\n",
    "# derivatives = torch.cat(derivatives, dim=1)\n",
    "\n",
    "# derivatives = derivatives.detach().numpy()\n",
    "# u_t = u_t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = [\"uf\", \"u_x\", \"u_xx\", \"u_xxx\", \"u_xxxx\"]\n",
    "\n",
    "# X_input = derivatives\n",
    "# y_input = u_t\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# X_input = poly.fit_transform(X_input)\n",
    "\n",
    "# poly_feature_names = poly.get_feature_names(feature_names)\n",
    "# for i, f in enumerate(poly_feature_names): poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Set normalize=1\n",
    "# w = TrainSTRidge(X_input[:, :], y_input, 1e-2, 100, l0_penalty=1, normalize=1)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, x_fft, x_PSD = fft1d_denoise(X_u_train[:, 0:1], c=-5, return_real=True)\n",
    "_, t_fft, t_PSD = fft1d_denoise(X_u_train[:, 1:2], c=-5, return_real=True)\n",
    "_, u_train_fft, u_train_PSD = fft1d_denoise(u_train, c=-5, return_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_fft, x_PSD = x_fft.detach(), x_PSD.detach()\n",
    "t_fft, t_PSD = t_fft.detach(), t_PSD.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    losses = pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = updated_grads[0][idx]+updated_grads[1][idx]\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.4178e-05, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.0428, grad_fn=<MseLossBackward0>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.loss(X_u_train, (x_fft, x_PSD, t_fft, t_PSD), u_train, (u_train_fft, u_train_PSD), update_network_params=True, update_pde_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.04005521535873413\n",
      "Parameter containing:\n",
      "tensor(-1.0096, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0072, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0146, requires_grad=True)\n",
      "Epoch 10:  0.016976967453956604\n",
      "Parameter containing:\n",
      "tensor(-1.0091, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0081, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0137, requires_grad=True)\n",
      "Epoch 20:  0.01190744899213314\n",
      "Parameter containing:\n",
      "tensor(-1.0082, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0092, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0127, requires_grad=True)\n",
      "Epoch 29:  0.00998506136238575\n",
      "Parameter containing:\n",
      "tensor(-1.0071, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0103, requires_grad=True) Parameter containing:\n",
      "tensor(-1.0117, requires_grad=True)\n",
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.0016022277995944023\n",
      "Epoch 9:  0.0016022275667637587\n"
     ]
    }
   ],
   "source": [
    "epochs1, epochs2 = 30, 10\n",
    "# TODO: Save best state dict and training for more epochs.\n",
    "optimizer1 = MADGRAD(pinn.parameters(), lr=1e-5, momentum=0.9)\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.param0, pinn.param1, pinn.param2)\n",
    "        \n",
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=300, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 10) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.8170313239097595, -1.0996100902557373, -0.9884783029556274]\n"
     ]
    }
   ],
   "source": [
    "pred_params = [pinn.param0.item(), pinn.param1.item(), pinn.param2.item()]\n",
    "print(pred_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.803348779678345 7.0001813845422385\n"
     ]
    }
   ],
   "source": [
    "errs = 100*np.abs(np.array(pred_params)+1)\n",
    "print(errs.mean(), errs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results ###\n",
    "# finetuned pinn\n",
    "\n",
    "# w/o DFT\n",
    "# clean all\n",
    "# [-1.0038105249404907, -1.0389138460159302, -1.018654465675354]\n",
    "# 2.0459612210591636 1.4387603533171842\n",
    "\n",
    "# w/o DFT\n",
    "# clean (x, t) and noisy lables\n",
    "# 1st run\n",
    "# [-0.9552096724510193, -1.0218358039855957, -1.0091873407363892]\n",
    "# 2.527115742365519 1.4736449310642017\n",
    "# 2nd run\n",
    "# [-0.9824689626693726, -1.0259006023406982, -1.0224767923355103]\n",
    "# 2.1969477335611978 0.34356397750920376\n",
    "\n",
    "# w/o DFT\n",
    "# noisy (x, t) and noisy lables\n",
    "# [-0.9445462822914124, -1.0215760469436646, -1.0037590265274048]\n",
    "# 2.692959705988566 2.1441092139699642\n",
    "\n",
    "###############################\n",
    "# w/ DFT\n",
    "# clean all\n",
    "# [-0.9454907774925232, -1.0266990661621094, -1.0051020383834839]\n",
    "# 2.8770109017690024 2.0223490873202765\n",
    "\n",
    "# w/ DFT\n",
    "# clean (x, t) and noisy lables\n",
    "# [-0.965919554233551, -1.0234534740447998, -1.0168591737747192]\n",
    "# 2.479769786198934 0.7094516920660559\n",
    "\n",
    "# w/ DFT\n",
    "# noisy (x, t) and noisy lables\n",
    "# [-0.9662449955940247, -1.0259292125701904, -1.0150245428085327]\n",
    "# 2.4902919928232827 0.7681037488669546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results ###\n",
    "\n",
    "# w/o DFT\n",
    "# noisy labels\n",
    "# [-0.9948846697807312, -1.0065652132034302, -0.9950999617576599]\n",
    "# 0.5526860555013021 0.07394681988603513\n",
    "\n",
    "# noisy (x, t) and labels\n",
    "# [-0.9892916083335876, -1.011358618736267, -0.9886858463287354]\n",
    "# 1.1127054691314697 0.029659549537678777\n",
    "\n",
    "# /w DFT\n",
    "# clean | /w DFT\n",
    "# [-0.9948897361755371, -1.0049610137939453, -0.9951441287994385]\n",
    "# 0.4975716272989909 0.01043744028051521\n",
    "\n",
    "# noisy (x, t) and labels | /w DFT\n",
    "# [-0.9947568774223328, -1.0052711963653564, -0.9935149550437927]\n",
    "# 0.5666454633076986 0.05789442242742208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder ###\n",
    "\n",
    "# Clean Exact and (x, t)\n",
    "# [-0.999634325504303, -0.9994997382164001, -0.9995566010475159]\n",
    "# (0.04364450772603353, 0.005516461306387781)\n",
    "\n",
    "# Pretrained with final_finetuned_pinn_5000 (do not use / report)\n",
    "# [-0.9996052980422974, -0.9995099902153015, -0.9995319247245789]\n",
    "# 0.04509290059407552 0.0040754479422416435\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-0.9967969655990601, -0.9969800114631653, -0.9973703026771545]\n",
    "# (0.2950906753540039, 0.023910676954986623)\n",
    "\n",
    "# Noisy Exact and noisy (x, t)\n",
    "# [-0.9975059032440186, -0.9962050914764404, -0.9969104528427124]\n",
    "# 0.3126184145609538 0.05316856937153804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder & With Robust PCA ###\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-1.0003160238265991, -1.0005097389221191, -0.9997726678848267]\n",
    "# (0.035103162129720054, 0.011791963483533422)\n",
    "\n",
    "# Noisy Exact and noisy (x, t) + Robust PCA\n",
    "# [-1.000351071357727, -0.9991073608398438, -0.9980993866920471]\n",
    "# (0.08140603701273601 0.09209241474722876)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without AutoEncoder & With Double Beta-Robust PCA ###\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# [-1.0013889074325562, -0.998389720916748, -1.0010501146316528]\n",
    "# 0.1349767049153646 0.023035484282371465\n",
    "\n",
    "# Noisy Exact and noisy (x, t) + Robust PCA\n",
    "# [-0.9980248212814331, -0.9982557892799377, -0.998869001865387]\n",
    "# 0.1616795857747396 0.03562172791384541"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
