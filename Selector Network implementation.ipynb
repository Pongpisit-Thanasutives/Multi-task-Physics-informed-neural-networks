{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I need to implement the GPU version for faster computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "import shap\n",
    "from utils import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "#         self.model.apply(self.xavier_init)\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with noisy features -> chk feature importance\n",
    "        layers = [nn.Linear(X_train_dim, 50), nn.Tanh(), nn.Linear(50, 1)]\n",
    "        self.nonlinear_model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=simple_solver_model(50))\n",
    "# selector = SeclectorNetwork(X_train_dim=6)\n",
    "\n",
    "# optimizer = torch.optim.LBFGS(list(network.parameters()) + list(selector.parameters()), \n",
    "#                               lr=5e-2, max_iter=80, max_eval=100, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "# # optimizer = torch.optim.Adam(list(network.parameters()) + list(selector.parameters()), lr=1e-3)\n",
    "# epochs = 5000; testing = False\n",
    "\n",
    "# if testing:\n",
    "#     # unsupervised_loss\n",
    "#     unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)))\n",
    "#     sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "#     # No MTL yet, apply the naive summation first to see if it's working?\n",
    "#     total_loss = unsup_loss + sup_loss\n",
    "#     print(total_loss)\n",
    "\n",
    "#     total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network.train(); selector.train()\n",
    "# curr_loss = 1000\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     def closure():\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Total loss calculation process\n",
    "#         # unsupervised_loss\n",
    "#         unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)))\n",
    "#         sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "#         # No MTL yet, apply the naive summation first to see if it's working?\n",
    "#         total_loss = unsup_loss + sup_loss\n",
    "#         total_loss.backward()\n",
    "        \n",
    "#         return total_loss\n",
    "    \n",
    "#     optimizer.step(closure)\n",
    "    \n",
    "#     l = closure()\n",
    "#     if l.item() != curr_loss:\n",
    "#         curr_loss = l.item()\n",
    "#     else: break; print(\"Stop training.\")\n",
    "    \n",
    "#     if (i % 10) == 0:\n",
    "#         print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "# print(\"Testing\")\n",
    "# network.eval()\n",
    "# F.mse_loss(network(*dimension_slicing(X_star)).detach(), u_star) # Around 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_selector, y_selector = network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "# e = shap.DeepExplainer(selector, X_selector)\n",
    "# shap_values = e.shap_values(X_selector)\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\n",
    "#     \"mean_abs_shap\": np.mean(np.abs(shap_values), axis=0), \n",
    "#     \"stdev_abs_shap\": np.std(np.abs(shap_values), axis=0), \n",
    "#     \"name\": ['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']\n",
    "# })\n",
    "\n",
    "# print(df.sort_values(\"mean_abs_shap\", ascending=False)[:10])\n",
    "\n",
    "# shap.summary_plot(shap_values, features=X_selector, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using statistical models to find feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.load(\"./saved_path_inverse_burger/data/derivatives-2000.npy\")\n",
    "y_np = np.load(\"./saved_path_inverse_burger/data/dynamics-2000.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "R score: 0.9998759225862276\n",
      "MSE: 8.7067296e-05\n",
      "('u_x', 0.4371625238752445)\n",
      "('u_xx', 0.31032778078721196)\n",
      "('uf', 0.17357992155962132)\n",
      "('u_xt', 0.04297585172713631)\n",
      "('u_tt', 0.0359539220507859)\n",
      "('u_tx', 0.0)\n"
     ]
    }
   ],
   "source": [
    "model = xgboost.XGBRegressor()\n",
    "# model = RandomForestRegressor()\n",
    "# model = catboost.CatBoostRegressor(verbose=0)\n",
    "# model = lightgbm.LGBMRegressor()\n",
    "\n",
    "sklearn_model = SklearnModel(model=model, X_train=X_np, y_train=y_np, feature_names=feature_names)\n",
    "print('MSE:', sklearn_model.test(X_np, y_np))\n",
    "feature_importances = sklearn_model.feature_importance();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
