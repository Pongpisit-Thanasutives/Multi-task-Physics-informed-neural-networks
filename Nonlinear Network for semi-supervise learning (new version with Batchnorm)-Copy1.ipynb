{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "#         self.model.apply(self.xavier_init)\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], bn=bn)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "    def forward(self, X_u_train):\n",
    "        inn = X_u_train\n",
    "        if self.normalize_derivative_features:\n",
    "            inn = minmax_normalize(inn)\n",
    "        unsup_loss = self.selector.loss(*self.network.get_selector_data(*dimension_slicing(inn)))\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], bn=nn.LayerNorm)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf, u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train)), u_train)\n",
    "    mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb9c6404c0b4171b1a4c7684cb01343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 2.14E-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-8, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'SGD'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "# suggested_lr = float(input())\n",
    "optimizer1 = lr_finder.optimizer\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "epochs1 = 2000; epochs2 = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  7.372777938842773\n",
      "Epoch 100:  1.5587310791015625\n",
      "Epoch 200:  1.2774720191955566\n",
      "Epoch 300:  1.124897837638855\n",
      "Epoch 400:  1.016829252243042\n",
      "Epoch 500:  0.9325682520866394\n",
      "Epoch 600:  0.8625723123550415\n",
      "Epoch 700:  0.8020140528678894\n",
      "Epoch 800:  0.7486374974250793\n",
      "Epoch 900:  0.7019697427749634\n",
      "Epoch 1000:  0.6566792726516724\n",
      "Epoch 1100:  0.6192731857299805\n",
      "Epoch 1200:  0.5833610892295837\n",
      "Epoch 1300:  0.5476791858673096\n",
      "Epoch 1400:  0.5196598768234253\n",
      "Epoch 1500:  0.4956129193305969\n",
      "Epoch 1600:  0.4755100905895233\n",
      "Epoch 1700:  0.4580637812614441\n",
      "Epoch 1800:  0.4452587366104126\n",
      "Epoch 1900:  0.43433934450149536\n"
     ]
    }
   ],
   "source": [
    "semisup_model.train()\n",
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0003895397821906954\n",
      "Epoch 10:  1.300813210036722e-06\n",
      "Epoch 20:  7.559316941296856e-07\n",
      "Epoch 30:  6.600974415960081e-07\n",
      "Epoch 40:  5.95641438394523e-07\n",
      "Epoch 50:  5.210356448515086e-07\n",
      "Epoch 60:  5.126315159031947e-07\n",
      "Epoch 70:  4.765784069604706e-07\n",
      "Epoch 80:  4.392971959532588e-07\n",
      "Epoch 90:  4.133203219680581e-07\n",
      "Epoch 100:  3.981241434303229e-07\n",
      "Epoch 110:  3.959651735385705e-07\n",
      "Finishing the second stage\n",
      "Testing\n",
      "Test MSE: 2.2498645648738602e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 10) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# should be able to reach the order of 1e-6.\n",
    "# So that I can use this algo instead of the ladder networks\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNorm_without_physical_reg_trained2000samples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2499e-06, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_burger/semisup_model_with_LayerNorm_without_physical_reg_trained2000samples.pth\"))\n",
    "semisup_model.eval()\n",
    "F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-2000-V3.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-2000-V3.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-25600-V3.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-25600-V3.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
