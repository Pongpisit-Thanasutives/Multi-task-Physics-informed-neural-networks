{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pyKeOps]: Warning, no cuda detected. Switching to cpu only.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = 1000\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 2.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    if mse_loss.requires_grad:\n",
    "        mse_loss.backward(retain_graph=False)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n"
     ]
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = False\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "    \n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None\n",
    "    \n",
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "suggested_lr = 1e-5\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 2000; epochs2 = 50000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = 300\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training the generator for \")\n",
    "# best_generator_loss = 1000; best_generator_state_dict = None\n",
    "# generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-5, momentum=0.95)\n",
    "# for i in trange(1000):\n",
    "#     semisup_model.eval()\n",
    "#     generator.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#     unsup_loss = semisup_model(X_gen)[1]\n",
    "#     generator_loss = sinkhorn_loss(X_gen, X_u_train)\n",
    "# #     generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "#     generator_loss.backward(retain_graph=True)\n",
    "#     generator_optimizer.step()\n",
    "    \n",
    "#     if generator_loss.item() < best_generator_loss:\n",
    "#         best_generator_loss = generator_loss.item()\n",
    "#         best_generator_state_dict = generator.state_dict()\n",
    "#         print(best_generator_loss)\n",
    "        \n",
    "#     if i%100==0:\n",
    "#         print(generator_loss.item())\n",
    "\n",
    "# print(\"The best generator loss:\", best_generator_loss)\n",
    "# generator.load_state_dict(best_generator_state_dict)\n",
    "# generator.eval()\n",
    "# X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# # Stage I\n",
    "# for i in range(epochs1):\n",
    "#     if i%adv_f==0 and i<=generator_training_limit:\n",
    "#         best_generator_loss = 1000; best_generator_state_dict = None\n",
    "#         print(\"Training the generator for \")\n",
    "#         for _ in trange(generator_training_epochs):\n",
    "#             semisup_model.eval()\n",
    "#             generator.train()\n",
    "#             generator_optimizer.zero_grad()\n",
    "#             X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#             unsup_loss = semisup_model(X_gen)[1]\n",
    "# #             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "#             generator_loss = sinkhorn_loss(X_gen, X_u_train)-unsup_loss\n",
    "#             generator_loss.backward(retain_graph=True)\n",
    "#             generator_optimizer.step()\n",
    "#             # Saving the best_generator_state_dict\n",
    "#             if generator_loss.item() < best_generator_loss:\n",
    "#                 best_generator_loss = generator_loss.item()\n",
    "#                 best_generator_state_dict = generator.state_dict()\n",
    "#         print(\"The best generator loss:\", best_generator_loss)\n",
    "#         generator.load_state_dict(best_generator_state_dict)\n",
    "#         generator.eval()\n",
    "#         X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#         X_gen = sampling_from_rows(X_gen, N_res)\n",
    "#         X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "#     semisup_model.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     optimizer1.step(pcgrad_closure)\n",
    "#     l = pcgrad_closure()\n",
    "#     if (i % F_print) == 0:\n",
    "#         if l.item() != curr_loss:\n",
    "#             curr_loss = l.item()\n",
    "#         else:\n",
    "#             print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "#             print(\"Finishing the first stage\")\n",
    "#             break\n",
    "#         print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = LBFGSNew(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "#                               lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  1.829027040933795e-09\n",
      "Epoch 100:  1.8118919697940328e-09\n",
      "Epoch 200:  1.7961687692746864e-09\n",
      "Epoch 300:  1.7771214499973098e-09\n",
      "Epoch 400:  1.7680195085745254e-09\n",
      "Epoch 500:  1.7441491584335722e-09\n",
      "Epoch 600:  1.7163697130229139e-09\n",
      "Epoch 700:  1.6974662786495287e-09\n",
      "Epoch 800:  1.6758971987940185e-09\n",
      "Epoch 900:  1.6593493246119806e-09\n",
      "Epoch 1000:  1.6502083033387294e-09\n",
      "Epoch 1100:  1.6391367152479575e-09\n",
      "Epoch 1200:  1.6311088035791954e-09\n",
      "Epoch 1300:  1.6243540956750735e-09\n",
      "Epoch 1400:  1.6192562846129022e-09\n",
      "Epoch 1500:  1.6111451062172932e-09\n",
      "Epoch 1600:  1.5970149647159815e-09\n",
      "Epoch 1700:  1.5888836912836268e-09\n",
      "Epoch 1800:  1.5635893690912894e-09\n",
      "Epoch 1900:  1.5536282260697476e-09\n",
      "Epoch 2000:  1.5399839181640118e-09\n",
      "Epoch 2100:  1.5305690048705856e-09\n",
      "Epoch 2200:  1.5172589851175644e-09\n",
      "Epoch 2300:  1.5075681814025188e-09\n",
      "Epoch 2400:  1.5017860288679685e-09\n",
      "Epoch 2500:  1.4914178780855991e-09\n",
      "Epoch 2600:  1.4785909163705924e-09\n",
      "Epoch 2700:  1.4700496375752437e-09\n",
      "Epoch 2800:  1.454985021354105e-09\n",
      "Epoch 2900:  1.4449555996165486e-09\n",
      "Epoch 3000:  1.4284293747834909e-09\n",
      "Epoch 3100:  1.4141531279321384e-09\n",
      "Epoch 3200:  1.4046693808111854e-09\n",
      "Epoch 3300:  1.3957428546262918e-09\n",
      "Epoch 3400:  1.3879336568933809e-09\n",
      "Epoch 3500:  1.3783708618930746e-09\n",
      "Epoch 3600:  1.3631077377951328e-09\n",
      "Epoch 3700:  1.352539302779121e-09\n",
      "Epoch 3800:  1.334019228416139e-09\n",
      "Epoch 3900:  1.3261772791040016e-09\n",
      "Epoch 4000:  1.3167777979106177e-09\n",
      "Epoch 4100:  1.3071904669814671e-09\n",
      "Epoch 4200:  1.2964891382694077e-09\n",
      "Epoch 4300:  1.287014605999559e-09\n",
      "Epoch 4400:  1.2811306460136507e-09\n",
      "Epoch 4500:  1.2775077662396939e-09\n",
      "Epoch 4600:  1.2696589335448039e-09\n",
      "Epoch 4700:  1.2624217227141799e-09\n",
      "Epoch 4800:  1.2564940199411012e-09\n",
      "Epoch 4900:  1.2511803815229428e-09\n",
      "Epoch 5000:  1.2462104681532082e-09\n",
      "Epoch 5100:  1.2432334051126759e-09\n",
      "Epoch 5200:  1.2379989255961732e-09\n",
      "Epoch 5300:  1.2346108579919246e-09\n",
      "Epoch 5400:  1.2301535345926595e-09\n",
      "Epoch 5500:  1.2264422810659426e-09\n",
      "Epoch 5600:  1.2183883901784043e-09\n",
      "Epoch 5700:  1.2141925243014384e-09\n",
      "Epoch 5800:  1.2126426529590617e-09\n",
      "Epoch 5900:  1.2086970313518464e-09\n",
      "Epoch 6000:  1.2054597320343419e-09\n",
      "Epoch 6100:  1.197684840192892e-09\n",
      "Epoch 6200:  1.194033871776412e-09\n",
      "Epoch 6300:  1.189206177976132e-09\n",
      "Epoch 6400:  1.1845473490978975e-09\n",
      "Epoch 6500:  1.179355391123238e-09\n",
      "Epoch 6600:  1.1756911000304626e-09\n",
      "Epoch 6700:  1.173424468703388e-09\n",
      "Epoch 6800:  1.1701682955944648e-09\n",
      "Epoch 6900:  1.1692983248323685e-09\n",
      "Epoch 7000:  1.1652908638026815e-09\n",
      "Epoch 7100:  1.1604499583484085e-09\n",
      "Epoch 7200:  1.1569084579221567e-09\n",
      "Epoch 7300:  1.150557538132091e-09\n",
      "Epoch 7400:  1.1454723836124003e-09\n",
      "Epoch 7500:  1.1414887923777428e-09\n",
      "Epoch 7600:  1.1376664055262609e-09\n",
      "Epoch 7700:  1.1333078919761874e-09\n",
      "Epoch 7800:  1.1244080111438848e-09\n",
      "Epoch 7900:  1.1206614525249847e-09\n",
      "Epoch 8000:  1.1152249124180003e-09\n",
      "Epoch 8100:  1.1134698718606728e-09\n",
      "Epoch 8200:  1.1099930974367567e-09\n",
      "Epoch 8300:  1.1080703021804084e-09\n",
      "Epoch 8400:  1.1058834958888042e-09\n",
      "Epoch 8500:  1.1030261148903264e-09\n",
      "Epoch 8600:  1.1005196753899327e-09\n",
      "Epoch 8700:  1.0943342898528385e-09\n",
      "Epoch 8800:  1.0918201898135749e-09\n",
      "Epoch 8900:  1.0909407821557693e-09\n",
      "Epoch 9000:  1.088516055069988e-09\n",
      "Epoch 9100:  1.0856512355772452e-09\n",
      "Epoch 9200:  1.0822381879549425e-09\n",
      "Epoch 9300:  1.079972333783985e-09\n",
      "Epoch 9400:  1.0771723513158804e-09\n",
      "Epoch 9500:  1.069663801978038e-09\n",
      "Epoch 9600:  1.0651716175757997e-09\n",
      "Epoch 9700:  1.0605298861321444e-09\n",
      "Epoch 9800:  1.0555131213507707e-09\n",
      "Epoch 9900:  1.0516932880122454e-09\n",
      "Testing\n",
      "Test MSE: 4.662938479782497e-08\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 100) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "MODEL_PATH = './saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained0unlabeledsamples_4.6e-8.pth'\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")\n",
    "torch.save(semisup_model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6629e-08, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "semisup_model.load_state_dict(torch.load(MODEL_PATH), strict=False)\n",
    "semisup_model.eval()\n",
    "F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-2000-V1.npy\", derivatives_train)\n",
    "np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-2000-V1.npy\", dynamics_train)\n",
    "np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-25600-V1.npy\", derivatives_test)\n",
    "np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-25600-V1.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
