{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = 1000\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 2.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    if mse_loss.requires_grad:\n",
    "        mse_loss.backward(retain_graph=False)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54566468c7145aa90302154ff134042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 2.30E-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n"
     ]
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'MAD'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "if choice == 'MAD':\n",
    "    optimizer1 = MADGRAD(params, lr=learning_rate1, momentum=0.9)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None\n",
    "    \n",
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "suggested_lr = 1e-5\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "\n",
    "epochs1 = 2000; epochs2 = 50000;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = 300\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training the generator for \")\n",
    "# best_generator_loss = 1000; best_generator_state_dict = None\n",
    "# generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-5, momentum=0.95)\n",
    "# for i in trange(1000):\n",
    "#     semisup_model.eval()\n",
    "#     generator.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#     unsup_loss = semisup_model(X_gen)[1]\n",
    "#     generator_loss = sinkhorn_loss(X_gen, X_u_train)\n",
    "# #     generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "#     generator_loss.backward(retain_graph=True)\n",
    "#     generator_optimizer.step()\n",
    "    \n",
    "#     if generator_loss.item() < best_generator_loss:\n",
    "#         best_generator_loss = generator_loss.item()\n",
    "#         best_generator_state_dict = generator.state_dict()\n",
    "#         print(best_generator_loss)\n",
    "        \n",
    "#     if i%100==0:\n",
    "#         print(generator_loss.item())\n",
    "\n",
    "# print(\"The best generator loss:\", best_generator_loss)\n",
    "# generator.load_state_dict(best_generator_state_dict)\n",
    "# generator.eval()\n",
    "# X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [06:07<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: -2.0700254440307617\n",
      "Semi-supervised solver loss @Epoch 0:  4.5143303871154785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [08:26<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best generator loss: -0.4814777374267578\n",
      "Semi-supervised solver loss @Epoch 100:  0.44397422671318054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/300 [00:20<08:15,  1.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b10a44d51d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msinkhorn_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_u_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munsup_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Saving the best_generator_state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1000; best_generator_state_dict = None\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "            generator_loss = sinkhorn_loss(X_gen, X_u_train)-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        generator.load_state_dict(best_generator_state_dict)\n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "        X_gen = sampling_from_rows(X_gen, N_res)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "\n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = LBFGSNew(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn=True, batch_mode=False)\n",
    "\n",
    "# optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "#                               lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  1.829027040933795e-09\n",
      "Epoch 100:  1.8118919697940328e-09\n",
      "Epoch 200:  1.7961687692746864e-09\n",
      "Epoch 300:  1.7771214499973098e-09\n",
      "Epoch 400:  1.7680195085745254e-09\n",
      "Epoch 500:  1.7441491584335722e-09\n",
      "Epoch 600:  1.7163697130229139e-09\n",
      "Epoch 700:  1.6974662786495287e-09\n",
      "Epoch 800:  1.6758971987940185e-09\n",
      "Epoch 900:  1.6593493246119806e-09\n",
      "Epoch 1000:  1.6502083033387294e-09\n",
      "Epoch 1100:  1.6391367152479575e-09\n",
      "Epoch 1200:  1.6311088035791954e-09\n",
      "Epoch 1300:  1.6243540956750735e-09\n",
      "Epoch 1400:  1.6192562846129022e-09\n",
      "Epoch 1500:  1.6111451062172932e-09\n",
      "Epoch 1600:  1.5970149647159815e-09\n",
      "Epoch 1700:  1.5888836912836268e-09\n",
      "Epoch 1800:  1.5635893690912894e-09\n",
      "Epoch 1900:  1.5536282260697476e-09\n",
      "Epoch 2000:  1.5399839181640118e-09\n",
      "Epoch 2100:  1.5305690048705856e-09\n",
      "Epoch 2200:  1.5172589851175644e-09\n",
      "Epoch 2300:  1.5075681814025188e-09\n",
      "Epoch 2400:  1.5017860288679685e-09\n",
      "Epoch 2500:  1.4914178780855991e-09\n",
      "Epoch 2600:  1.4785909163705924e-09\n",
      "Epoch 2700:  1.4700496375752437e-09\n",
      "Epoch 2800:  1.454985021354105e-09\n",
      "Epoch 2900:  1.4449555996165486e-09\n",
      "Epoch 3000:  1.4284293747834909e-09\n",
      "Epoch 3100:  1.4141531279321384e-09\n",
      "Epoch 3200:  1.4046693808111854e-09\n",
      "Epoch 3300:  1.3957428546262918e-09\n",
      "Epoch 3400:  1.3879336568933809e-09\n",
      "Epoch 3500:  1.3783708618930746e-09\n",
      "Epoch 3600:  1.3631077377951328e-09\n",
      "Epoch 3700:  1.352539302779121e-09\n",
      "Epoch 3800:  1.334019228416139e-09\n",
      "Epoch 3900:  1.3261772791040016e-09\n",
      "Epoch 4000:  1.3167777979106177e-09\n",
      "Epoch 4100:  1.3071904669814671e-09\n",
      "Epoch 4200:  1.2964891382694077e-09\n",
      "Epoch 4300:  1.287014605999559e-09\n",
      "Epoch 4400:  1.2811306460136507e-09\n",
      "Epoch 4500:  1.2775077662396939e-09\n",
      "Epoch 4600:  1.2696589335448039e-09\n",
      "Epoch 4700:  1.2624217227141799e-09\n",
      "Epoch 4800:  1.2564940199411012e-09\n",
      "Epoch 4900:  1.2511803815229428e-09\n",
      "Epoch 5000:  1.2462104681532082e-09\n",
      "Epoch 5100:  1.2432334051126759e-09\n",
      "Epoch 5200:  1.2379989255961732e-09\n",
      "Epoch 5300:  1.2346108579919246e-09\n",
      "Epoch 5400:  1.2301535345926595e-09\n",
      "Epoch 5500:  1.2264422810659426e-09\n",
      "Epoch 5600:  1.2183883901784043e-09\n",
      "Epoch 5700:  1.2141925243014384e-09\n",
      "Epoch 5800:  1.2126426529590617e-09\n",
      "Epoch 5900:  1.2086970313518464e-09\n",
      "Epoch 6000:  1.2054597320343419e-09\n",
      "Epoch 6100:  1.197684840192892e-09\n",
      "Epoch 6200:  1.194033871776412e-09\n",
      "Epoch 6300:  1.189206177976132e-09\n",
      "Epoch 6400:  1.1845473490978975e-09\n",
      "Epoch 6500:  1.179355391123238e-09\n",
      "Epoch 6600:  1.1756911000304626e-09\n",
      "Epoch 6700:  1.173424468703388e-09\n",
      "Epoch 6800:  1.1701682955944648e-09\n",
      "Epoch 6900:  1.1692983248323685e-09\n",
      "Epoch 7000:  1.1652908638026815e-09\n",
      "Epoch 7100:  1.1604499583484085e-09\n",
      "Epoch 7200:  1.1569084579221567e-09\n",
      "Epoch 7300:  1.150557538132091e-09\n",
      "Epoch 7400:  1.1454723836124003e-09\n",
      "Epoch 7500:  1.1414887923777428e-09\n",
      "Epoch 7600:  1.1376664055262609e-09\n",
      "Epoch 7700:  1.1333078919761874e-09\n",
      "Epoch 7800:  1.1244080111438848e-09\n",
      "Epoch 7900:  1.1206614525249847e-09\n",
      "Epoch 8000:  1.1152249124180003e-09\n",
      "Epoch 8100:  1.1134698718606728e-09\n",
      "Epoch 8200:  1.1099930974367567e-09\n",
      "Epoch 8300:  1.1080703021804084e-09\n",
      "Epoch 8400:  1.1058834958888042e-09\n",
      "Epoch 8500:  1.1030261148903264e-09\n",
      "Epoch 8600:  1.1005196753899327e-09\n",
      "Epoch 8700:  1.0943342898528385e-09\n",
      "Epoch 8800:  1.0918201898135749e-09\n",
      "Epoch 8900:  1.0909407821557693e-09\n",
      "Epoch 9000:  1.088516055069988e-09\n",
      "Epoch 9100:  1.0856512355772452e-09\n",
      "Epoch 9200:  1.0822381879549425e-09\n",
      "Epoch 9300:  1.079972333783985e-09\n",
      "Epoch 9400:  1.0771723513158804e-09\n",
      "Epoch 9500:  1.069663801978038e-09\n",
      "Epoch 9600:  1.0651716175757997e-09\n",
      "Epoch 9700:  1.0605298861321444e-09\n",
      "Epoch 9800:  1.0555131213507707e-09\n",
      "Epoch 9900:  1.0516932880122454e-09\n",
      "Testing\n",
      "Test MSE: 4.662938479782497e-08\n"
     ]
    }
   ],
   "source": [
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 100) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "# MODEL_PATH = './saved_path_inverse_burger/lbfgsnew_results/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained0unlabeledsamples_4.6e-8.pth'\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")\n",
    "# torch.save(semisup_model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6629e-08, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "# semisup_model.load_state_dict(torch.load(MODEL_PATH), strict=False)\n",
    "# semisup_model.eval()\n",
    "# F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-2000-V1.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-2000-V1.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/derivatives-25600-V1.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/lbfgsnew_results/data/dynamics-25600-V1.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
