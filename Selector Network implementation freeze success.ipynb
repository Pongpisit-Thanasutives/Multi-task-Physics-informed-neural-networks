{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I need to implement the GPU version for faster computation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        self.model = model\n",
    "        self.model.apply(self.xavier_init)\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, th, gamma=1e-5):\n",
    "        super().__init__()\n",
    "        # Selector model\n",
    "        layers = [nn.Linear(X_train_dim, 50), nn.Tanh(), \n",
    "                            nn.Linear(50, 50), nn.Tanh(), \n",
    "                            nn.Linear(50, X_train_dim), ThresholdSoftmax(th=th)]\n",
    "        self.selector_model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Nonlinear model\n",
    "        layers = [nn.Linear(3, 100), nn.Tanh(), nn.Linear(100, 1)] # 50 or 100\n",
    "        self.nonlinear_model = nn.Sequential(*layers)\n",
    "        # Mask and gamma parameter, I may need to tune the gamma param!.\n",
    "        self.mask = None\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    # You should either call forward or loss once\n",
    "    def forward(self, inn):\n",
    "        self.mask = self.selector_model(inn)\n",
    "        ut_approx = self.nonlinear_model(inn[:, self.mask])\n",
    "        return ut_approx\n",
    "    \n",
    "    # You should either call forward or loss once\n",
    "    def loss(self, X_input, y_input, include_mask_loss=False):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        if include_mask_loss:\n",
    "            # TODO: difference btw those passing and those not\n",
    "            mse_loss -= self.selector_model[-1].prob[self.mask].sum() * self.gamma\n",
    "        return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network(model=simple_solver_model(50))\n",
    "selector = SeclectorNetwork(X_train_dim=6, th=0.5)\n",
    "\n",
    "# optimizer = torch.optim.LBFGS(list(network.parameters()) + list(selector.parameters()), \n",
    "#                               lr=5e-2, max_iter=80, max_eval=100, \n",
    "#                               history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "optimizer = torch.optim.Adam(list(network.parameters()) + list(selector.parameters()), lr=1e-3)\n",
    "epochs = 1200; testing = False\n",
    "\n",
    "if testing:\n",
    "    # unsupervised_loss\n",
    "    unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)))\n",
    "    sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "    # No MTL yet, apply the naive summation first to see if it's working?\n",
    "    total_loss = unsup_loss + sup_loss\n",
    "    print(total_loss)\n",
    "\n",
    "    total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.30441462993621826\n",
      "tensor([0, 1, 2])\n",
      "Epoch 100:  0.04821036010980606\n",
      "tensor([0, 1, 2])\n",
      "Epoch 200:  0.03587862104177475\n",
      "tensor([0, 1, 2])\n",
      "Epoch 300:  0.028232170268893242\n",
      "tensor([0, 1, 2])\n",
      "Epoch 400:  0.022483276203274727\n",
      "tensor([0, 1, 2])\n",
      "Epoch 500:  0.018681025132536888\n",
      "tensor([0, 1, 2])\n",
      "Epoch 600:  0.01666880026459694\n",
      "tensor([0, 1, 2])\n",
      "Epoch 700:  0.014937910251319408\n",
      "tensor([0, 1, 2])\n",
      "Epoch 800:  0.013383101671934128\n",
      "tensor([0, 1, 2])\n",
      "Epoch 900:  0.01193489320576191\n",
      "tensor([0, 1, 2])\n",
      "Epoch 1000:  0.011945461854338646\n",
      "tensor([0, 1, 2])\n",
      "Epoch 1100:  0.01184071134775877\n",
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "network.train(); selector.train(); best_train_loss = 1e6\n",
    "\n",
    "for i in range(epochs):\n",
    "    ### Add the closure function to calculate the gradient. For LBFGS.\n",
    "    def closure():\n",
    "        if torch.is_grad_enabled():\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Total loss calculation process\n",
    "        # unsupervised_loss\n",
    "        unsup_loss = selector.loss(*network.get_selector_data(*dimension_slicing(X_u_train)), include_mask_loss=True)\n",
    "        sup_loss = F.mse_loss(network.uf, u_train)\n",
    "\n",
    "        # No MTL yet, apply the naive summation first to see if it's working?\n",
    "        total_loss = unsup_loss + sup_loss\n",
    "        \n",
    "        if total_loss.requires_grad:\n",
    "            total_loss.backward()\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    # calculate the loss again for monitoring\n",
    "    l = closure()\n",
    "\n",
    "    if (i % 100) == 0:\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(selector.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0077, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.eval()\n",
    "F.mse_loss(network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('uf', 'u_x', 'u_xx', 'u_tt', 'u_xt', 'u_tx')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0304e-01, 3.6693e-01, 5.2988e-01, 3.9262e-05, 5.9319e-05, 4.8331e-05],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.selector_model[-1].prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0, 4, 5, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(selector.selector_model[-1].prob, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
