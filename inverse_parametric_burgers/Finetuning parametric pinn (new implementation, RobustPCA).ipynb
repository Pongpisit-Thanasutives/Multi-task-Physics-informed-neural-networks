{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Running Python 3.9.6\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sympy import sympify\n",
    "import sys; sys.path.insert(0, \"../\"); from utils import *\n",
    "from models import SympyTorch, PartialDerivativeCalculator, CoeffLearner\n",
    "from parametric_discovery_pinn import ParametricPINN, BurgerPINN, FinalParametricPINN\n",
    "from pytorch_robust_pca import *\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from robust_pde_diff import print_pde, RobustPCA, Robust_LRSTR\n",
    "from parametric_pde_diff import TrainSGTRidge, create_groups\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from time import time\n",
    "\n",
    "from pysr import pysr, best\n",
    "\n",
    "fontsize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../parametric_pde_data/parametric_burgers.pkl\n",
      "Perturbed Exact with intensity = 0.01\n",
      "Noisy (x, t)\n",
      "Running Robust PCA on (x, t)\n",
      "iteration: 1, error: 11.099553579824038\n",
      "iteration: 100, error: 0.015955346989596244\n",
      "iteration: 200, error: 0.013236305955974697\n",
      "iteration: 300, error: 0.011224371934239532\n",
      "iteration: 400, error: 0.00965597303671397\n",
      "iteration: 500, error: 0.008252160189746671\n",
      "iteration: 600, error: 0.007131638710814129\n",
      "iteration: 700, error: 0.006261063752339695\n",
      "iteration: 800, error: 0.005486484591351327\n",
      "iteration: 900, error: 0.004770459372913774\n",
      "iteration: 1000, error: 0.00420824753142068\n",
      "iteration: 1100, error: 0.0037154388798231234\n",
      "iteration: 1200, error: 0.0034512475957567588\n",
      "iteration: 1300, error: 0.0029932164347751993\n",
      "iteration: 1400, error: 0.002635502130577575\n",
      "iteration: 1500, error: 0.0023804236158479\n",
      "iteration: 1600, error: 0.0020264033055168843\n",
      "iteration: 1700, error: 0.0017044896470677104\n",
      "iteration: 1800, error: 0.0012762846390805976\n",
      "iteration: 1900, error: 0.0009681039132882523\n",
      "iteration: 2000, error: 0.0006643340254477578\n",
      "iteration: 2100, error: 0.0004109023991342978\n",
      "iteration: 2200, error: 0.00028045851892989594\n",
      "iteration: 2300, error: 0.00020308468859729515\n",
      "iteration: 2400, error: 0.000137528841067964\n",
      "iteration: 2500, error: 8.820088751525045e-05\n",
      "iteration: 2600, error: 5.878626228414934e-05\n",
      "iteration: 2700, error: 3.825200137748654e-05\n",
      "iteration: 2800, error: 2.4488258754092998e-05\n",
      "iteration: 2900, error: 1.5510966800710946e-05\n",
      "iteration: 3000, error: 9.822818496695719e-06\n",
      "iteration: 3100, error: 6.137116298865021e-06\n",
      "iteration: 3200, error: 3.823435519471042e-06\n",
      "iteration: 3300, error: 2.3778409861193475e-06\n",
      "iteration: 3400, error: 1.4771845130932124e-06\n",
      "iteration: 3500, error: 9.170370865878301e-07\n",
      "iteration: 3600, error: 5.6904943541647e-07\n",
      "iteration: 3700, error: 3.5301496568301335e-07\n",
      "iteration: 3800, error: 2.1895727767456184e-07\n",
      "iteration: 3900, error: 1.3579248748922044e-07\n",
      "iteration: 4000, error: 8.42091136677296e-08\n",
      "iteration: 4100, error: 5.221798246289425e-08\n",
      "iteration: 4200, error: 3.237914993086805e-08\n",
      "iteration: 4300, error: 2.0077033896302606e-08\n",
      "iteration: 4400, error: 1.2448733452320081e-08\n",
      "iteration: 4500, error: 7.718698536310553e-09\n",
      "iteration: 4600, error: 4.785835382198434e-09\n",
      "iteration: 4700, error: 2.967337564164412e-09\n",
      "iteration: 4800, error: 1.8398064226176811e-09\n",
      "iteration: 4900, error: 1.1407065340163586e-09\n",
      "iteration: 5000, error: 7.072501182149161e-10\n",
      "iteration: 5100, error: 4.384997613302257e-10\n",
      "iteration: 5200, error: 2.718708250648571e-10\n",
      "iteration: 5300, error: 1.6855897289214537e-10\n",
      "iteration: 5400, error: 1.0450702357876064e-10\n",
      "iteration: 5500, error: 6.479281087979796e-11\n",
      "iteration: 5600, error: 4.0171427684192374e-11\n",
      "iteration: 5700, error: 2.4906224291983185e-11\n",
      "iteration: 5800, error: 1.5441595629785867e-11\n",
      "iteration: 5900, error: 9.573196199533293e-12\n",
      "iteration: 6000, error: 5.935476536059789e-12\n",
      "iteration: 6100, error: 3.680159470210916e-12\n",
      "iteration: 6200, error: 2.281616953752634e-12\n",
      "iteration: 6300, error: 1.4159379522185527e-12\n",
      "iteration: 6400, error: 8.762299914200474e-13\n",
      "iteration: 6500, error: 5.434821774333213e-13\n",
      "iteration: 6600, error: 3.3735619030351493e-13\n",
      "iteration: 6700, error: 2.094869549715815e-13\n",
      "iteration: 6800, error: 1.3007585641671478e-13\n",
      "iteration: 6900, error: 7.946561383598183e-14\n",
      "iteration: 7000, error: 5.183725355214791e-14\n",
      "iteration: 7100, error: 3.2932454282991264e-14\n",
      "iteration: 7200, error: 2.312058863136585e-14\n",
      "iteration: 7300, error: 1.6078840007928333e-14\n",
      "iteration: 7400, error: 1.472660179284669e-14\n",
      "iteration: 7500, error: 1.2188425027261298e-14\n",
      "iteration: 7600, error: 1.056045837359039e-14\n",
      "iteration: 7700, error: 8.52788121278967e-15\n",
      "iteration: 7800, error: 7.781516906915176e-15\n",
      "iteration: 7900, error: 7.781828500039138e-15\n",
      "iteration: 8000, error: 7.781733273554413e-15\n",
      "iteration: 8100, error: 7.782457352288162e-15\n",
      "iteration: 8200, error: 7.78103648959297e-15\n",
      "iteration: 8300, error: 7.782695345833696e-15\n",
      "iteration: 8400, error: 7.782287214341972e-15\n",
      "iteration: 8500, error: 7.781058147200648e-15\n",
      "iteration: 8600, error: 7.78252521309041e-15\n",
      "iteration: 8700, error: 7.781495250584328e-15\n",
      "iteration: 8800, error: 7.781998648014204e-15\n",
      "iteration: 8900, error: 7.781733273554413e-15\n",
      "iteration: 9000, error: 7.782457352288162e-15\n",
      "iteration: 9100, error: 7.781828500039138e-15\n",
      "iteration: 9200, error: 7.781754929222851e-15\n",
      "iteration: 9300, error: 7.782287214341972e-15\n",
      "iteration: 9400, error: 7.78103648959297e-15\n",
      "iteration: 9500, error: 7.782695345833696e-15\n",
      "iteration: 9600, error: 7.781495250584328e-15\n",
      "iteration: 9700, error: 7.781998648014204e-15\n",
      "iteration: 9800, error: 7.78252521309041e-15\n",
      "iteration: 9900, error: 7.781516906915176e-15\n",
      "iteration: 10000, error: 7.781828500039138e-15\n",
      "iteration: 10100, error: 7.781733273554413e-15\n",
      "iteration: 10200, error: 7.782457352288162e-15\n",
      "iteration: 10300, error: 7.78103648959297e-15\n",
      "iteration: 10400, error: 7.782695345833696e-15\n",
      "iteration: 10500, error: 7.782287214341972e-15\n",
      "iteration: 10600, error: 7.781058147200648e-15\n",
      "iteration: 10700, error: 7.78252521309041e-15\n",
      "iteration: 10800, error: 7.781495250584328e-15\n",
      "iteration: 10900, error: 7.781998648014204e-15\n",
      "iteration: 11000, error: 7.781733273554413e-15\n",
      "iteration: 11100, error: 7.782457352288162e-15\n",
      "iteration: 11200, error: 7.781828500039138e-15\n",
      "iteration: 11300, error: 7.781754929222851e-15\n",
      "iteration: 11400, error: 7.782287214341972e-15\n",
      "iteration: 11500, error: 7.78103648959297e-15\n",
      "iteration: 11600, error: 7.782695345833696e-15\n",
      "iteration: 11700, error: 7.781495250584328e-15\n",
      "iteration: 11800, error: 7.781998648014204e-15\n",
      "iteration: 11900, error: 7.78252521309041e-15\n",
      "iteration: 12000, error: 7.781516906915176e-15\n",
      "iteration: 12100, error: 7.781828500039138e-15\n",
      "iteration: 12200, error: 7.781733273554413e-15\n",
      "iteration: 12300, error: 7.782457352288162e-15\n",
      "iteration: 12400, error: 7.78103648959297e-15\n",
      "iteration: 12500, error: 7.782695345833696e-15\n",
      "iteration: 12600, error: 7.782287214341972e-15\n",
      "iteration: 12700, error: 7.781058147200648e-15\n",
      "iteration: 12800, error: 7.78252521309041e-15\n",
      "iteration: 12900, error: 7.781495250584328e-15\n",
      "iteration: 13000, error: 7.781998648014204e-15\n",
      "iteration: 13100, error: 7.781733273554413e-15\n",
      "iteration: 13200, error: 7.782457352288162e-15\n",
      "iteration: 13300, error: 7.781828500039138e-15\n",
      "iteration: 13400, error: 7.781754929222851e-15\n",
      "iteration: 13500, error: 7.782287214341972e-15\n",
      "iteration: 13600, error: 7.78103648959297e-15\n",
      "iteration: 13700, error: 7.782695345833696e-15\n",
      "iteration: 13800, error: 7.781495250584328e-15\n",
      "iteration: 13900, error: 7.781998648014204e-15\n",
      "iteration: 14000, error: 7.78252521309041e-15\n",
      "iteration: 14100, error: 7.781516906915176e-15\n",
      "iteration: 14200, error: 7.781828500039138e-15\n",
      "iteration: 14300, error: 7.781733273554413e-15\n",
      "iteration: 14400, error: 7.782457352288162e-15\n",
      "iteration: 14500, error: 7.78103648959297e-15\n",
      "iteration: 14600, error: 7.782695345833696e-15\n",
      "iteration: 14700, error: 7.782287214341972e-15\n",
      "iteration: 14800, error: 7.781058147200648e-15\n",
      "iteration: 14900, error: 7.78252521309041e-15\n",
      "iteration: 15000, error: 7.781495250584328e-15\n",
      "iteration: 15100, error: 7.781998648014204e-15\n",
      "iteration: 15200, error: 7.781733273554413e-15\n",
      "iteration: 15300, error: 7.782457352288162e-15\n",
      "iteration: 15400, error: 7.781828500039138e-15\n",
      "iteration: 15500, error: 7.781754929222851e-15\n",
      "iteration: 15600, error: 7.782287214341972e-15\n",
      "iteration: 15700, error: 7.78103648959297e-15\n",
      "iteration: 15800, error: 7.782695345833696e-15\n",
      "iteration: 15900, error: 7.781495250584328e-15\n",
      "iteration: 16000, error: 7.781998648014204e-15\n",
      "iteration: 16100, error: 7.78252521309041e-15\n",
      "iteration: 16200, error: 7.781516906915176e-15\n",
      "iteration: 16300, error: 7.781828500039138e-15\n",
      "iteration: 16400, error: 7.781733273554413e-15\n",
      "iteration: 16500, error: 7.782457352288162e-15\n",
      "iteration: 16600, error: 7.78103648959297e-15\n",
      "iteration: 16700, error: 7.782695345833696e-15\n",
      "iteration: 16800, error: 7.782287214341972e-15\n",
      "iteration: 16900, error: 7.781058147200648e-15\n",
      "iteration: 17000, error: 7.78252521309041e-15\n",
      "iteration: 17100, error: 7.781495250584328e-15\n",
      "iteration: 17200, error: 7.781998648014204e-15\n",
      "iteration: 17300, error: 7.781733273554413e-15\n",
      "iteration: 17400, error: 7.782457352288162e-15\n",
      "iteration: 17500, error: 7.781828500039138e-15\n",
      "iteration: 17600, error: 7.781754929222851e-15\n",
      "iteration: 17700, error: 7.782287214341972e-15\n",
      "iteration: 17800, error: 7.78103648959297e-15\n",
      "iteration: 17900, error: 7.782695345833696e-15\n",
      "iteration: 18000, error: 7.781495250584328e-15\n",
      "iteration: 18100, error: 7.781998648014204e-15\n",
      "iteration: 18200, error: 7.78252521309041e-15\n",
      "iteration: 18300, error: 7.781516906915176e-15\n",
      "iteration: 18400, error: 7.781828500039138e-15\n",
      "iteration: 18500, error: 7.781733273554413e-15\n",
      "iteration: 18600, error: 7.782457352288162e-15\n",
      "iteration: 18700, error: 7.78103648959297e-15\n",
      "iteration: 18800, error: 7.782695345833696e-15\n",
      "iteration: 18900, error: 7.782287214341972e-15\n",
      "iteration: 19000, error: 7.781058147200648e-15\n",
      "iteration: 19100, error: 7.78252521309041e-15\n",
      "iteration: 19200, error: 7.781495250584328e-15\n",
      "iteration: 19300, error: 7.781998648014204e-15\n",
      "iteration: 19400, error: 7.781733273554413e-15\n",
      "iteration: 19500, error: 7.782457352288162e-15\n",
      "iteration: 19600, error: 7.781828500039138e-15\n",
      "iteration: 19700, error: 7.781754929222851e-15\n",
      "iteration: 19800, error: 7.782287214341972e-15\n",
      "iteration: 19900, error: 7.78103648959297e-15\n",
      "iteration: 20000, error: 7.782695345833696e-15\n",
      "Robust PCA Loss: 2.560245325178477e-33\n",
      "Running Robust PCA on u_train\n",
      "iteration: 1, error: 0.23843815166871699\n",
      "iteration: 100, error: 0.0015883011409307498\n",
      "iteration: 200, error: 0.0008931062572604808\n",
      "iteration: 300, error: 0.00013402594425661497\n",
      "iteration: 400, error: 6.747548243308697e-05\n",
      "iteration: 500, error: 4.024703222352646e-05\n",
      "iteration: 600, error: 2.6057351123858153e-05\n",
      "iteration: 700, error: 1.779376165936663e-05\n",
      "iteration: 800, error: 1.4118142796358055e-05\n",
      "iteration: 900, error: 1.0445255855634336e-05\n",
      "iteration: 1000, error: 7.931777159840609e-06\n",
      "iteration: 1100, error: 7.120368899941257e-06\n",
      "iteration: 1200, error: 7.120368899941257e-06\n",
      "iteration: 1300, error: 4.9307393654775754e-06\n",
      "iteration: 1400, error: 4.9307393654775754e-06\n",
      "iteration: 1500, error: 4.281293528228345e-06\n",
      "iteration: 1600, error: 3.5956234180825526e-06\n",
      "iteration: 1700, error: 3.5956234180825526e-06\n",
      "iteration: 1800, error: 3.5956234180825526e-06\n",
      "iteration: 1900, error: 2.456971192746598e-06\n",
      "iteration: 2000, error: 1.6625115704923074e-06\n",
      "iteration: 2100, error: 1.6625115704923074e-06\n",
      "iteration: 2200, error: 1.6625115704923074e-06\n",
      "iteration: 2300, error: 6.78273341576247e-07\n",
      "iteration: 2400, error: 6.78273341576247e-07\n",
      "iteration: 2500, error: 6.78273341576247e-07\n",
      "iteration: 2600, error: 6.78273341576247e-07\n",
      "iteration: 2700, error: 6.78273341576247e-07\n",
      "iteration: 2800, error: 6.78273341576247e-07\n",
      "iteration: 2900, error: 6.78273341576247e-07\n",
      "iteration: 3000, error: 6.78273341576247e-07\n",
      "iteration: 3100, error: 6.78273341576247e-07\n",
      "iteration: 3200, error: 6.78273341576247e-07\n",
      "iteration: 3300, error: 6.78273341576247e-07\n",
      "iteration: 3400, error: 6.78273341576247e-07\n",
      "iteration: 3500, error: 6.78273341576247e-07\n",
      "iteration: 3600, error: 6.78273341576247e-07\n",
      "iteration: 3700, error: 6.78273341576247e-07\n",
      "iteration: 3800, error: 6.78273341576247e-07\n",
      "iteration: 3900, error: 6.78273341576247e-07\n",
      "iteration: 4000, error: 6.78273341576247e-07\n",
      "iteration: 4100, error: 6.78273341576247e-07\n",
      "iteration: 4200, error: 6.78273341576247e-07\n",
      "iteration: 4300, error: 6.78273341576247e-07\n",
      "iteration: 4400, error: 6.78273341576247e-07\n",
      "iteration: 4500, error: 6.78273341576247e-07\n",
      "iteration: 4600, error: 6.78273341576247e-07\n",
      "iteration: 4700, error: 6.78273341576247e-07\n",
      "iteration: 4800, error: 6.78273341576247e-07\n",
      "iteration: 4900, error: 6.78273341576247e-07\n",
      "iteration: 5000, error: 6.78273341576247e-07\n",
      "iteration: 5100, error: 6.78273341576247e-07\n",
      "iteration: 5200, error: 6.78273341576247e-07\n",
      "iteration: 5300, error: 6.78273341576247e-07\n",
      "iteration: 5400, error: 6.78273341576247e-07\n",
      "iteration: 5500, error: 6.78273341576247e-07\n",
      "iteration: 5600, error: 6.78273341576247e-07\n",
      "iteration: 5700, error: 6.78273341576247e-07\n",
      "iteration: 5800, error: 6.78273341576247e-07\n",
      "iteration: 5900, error: 3.8932714920175477e-07\n",
      "iteration: 6000, error: 3.864402396129072e-07\n",
      "iteration: 6100, error: 3.864402396129072e-07\n",
      "iteration: 6200, error: 3.864402396129072e-07\n",
      "iteration: 6300, error: 3.864402396129072e-07\n",
      "iteration: 6400, error: 3.864402396129072e-07\n",
      "iteration: 6500, error: 3.864402396129072e-07\n",
      "iteration: 6600, error: 3.864402396129072e-07\n",
      "iteration: 6700, error: 3.864402396129072e-07\n",
      "iteration: 6800, error: 3.864402396129072e-07\n",
      "iteration: 6900, error: 3.864402396129072e-07\n",
      "iteration: 7000, error: 3.864402396129072e-07\n",
      "iteration: 7100, error: 3.864402396129072e-07\n",
      "iteration: 7200, error: 3.864402396129072e-07\n",
      "iteration: 7300, error: 3.864402396129072e-07\n",
      "iteration: 7400, error: 3.864402396129072e-07\n",
      "iteration: 7500, error: 3.864402396129072e-07\n",
      "iteration: 7600, error: 3.864402396129072e-07\n",
      "iteration: 7700, error: 3.864402396129072e-07\n",
      "iteration: 7800, error: 3.864402396129072e-07\n",
      "iteration: 7900, error: 3.864402396129072e-07\n",
      "iteration: 8000, error: 3.864402396129072e-07\n",
      "iteration: 8100, error: 3.864402396129072e-07\n",
      "iteration: 8200, error: 3.864402396129072e-07\n",
      "iteration: 8300, error: 3.864402396129072e-07\n",
      "iteration: 8400, error: 3.864402396129072e-07\n",
      "iteration: 8500, error: 3.864402396129072e-07\n",
      "iteration: 8600, error: 3.864402396129072e-07\n",
      "iteration: 8700, error: 3.864402396129072e-07\n",
      "iteration: 8800, error: 3.864402396129072e-07\n",
      "iteration: 8900, error: 3.864402396129072e-07\n",
      "iteration: 9000, error: 1.2928759319488954e-07\n",
      "iteration: 9100, error: 1.2928759319488954e-07\n",
      "iteration: 9200, error: 1.2928759319488954e-07\n",
      "iteration: 9300, error: 1.2928759319488954e-07\n",
      "iteration: 9400, error: 1.2928759319488954e-07\n",
      "iteration: 9500, error: 1.2928759319488954e-07\n",
      "iteration: 9600, error: 1.2928759319488954e-07\n",
      "iteration: 9700, error: 1.2928759319488954e-07\n",
      "iteration: 9800, error: 1.2928759319488954e-07\n",
      "iteration: 9900, error: 1.2928759319488954e-07\n",
      "iteration: 10000, error: 1.2928759319488954e-07\n",
      "iteration: 10100, error: 1.2928759319488954e-07\n",
      "iteration: 10200, error: 1.2928759319488954e-07\n",
      "iteration: 10300, error: 1.2928759319488954e-07\n",
      "iteration: 10400, error: 1.2928759319488954e-07\n",
      "iteration: 10500, error: 1.2928759319488954e-07\n",
      "iteration: 10600, error: 1.2928759319488954e-07\n",
      "iteration: 10700, error: 1.2928759319488954e-07\n",
      "iteration: 10800, error: 1.2928759319488954e-07\n",
      "iteration: 10900, error: 1.2928759319488954e-07\n",
      "iteration: 11000, error: 1.2928759319488954e-07\n",
      "iteration: 11100, error: 1.2928759319488954e-07\n",
      "iteration: 11200, error: 1.2928759319488954e-07\n",
      "iteration: 11300, error: 1.2928759319488954e-07\n",
      "iteration: 11400, error: 1.2928759319488954e-07\n",
      "iteration: 11500, error: 1.2928759319488954e-07\n",
      "iteration: 11600, error: 1.2928759319488954e-07\n",
      "iteration: 11700, error: 1.2928759319488954e-07\n",
      "iteration: 11800, error: 1.2928759319488954e-07\n",
      "iteration: 11900, error: 1.2928759319488954e-07\n",
      "iteration: 12000, error: 1.2928759319488954e-07\n",
      "iteration: 12100, error: 1.2928759319488954e-07\n",
      "iteration: 12200, error: 1.2928759319488954e-07\n",
      "iteration: 12300, error: 1.2928759319488954e-07\n",
      "iteration: 12400, error: 1.2928759319488954e-07\n",
      "iteration: 12500, error: 1.2928759319488954e-07\n",
      "iteration: 12600, error: 1.2928759319488954e-07\n",
      "iteration: 12700, error: 1.2928759319488954e-07\n",
      "iteration: 12800, error: 1.2928759319488954e-07\n",
      "iteration: 12900, error: 1.2928759319488954e-07\n",
      "iteration: 13000, error: 1.2928759319488954e-07\n",
      "iteration: 13100, error: 1.2928759319488954e-07\n",
      "iteration: 13200, error: 1.2928759319488954e-07\n",
      "iteration: 13300, error: 1.2928759319488954e-07\n",
      "iteration: 13400, error: 1.2928759319488954e-07\n",
      "iteration: 13500, error: 1.2928759319488954e-07\n",
      "iteration: 13600, error: 1.2928759319488954e-07\n",
      "iteration: 13700, error: 1.2928759319488954e-07\n",
      "iteration: 13800, error: 1.2928759319488954e-07\n",
      "iteration: 13900, error: 1.2928759319488954e-07\n",
      "iteration: 14000, error: 1.2928759319488954e-07\n",
      "iteration: 14100, error: 1.2928759319488954e-07\n",
      "iteration: 14200, error: 1.2928759319488954e-07\n",
      "iteration: 14300, error: 1.2928759319488954e-07\n",
      "iteration: 14400, error: 1.2928759319488954e-07\n",
      "iteration: 14500, error: 1.2928759319488954e-07\n",
      "iteration: 14600, error: 1.2928759319488954e-07\n",
      "iteration: 14700, error: 1.2928759319488954e-07\n",
      "iteration: 14800, error: 1.2928759319488954e-07\n",
      "iteration: 14900, error: 1.2928759319488954e-07\n",
      "iteration: 15000, error: 1.2928759319488954e-07\n",
      "iteration: 15100, error: 1.2928759319488954e-07\n",
      "iteration: 15200, error: 1.2928759319488954e-07\n",
      "iteration: 15300, error: 1.2928759319488954e-07\n",
      "iteration: 15400, error: 1.2928759319488954e-07\n",
      "iteration: 15500, error: 1.2928759319488954e-07\n",
      "iteration: 15600, error: 1.2928759319488954e-07\n",
      "iteration: 15700, error: 1.2928759319488954e-07\n",
      "iteration: 15800, error: 1.2928759319488954e-07\n",
      "iteration: 15900, error: 1.2928759319488954e-07\n",
      "iteration: 16000, error: 1.2928759319488954e-07\n",
      "iteration: 16100, error: 1.2928759319488954e-07\n",
      "iteration: 16200, error: 1.2928759319488954e-07\n",
      "iteration: 16300, error: 1.2928759319488954e-07\n",
      "iteration: 16400, error: 1.2928759319488954e-07\n",
      "iteration: 16500, error: 1.2928759319488954e-07\n",
      "iteration: 16600, error: 1.2928759319488954e-07\n",
      "iteration: 16700, error: 1.2928759319488954e-07\n",
      "iteration: 16800, error: 1.2928759319488954e-07\n",
      "iteration: 16900, error: 1.2928759319488954e-07\n",
      "iteration: 17000, error: 1.2928759319488954e-07\n",
      "iteration: 17100, error: 1.2928759319488954e-07\n",
      "iteration: 17200, error: 1.2928759319488954e-07\n",
      "iteration: 17300, error: 1.2928759319488954e-07\n",
      "iteration: 17400, error: 1.2928759319488954e-07\n",
      "iteration: 17500, error: 1.2928759319488954e-07\n",
      "iteration: 17600, error: 1.2928759319488954e-07\n",
      "iteration: 17700, error: 1.2928759319488954e-07\n",
      "iteration: 17800, error: 1.2928759319488954e-07\n",
      "iteration: 17900, error: 1.2928759319488954e-07\n",
      "iteration: 18000, error: 1.2928759319488954e-07\n",
      "iteration: 18100, error: 1.2928759319488954e-07\n",
      "iteration: 18200, error: 1.2928759319488954e-07\n",
      "iteration: 18300, error: 1.2928759319488954e-07\n",
      "iteration: 18400, error: 1.2928759319488954e-07\n",
      "iteration: 18500, error: 1.2928759319488954e-07\n",
      "iteration: 18600, error: 1.2928759319488954e-07\n",
      "iteration: 18700, error: 1.2928759319488954e-07\n",
      "iteration: 18800, error: 1.2928759319488954e-07\n",
      "iteration: 18900, error: 1.2928759319488954e-07\n",
      "iteration: 19000, error: 1.2928759319488954e-07\n",
      "iteration: 19100, error: 1.2928759319488954e-07\n",
      "iteration: 19200, error: 1.2928759319488954e-07\n",
      "iteration: 19300, error: 1.2928759319488954e-07\n",
      "iteration: 19400, error: 1.2928759319488954e-07\n",
      "iteration: 19500, error: 1.2928759319488954e-07\n",
      "iteration: 19600, error: 1.2928759319488954e-07\n",
      "iteration: 19700, error: 1.2928759319488954e-07\n",
      "iteration: 19800, error: 1.2928759319488954e-07\n",
      "iteration: 19900, error: 1.2928759319488954e-07\n",
      "iteration: 20000, error: 1.2928759319488954e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_parametric_burgers/../utils.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "data = pickle_load(\"../parametric_pde_data/parametric_burgers.pkl\")\n",
    "\n",
    "x = data['x']; spatial_dims = x.shape[0]\n",
    "t = data['t']; time_dims = t.shape[0]\n",
    "\n",
    "Exact = data['u']\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.01\n",
    "noisy_xt = True\n",
    "\n",
    "Exact = perturb(Exact, intensity=noise_intensity, noise_type=\"normal\")\n",
    "print(\"Perturbed Exact with intensity =\", float(noise_intensity))\n",
    "\n",
    "X_star = np.hstack((to_column_vector(X), to_column_vector(T)))\n",
    "u_star = to_column_vector(Exact.T)\n",
    "\n",
    "# Add noise to (x, t) before setting the lb, and ub.\n",
    "if noisy_xt: \n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "# domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "# Sampling training data points\n",
    "N = 20000\n",
    "training_idxs = sampling_from_rows(X_star, N, True)\n",
    "X_train = X_star[training_idxs, :]\n",
    "u_train = u_star[training_idxs, :]\n",
    "\n",
    "# Robust PCA\n",
    "print(\"Running Robust PCA on (x, t)\")\n",
    "rpca = R_pca_numpy(X_train)\n",
    "X_train_L, X_train_S = rpca.fit(tol=1e-16, max_iter=20000, iter_print=100)\n",
    "print('Robust PCA Loss:', mean_squared_error(X_train, X_train_L+X_train_S))\n",
    "X_train_S = (X_train_S-X_train_S.min())-(X_train_S.max()-X_train_S.min())\n",
    "X_train_L = to_tensor(X_train_L, True)\n",
    "X_train_S = to_tensor(X_train_S, True)\n",
    "# X_train = X_train_L + X_train_S\n",
    "\n",
    "print(\"Running Robust PCA on u_train\")\n",
    "u_train_L, u_train_S = R_pca_numpy(u_train).fit(tol=1e-16, max_iter=20000, iter_print=100)\n",
    "u_train_L = to_tensor(u_train_L, False)\n",
    "u_train_S = to_tensor(u_train_S, False)\n",
    "u_train = u_train_L + u_train_S\n",
    "\n",
    "del rpca\n",
    "\n",
    "# to_tensor\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "lb = to_tensor(lb, False)\n",
    "ub = to_tensor(ub, False)\n",
    "\n",
    "u_xx_true = 0.1*np.ones(time_dims)\n",
    "uu_x_true = -1*(1+0.25*np.sin(t))\n",
    "\n",
    "feature_names = ['u', 'u_x', 'u_xx']\n",
    "\n",
    "del X_star, u_star, X, T, Exact, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrained weights\n"
     ]
    }
   ],
   "source": [
    "pinn = ParametricPINN(scale=False, lb=lb, ub=ub)\n",
    "print(\"Loaded the pretrained weights\")\n",
    "pinn.load_state_dict(torch.load(\"./saved_path_inverse_parametric_burgers/parametric_pinn.pth\"))\n",
    "model = nn.Sequential(pinn.preprocessor_net, pinn.pde_solver_net)\n",
    "\n",
    "pde_terms = [\"u*u_x\", \"u_xx\"]\n",
    "func_terms = [\"-0.1872898*sin(t)-1.0238724\", \"0.09875935\"]\n",
    "final_burger_pinn = FinalParametricPINN(model=model, pde_terms=pde_terms, func_terms=func_terms, uncert=True, scale=pinn.scale, lb=pinn.lb, ub=pinn.ub)\n",
    "del pinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustFinalParametricPINN(nn.Module):\n",
    "    def __init__(self, pinn, beta1=0.0, beta2=0.0, beta3=0.0, is_beta1_trainable=True, is_beta2_trainable=True, is_beta3_trainable=False, hidden_nodes=50):\n",
    "        super(RobustFinalParametricPINN, self).__init__()\n",
    "        self.pinn = pinn\n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta3 = beta3\n",
    "        \n",
    "        if is_beta1_trainable: self.beta1 = nn.Parameter(data=torch.tensor([self.beta1]), requires_grad=True)\n",
    "        if is_beta2_trainable: self.beta2 = nn.Parameter(data=torch.tensor([self.beta2]), requires_grad=True)\n",
    "        if is_beta3_trainable: self.beta3 = nn.Parameter(data=torch.tensor([self.beta3]), requires_grad=True)\n",
    "        \n",
    "        self.proj = nn.Sequential(nn.Linear(2, hidden_nodes), nn.Tanh(), nn.Linear(hidden_nodes, 2), nn.Tanh())\n",
    "        self.labels_proj = nn.Sequential(nn.Linear(1, hidden_nodes), nn.Tanh(), nn.Linear(hidden_nodes, 1), nn.Tanh())\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        return self.pinn(x, t)\n",
    "            \n",
    "#     def loss(self, L, S, y_train):\n",
    "#         recov = L + S - self.beta2*self.proj(S)\n",
    "#         return self.pinn.loss(recov[:, 0:1], recov[:, 1:2], y_train)\n",
    "    \n",
    "    def loss(self, L, S, y_train_L, y_train_S):\n",
    "        recov = L-self.beta2*self.proj(S)\n",
    "        corr = self.labels_proj(y_train_S)\n",
    "        return self.pinn.loss(recov[:, 0:1], recov[:, 1:2], y_train_L-self.beta3*corr/torch.norm(corr, p=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_buger_pinn = RobustFinalParametricPINN(pinn=final_burger_pinn, is_beta1_trainable=False, is_beta3_trainable=True, hidden_nodes=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, X_train_L, X_train_S, u_train\n",
    "    losses = robust_buger_pinn.loss(X_train, X_train_S, u_train, u_train_S)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in robust_buger_pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(robust_buger_pinn.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return losses[0]+losses[1]\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_closure():\n",
    "    global N, X_train, X_train_L, X_train_S, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    mse_loss, pde_loss = robust_buger_pinn.loss(X_train, X_train_S, u_train, u_train_S)\n",
    "    loss = mse_loss + pde_loss\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7706e-05, grad_fn=<AddBackward0>)\n",
      "tensor(6.7282e-05, grad_fn=<AddBackward0>)\n",
      "tensor(6.7176e-05, grad_fn=<AddBackward0>)\n",
      "tensor(6.6939e-05, grad_fn=<AddBackward0>)\n",
      "tensor(6.6939e-05, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# optimizer = MADGRAD(robust_buger_pinn.parameters(), lr=1e-6, momentum=0.9)\n",
    "# robust_buger_pinn.train()\n",
    "# for i in range(150):\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "#     if i%10==0: print(pcgrad_closure(return_list=True))\n",
    "        \n",
    "f_opt = torch.optim.LBFGS(robust_buger_pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(1.25*500), history_size=300, line_search_fn='strong_wolfe')\n",
    "final_burger_pinn.is_uncert = False\n",
    "for i in range(50):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0: print(finetuning_closure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.999883770942688, -0.23896124958992004, 0.09957490116357803]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.item() for x in robust_buger_pinn.pinn.pdc.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6174073020617186, 1.985738038189975)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errs = np.array([100*abs(0.999883770942688-1), 100*abs(0.23896124958992004-0.25)/0.25, 100*abs(0.09957490116357803-0.1)/0.1])\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " Parameter containing:\n",
       " tensor([-0.0048], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0063], requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_buger_pinn.beta1, robust_buger_pinn.beta2, robust_buger_pinn.beta3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(robust_buger_pinn, \"./saved_path_inverse_parametric_burgers/noisy2_final_doublerpca_parametric_pinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New results\n",
    "\n",
    "# Noisy Exact & Noisy (x, t) & X_train = X_train - self.beta2*self.proj(S) | beta1 = 0.0\n",
    "# an attempt with reasonable results\n",
    "# params: [-0.9991318583488464, -0.23970049619674683, 0.10075154900550842]\n",
    "# errs: (1.6527215639750146, 1.7654708038535187)\n",
    "\n",
    "# Noisy Exact & Noisy (x, t) & rpca on (x, t) and rpca on Exact (double rpca)\n",
    "# params: [-0.999883770942688, -0.23896124958992004, 0.09957490116357803]\n",
    "# errs (1.6174073020617186, 1.985738038189975)\n",
    "# (0.0,\n",
    "#  Parameter containing:\n",
    "#  tensor([-0.0048], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([-0.0063], requires_grad=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Exact and clean (x, t) | final_parametric_pinn.pth\n",
    "# params: [-1.0013394355773926, -0.2480146884918213, 0.0997583195567131]\n",
    "# errs: (0.38991620143254785, 0.28918291006992664)\n",
    "\n",
    "# Noisy Exact and clean (x, t)\n",
    "# params: [-1.0029208660125732, -0.24306637048721313, 0.09983817487955093]\n",
    "# errs: (1.075787842273714, 1.2016070382040953)\n",
    "\n",
    "# Noisy Exact and Noisy (x, t)\n",
    "# params: [-1.0028020143508911, -0.24078042805194855, 0.10122136771678925]\n",
    "# errs: (1.729799310366311, 1.4368618683618857)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star_L+X_star_S\n",
    "# params: [-1.000922441482544, -0.24657735228538513, 0.09988813102245331]\n",
    "# errs: (0.5243907372156797, 0.5973244500071551)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star_L+X_star_S\n",
    "# params: [-1.0046403408050537, -0.24455536901950836, 0.1004636213183403]\n",
    "# errs: (1.035169263680774, 0.8079990064924926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(final_burger_pinn, \"./saved_path_inverse_parametric_burgers/noisy_final_parametric_pinn.pth\")\n",
    "# save(final_burger_pinn, \"./saved_path_inverse_parametric_burgers/noisy2_final_parametric_pinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
