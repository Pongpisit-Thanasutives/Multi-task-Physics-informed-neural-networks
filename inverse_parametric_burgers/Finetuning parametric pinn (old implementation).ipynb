{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, \"../\"); from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sympy import sympify\n",
    "import sys; sys.path.insert(0, \"../\"); from utils import *\n",
    "from parametric_discovery_pinn import ParametricPINN, BurgerPINN\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from robust_pde_diff import print_pde, RobustPCA, Robust_LRSTR\n",
    "from parametric_pde_diff import TrainSGTRidge, create_groups\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from numpy.fft import fft, ifft, fftfreq\n",
    "from time import time\n",
    "\n",
    "from pysr import pysr, best\n",
    "\n",
    "fontsize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../parametric_pde_data/parametric_burgers.pkl\n"
     ]
    }
   ],
   "source": [
    "data = pickle_load(\"../parametric_pde_data/parametric_burgers.pkl\")\n",
    "\n",
    "x = data['x']; spatial_dims = x.shape[0]\n",
    "t = data['t']; time_dims = t.shape[0]\n",
    "\n",
    "Exact = data['u']\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "X_star = np.hstack((to_column_vector(X), to_column_vector(T)))\n",
    "u_star = to_column_vector(Exact.T)\n",
    "\n",
    "# domain bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "# Sampling training data points\n",
    "N = 20000\n",
    "training_idxs = sampling_from_rows(X_star, N, True)\n",
    "X_train = X_star[training_idxs, :]\n",
    "u_train = u_star[training_idxs, :]\n",
    "\n",
    "# to_tensor\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "lb = to_tensor(lb, False)\n",
    "ub = to_tensor(ub, False)\n",
    "\n",
    "u_xx_true = 0.1*np.ones(time_dims)\n",
    "uu_x_true = -1*(1+0.25*np.sin(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn = ParametricPINN(scale=False, lb=lb, ub=ub)\n",
    "print(\"Loaded the pretrained weights\")\n",
    "pinn.load_state_dict(torch.load(\"./saved_path_inverse_parametric_burgers/parametric_pinn.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ensure that all coefs are different\n"
     ]
    }
   ],
   "source": [
    "expression, _ = build_exp(\"-0.1872898*sin(t)-1.0238724\")\n",
    "mod = SymPyModule(expressions=[expression])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(pinn.preprocessor_net, pinn.pde_solver_net)\n",
    "pinn = BurgerPINN(model=model, funcs=mod, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    losses = pinn.loss(*dimension_slicing(X_train), u_train)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return losses[0]+losses[1]\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    mse_loss, pde_loss = pinn.loss(*dimension_slicing(X_train), u_train)\n",
    "    loss = mse_loss + pde_loss\n",
    "    if loss.requires_grad: loss.backward(retain_graph=False)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.4728e-06, grad_fn=<MseLossBackward>), tensor(1.6235e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.5480e-06, grad_fn=<MseLossBackward>), tensor(1.5724e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.5743e-06, grad_fn=<MseLossBackward>), tensor(1.5097e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.5908e-06, grad_fn=<MseLossBackward>), tensor(1.4492e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.6750e-06, grad_fn=<MseLossBackward>), tensor(1.3866e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.7436e-06, grad_fn=<MseLossBackward>), tensor(1.3289e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.8061e-06, grad_fn=<MseLossBackward>), tensor(1.2756e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.8518e-06, grad_fn=<MseLossBackward>), tensor(1.2273e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.9114e-06, grad_fn=<MseLossBackward>), tensor(1.1814e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(1.9878e-06, grad_fn=<MseLossBackward>), tensor(1.1376e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(2.0668e-06, grad_fn=<MseLossBackward>), tensor(1.0959e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(2.1432e-06, grad_fn=<MseLossBackward>), tensor(1.0571e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(2.2157e-06, grad_fn=<MseLossBackward>), tensor(1.0213e-05, grad_fn=<MseLossBackward>))\n",
      "(tensor(2.2922e-06, grad_fn=<MseLossBackward>), tensor(9.8730e-06, grad_fn=<MseLossBackward>))\n",
      "(tensor(2.3640e-06, grad_fn=<MseLossBackward>), tensor(9.5578e-06, grad_fn=<MseLossBackward>))\n",
      "3.0944013360567624e-06\n",
      "1.786952907423256e-06\n",
      "1.350413981526799e-06\n",
      "1.0824634273376432e-06\n",
      "9.509525966677757e-07\n",
      "9.509525966677757e-07\n",
      "9.509525966677757e-07\n",
      "9.509525966677757e-07\n",
      "9.509525966677757e-07\n",
      "9.509525966677757e-07\n"
     ]
    }
   ],
   "source": [
    "optimizer = MADGRAD(pinn.parameters(), lr=1e-5, momentum=0.9)\n",
    "for i in range(150):\n",
    "    pinn.train()\n",
    "    optimizer.step(pcgrad_closure)\n",
    "    if i%10==0:\n",
    "        loss = pcgrad_closure(return_list=True)\n",
    "        print(loss)\n",
    "        \n",
    "f_opt = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300, line_search_fn='strong_wolfe')\n",
    "for i in range(100):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(0.0998, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.000081181526184, -0.24583321809768677]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.item() for x in pinn.funcs.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5649436378479006, 0.7790835192709206)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errs = np.array([100*(0.1-0.0998), 100*(1.000081181526184-1), 100*(0.25-0.24583321809768677)/0.25])\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(pinn.state_dict(), \"./saved_path_inverse_parametric_burgers/final_finetuned_pinn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
