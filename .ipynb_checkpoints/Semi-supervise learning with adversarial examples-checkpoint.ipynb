{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "# import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from geomloss import SamplesLoss\n",
    "from utils import *\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = N\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "# lb and ub are used in adversarial training\n",
    "lb = to_tensor(lb, False)\n",
    "ub = to_tensor(ub, False)\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55523d3fc467402784ed0788680cbd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 2.37E+00\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted the fake labels used in Learning rate finder\n"
     ]
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'Adam'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else:\n",
    "    lr_finder = None\n",
    "    suggested_lr = None\n",
    "    \n",
    "print(\"Deleted the fake labels used in Learning rate finder\")\n",
    "u_train = u_train[:N, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "# suggested_lr = 1e-4\n",
    "\n",
    "if lr_finder and suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "    \n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = suggested_lr\n",
    "        \n",
    "epochs1 = 2000; epochs2 = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the generator\n",
    "generator = TorchMLP([2, 50, 50, 2])\n",
    "# generator_training_epochs indicates how string the generator is\n",
    "adv_f = 100; generator_training_epochs = 300; generator_training_limit = 300\n",
    "# I can use the Learning rate finder to find a good lr for the generator optim  as well\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-4, momentum=0.95)\n",
    "sinkhorn_loss = SamplesLoss(\"sinkhorn\", p=2, blur=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Training the generator for \")\n",
    "# best_generator_loss = 1000; best_generator_state_dict = None\n",
    "# generator_optimizer = torch.optim.SGD(generator.parameters(), lr=3e-5, momentum=0.95)\n",
    "# for i in trange(1000):\n",
    "#     semisup_model.eval()\n",
    "#     generator.train()\n",
    "#     generator_optimizer.zero_grad()\n",
    "#     X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "#     unsup_loss = semisup_model(X_gen)[1]\n",
    "#     generator_loss = sinkhorn_loss(X_gen, X_u_train)\n",
    "# #     generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "#     generator_loss.backward(retain_graph=True)\n",
    "#     generator_optimizer.step()\n",
    "    \n",
    "#     if generator_loss.item() < best_generator_loss:\n",
    "#         best_generator_loss = generator_loss.item()\n",
    "#         best_generator_state_dict = generator.state_dict()\n",
    "#         print(best_generator_loss)\n",
    "        \n",
    "#     if i%100==0:\n",
    "#         print(generator_loss.item())\n",
    "\n",
    "# print(\"The best generator loss:\", best_generator_loss)\n",
    "# generator.load_state_dict(best_generator_state_dict)\n",
    "# generator.eval()\n",
    "# X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the generator for \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 73/300 [01:48<05:25,  1.44s/it]"
     ]
    }
   ],
   "source": [
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    if i%adv_f==0 and i<=generator_training_limit:\n",
    "        best_generator_loss = 1000; best_generator_state_dict = None\n",
    "        print(\"Training the generator for \")\n",
    "        for _ in trange(generator_training_epochs):\n",
    "            semisup_model.eval()\n",
    "            generator.train()\n",
    "            generator_optimizer.zero_grad()\n",
    "            X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "            unsup_loss = semisup_model(X_gen)[1]\n",
    "#             generator_loss = distance_loss(X_gen, X_u_train[:N, :], distance_function=torch_energy_loss)-unsup_loss\n",
    "            generator_loss = sinkhorn_loss(X_gen, X_u_train)-unsup_loss\n",
    "            generator_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "            # Saving the best_generator_state_dict\n",
    "            if generator_loss.item() < best_generator_loss:\n",
    "                best_generator_loss = generator_loss.item()\n",
    "                best_generator_state_dict = generator.state_dict()\n",
    "        print(\"The best generator loss:\", best_generator_loss)\n",
    "        generator.load_state_dict(best_generator_state_dict)\n",
    "        generator.eval()\n",
    "        X_gen = scale_to_range(generator(X_u_train[:N, :]), lb, ub)\n",
    "        X_u_train = torch.cat([X_u_train[:N, :], X_gen], dim=0).detach().requires_grad_(True)\n",
    "    \n",
    "    semisup_model.train()\n",
    "    generator_optimizer.zero_grad()\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Semi-supervised solver loss @Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 10) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2366e-06, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model and testing\n",
    "# semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_burger/running_exp.pth\"), strict=False)\n",
    "# semisup_model.eval()\n",
    "# F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-4000-V1-with-2000unlabledadversarialsamples.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-4000-V1-with-2000unlabledadversarialsamples.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-2000unlabledadversarialsamples.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-25600-V1-with-2000unlabledadversarialsamples.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
