{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, \n",
    "                    cplx2tensor, ComplexTorchMLP, complex_mse, TanhProb)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from robust_pde_diff import print_pde, RobustPCA, Robust_LRSTR\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from lbfgsnew import LBFGSNew\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Training with 4000 unsup samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_qho/../utils.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/harmonic_osc.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "\n",
    "spatial_dim = x.shape[0]\n",
    "time_dim = t.shape[0]\n",
    "\n",
    "potential = np.vstack([0.5*np.power(x,2).reshape((1,spatial_dim)) for _ in range(time_dim)])\n",
    "Exact = data['usol']\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "# Adjust the diemnsion of Exact and potential (0.5*x**2)\n",
    "if Exact.T.shape == X.shape: Exact = Exact.T\n",
    "if potential.T.shape == X.shape: potential = potential.T\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "# Converting in a feature vector for each feature\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "h_star = to_column_vector(Exact)\n",
    "u_star = to_column_vector(Exact_u)\n",
    "v_star = to_column_vector(Exact_v)\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "# Converting the grounds to be tensor\n",
    "X_star = to_tensor(X_star, True)\n",
    "h_star = to_complex_tensor(h_star, False)\n",
    "\n",
    "N = 2000; include_N_res = 2\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "h_train = torch.complex(u_train, v_train).to(device)\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res>0:\n",
    "    N_res = int(N*include_N_res)\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = idx_res[:N_res]\n",
    "    X_res = to_tensor(X_star[idx_res, :], True)\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_train = torch.vstack([X_train, X_res])\n",
    "\n",
    "# Potential is calculated from x\n",
    "# Hence, Quadratic features of x are required.\n",
    "feature_names = ['hf', 'h_x', 'h_xx', 'h_xxx', 'V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = (t[1]-t[0])[0]\n",
    "dx = (x[2]-x[1])[0]\n",
    "\n",
    "fd_h_t = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_x = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_xx = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_xxx = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "\n",
    "for i in range(spatial_dim):\n",
    "    fd_h_t[:,i] = FiniteDiff(Exact[:,i], dt, 1)\n",
    "for i in range(time_dim):\n",
    "    fd_h_x[i,:] = FiniteDiff(Exact[i,:], dx, 1)\n",
    "    fd_h_xx[i,:] = FiniteDiff(Exact[i,:], dx, 2)\n",
    "    fd_h_xxx[i,:] = FiniteDiff(Exact[i,:], dx, 3)\n",
    "\n",
    "fd_h_t = to_column_vector(fd_h_t)\n",
    "fd_h_x = to_column_vector(fd_h_x)\n",
    "fd_h_xx = to_column_vector(fd_h_xx)\n",
    "fd_h_xxx = to_column_vector(fd_h_xxx)\n",
    "V = to_column_vector(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = cat_numpy(h_star.detach().numpy(), V, fd_h_x, fd_h_xx, fd_h_xxx)\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not needed anymore.\n",
    "\n",
    "# c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "# complex_poly_features = c_poly.fit()\n",
    "# complex_poly_features\n",
    "\n",
    "# w = TrainSTRidge(complex_poly_features, fd_h_t, 1e-10, 10)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_PATH = \"./saved_path_inverse_qho/pretrained_cpinn_2000labeledsamples.pth\"\n",
    "\n",
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )\n",
    "\n",
    "if PRETRAINED_PATH is not None: complex_model.load_state_dict(cpu_load(PRETRAINED_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=-1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=-1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        derivatives = []\n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        # Appending the estimated solutions\n",
    "        derivatives.append(cplx2tensor(uf))\n",
    "        \n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives.append(out)\n",
    "            \n",
    "        # Appending the potential function\n",
    "        derivatives.append(0.5*torch.pow(x,2))\n",
    "\n",
    "        return torch.cat(derivatives, dim=-1), u_t\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return -1 + 2*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexAttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=5e-2):\n",
    "        super(ComplexAttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = CplxLinear(layers[0], layers[0], bias=True)\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = ComplexTorchMLP(dimensions=layers, activation_function=CplxToCplx[F.relu](), bn=bn, dropout_rate=0.0)\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])+(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = torch.tensor([1.0, 1.0, 2.0, 3.0, 1.0])/10\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*F.threshold(self.weighted_features(inn), self.th, 0.0))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn).real).mean(dim=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        l1 = complex_mse(ut_approx, y_input)\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1 + self.reg_intensity*l2\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None, uncert=False):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.weights = None\n",
    "        if uncert: \n",
    "            self.weights = torch.tensor([0.0, 0.0])\n",
    "        \n",
    "    def forward(self, X_h_train, h_train, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_h_train))\n",
    "        \n",
    "        h_row = h_train.shape[0]\n",
    "        fd_guidance = complex_mse(self.network.uf[:h_row, :], h_train)\n",
    "        \n",
    "        # I am not sure a good way to normalize/scale a complex tensor\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "            \n",
    "        if include_unsup and self.weights is not None:\n",
    "            return (torch.exp(-self.weights[0])*fd_guidance)+self.weights[0], (torch.exp(-self.weights[1])*unsup_loss)+self.weights[1]\n",
    "        else:\n",
    "            return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering ['hf', 'h_x', 'h_xx', 'h_xxx', 'V']\n"
     ]
    }
   ],
   "source": [
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(\n",
    "    network=ComplexNetwork(model=complex_model, index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "    selector=ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=TanhProb(), bn=True),\n",
    "    normalize_derivative_features=True,\n",
    "    mini=torch.tensor(np.abs(derivatives).min(axis=0), dtype=torch.cfloat), # does not matter, will be replaced\n",
    "    maxi=torch.tensor(np.abs(derivatives).max(axis=0), dtype=torch.cfloat), # does not matter, will be replaced\n",
    "    uncert=False,\n",
    ")\n",
    "\n",
    "# semisup_model.network.load_state_dict(torch.load(\"...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    def pretraining_closure():\n",
    "        global N, X_h_train, h_train\n",
    "        if torch.enable_grad(): pretraining_optimizer.zero_grad()\n",
    "        # Only focusing on first [:N, :] elements\n",
    "        mse_loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), h_train[:N, :])\n",
    "        if mse_loss.requires_grad: mse_loss.backward(retain_graph=False)\n",
    "        return mse_loss\n",
    "    \n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # See how well the model perform on the test set\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = complex_mse(semisup_model.network(*dimension_slicing(X_star)).detach(), h_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 20000\n",
    "test_idx = np.random.choice(X_star.shape[0], n_test, replace=False)\n",
    "referenced_derivatives, h_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[test_idx, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing V\n",
      "Computing hf^2\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing hf V\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_x V\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xx V\n",
      "Computing h_xxx^2\n",
      "Computing h_xxx V\n",
      "Computing V^2\n",
      "PDE derived using STRidge\n",
      "u_t = (-0.000894 +0.494751i)h_xx\n",
      "    + (0.000650 -0.994282i)hf V\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "derivatives = referenced_derivatives.detach().numpy()\n",
    "\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "\n",
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000, l0_penalty=5, normalize=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, h_train\n",
    "    fd_guidance, unsup_loss = semisup_model(X_train, h_train, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-7\n",
    "# optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# for i in range(50):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "    \n",
    "#     if i%10==0: \n",
    "#         loss = pcgrad_closure(return_list=True); print(loss)\n",
    "#         fi = semisup_model.selector.latest_weighted_features\n",
    "#         print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning both the solver and selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model's weights properly\n"
     ]
    }
   ],
   "source": [
    "# A cell for saving and loading\n",
    "# torch.save(semisup_model.state_dict(), \"saved_path_inverse_qho/qho_complex_model_2000labeledsamples_jointtrainwith4000unlabeledsamples.pth\")\n",
    "semisup_model = load_weights(semisup_model, \"tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0389892597449943e-05\n",
      "6.586108611372765e-06\n",
      "5.915461315453285e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n",
      "5.914753273827955e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=350, history_size=350)\n",
    "# f_opt = LBFGSNew(semisup_model.network.parameters(),\n",
    "#         lr=1e-1, max_iter=300,\n",
    "#         max_eval=int(300*1.25), history_size=150,\n",
    "#         line_search_fn=True, batch_mode=False)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, h_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), h_train)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced_derivatives, h_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[test_idx, :]))\n",
    "# derivatives = referenced_derivatives.detach().numpy()\n",
    "\n",
    "# dictionary = {}\n",
    "# for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "# c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "# complex_poly_features = c_poly.fit()\n",
    "\n",
    "# w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000, l0_penalty=5, normalize=1)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "X_selector = (X_selector - semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6371, 0.1452, 0.2206, 0.1356, 0.4887], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.weighted_features(X_selector)\n",
    "semisup_model.selector.latest_weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003284808248281479\n",
      "[3 1 2 0 4]\n"
     ]
    }
   ],
   "source": [
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=5e-2, max_iter=200, history_size=200)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = complex_mse(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "max_it = 1\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        print(np.argsort(semisup_model.selector.latest_weighted_features.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "V 0.7488443\n",
      "hf 0.43340284\n",
      "h_xx 0.20097099\n",
      "h_x 0.14522594\n",
      "h_xxx 0.1355661\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(semisup_model, \"semisup_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Softmax layer's outputs as feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=0.2, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
