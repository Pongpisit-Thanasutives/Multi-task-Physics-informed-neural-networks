{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, ComplexTorchMLP, complex_mse\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from robust_pde_diff import print_pde, RobustPCA, Robust_LRSTR\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from lbfgsnew import LBFGSNew\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Training with 4000 unsup samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_qho/../utils.py:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "DATA_PATH = '../PDE_FIND_experimental_datasets/harmonic_osc.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "\n",
    "spatial_dim = x.shape[0]\n",
    "time_dim = t.shape[0]\n",
    "\n",
    "potential = np.vstack([0.5*np.power(x,2).reshape((1,spatial_dim)) for _ in range(time_dim)])\n",
    "Exact = data['usol']\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "\n",
    "# Adjust the diemnsion of Exact and potential (0.5*x**2)\n",
    "if Exact.T.shape == X.shape: Exact = Exact.T\n",
    "if potential.T.shape == X.shape: potential = potential.T\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "# Converting in a feature vector for each feature\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "h_star = to_column_vector(Exact)\n",
    "u_star = to_column_vector(Exact_u)\n",
    "v_star = to_column_vector(Exact_v)\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(axis=0)\n",
    "ub = X_star.max(axis=0)\n",
    "\n",
    "# Converting the grounds to be tensor\n",
    "X_star = to_tensor(X_star, True)\n",
    "h_star = to_complex_tensor(h_star, False)\n",
    "\n",
    "N = 2000; include_N_res = 2\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "h_train = torch.complex(u_train, v_train).to(device)\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res>0:\n",
    "    N_res = int(N*include_N_res)\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = idx_res[:N_res]\n",
    "    X_res = to_tensor(X_star[idx_res, :], True)\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_train = torch.vstack([X_train, X_res])\n",
    "\n",
    "# Potential is calculated from x\n",
    "# Hence, Quadratic features of x are required.\n",
    "feature_names = ['hf', 'h_x', 'h_xx', 'h_xxx', 'V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = (t[1]-t[0])[0]\n",
    "dx = (x[2]-x[1])[0]\n",
    "\n",
    "fd_h_t = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_x = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_xx = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "fd_h_xxx = np.zeros((time_dim, spatial_dim), dtype=np.complex64)\n",
    "\n",
    "for i in range(spatial_dim):\n",
    "    fd_h_t[:,i] = FiniteDiff(Exact[:,i], dt, 1)\n",
    "for i in range(time_dim):\n",
    "    fd_h_x[i,:] = FiniteDiff(Exact[i,:], dx, 1)\n",
    "    fd_h_xx[i,:] = FiniteDiff(Exact[i,:], dx, 2)\n",
    "    fd_h_xxx[i,:] = FiniteDiff(Exact[i,:], dx, 3)\n",
    "\n",
    "fd_h_t = to_column_vector(fd_h_t)\n",
    "fd_h_x = to_column_vector(fd_h_x)\n",
    "fd_h_xx = to_column_vector(fd_h_xx)\n",
    "fd_h_xxx = to_column_vector(fd_h_xxx)\n",
    "V = to_column_vector(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = cat_numpy(h_star.detach().numpy(), V, fd_h_x, fd_h_xx, fd_h_xxx)\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not needed anymore.\n",
    "\n",
    "# c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "# complex_poly_features = c_poly.fit()\n",
    "# complex_poly_features\n",
    "\n",
    "# w = TrainSTRidge(complex_poly_features, fd_h_t, 1e-10, 10)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_PATH = \"./saved_path_inverse_qho/pretrained_cpinn_2000labeledsamples.pth\"\n",
    "\n",
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )\n",
    "\n",
    "if PRETRAINED_PATH is not None: complex_model.load_state_dict(cpu_load(PRETRAINED_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=-1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=-1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        derivatives = []\n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        # Appending the estimated solutions\n",
    "        derivatives.append(cplx2tensor(uf))\n",
    "        \n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives.append(out)\n",
    "            \n",
    "        # Appending the potential function\n",
    "        derivatives.append(0.5*torch.pow(x,2))\n",
    "\n",
    "        return torch.cat(derivatives, dim=-1), u_t\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return -1 + 2*(inp-self.lb)/(self.ub-self.lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexAttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.1):\n",
    "        super(ComplexAttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = CplxLinear(layers[0], layers[0], bias=True)\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = ComplexTorchMLP(dimensions=layers, activation_function=CplxToCplx[F.relu](), bn=bn, dropout_rate=0.0)\n",
    "        self.latest_weighted_features = None\n",
    "#         self.th = 1/layers[0]\n",
    "        self.th = 0.1\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        feature_importances = self.weighted_features(inn)\n",
    "        inn = inn*feature_importances\n",
    "        return self.nonlinear_model(inn)\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n",
    "        self.latest_weighted_features = self.latest_weighted_features.mean(dim=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = complex_mse(ut_approx, y_input)\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        return mse_loss+self.reg_intensity*(torch.norm(reg_term, p=0)+(torch.tensor([1.0, 1.0, 1.0, 2.0, 1.0])*reg_term).sum())\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None, uncert=False):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.weights = None\n",
    "        if uncert: \n",
    "            self.weights = torch.tensor([0.0, 0.0])\n",
    "        \n",
    "    def forward(self, X_h_train, h_train, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_h_train))\n",
    "        \n",
    "        h_row = h_train.shape[0]\n",
    "        fd_guidance = complex_mse(self.network.uf[:h_row, :], h_train)\n",
    "        \n",
    "        # I am not sure a good way to normalize/scale a complex tensor\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "            \n",
    "        if include_unsup and self.weights is not None:\n",
    "            return (torch.exp(-self.weights[0])*fd_guidance)+self.weights[0], (torch.exp(-self.weights[1])*unsup_loss)+self.weights[1]\n",
    "        else:\n",
    "            return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering ['hf', 'h_x', 'h_xx', 'h_xxx', 'V']\n"
     ]
    }
   ],
   "source": [
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(\n",
    "    network=ComplexNetwork(model=complex_model, index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "    selector=ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=F.softmax, bn=True),\n",
    "    normalize_derivative_features=True,\n",
    "    mini=torch.tensor(np.abs(derivatives).min(axis=0), dtype=torch.cfloat), # does not matter, will be replaced\n",
    "    maxi=torch.tensor(np.abs(derivatives).max(axis=0), dtype=torch.cfloat), # does not matter, will be replaced\n",
    "    uncert=False,\n",
    ")\n",
    "\n",
    "# semisup_model.network.load_state_dict(torch.load(\"...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    def pretraining_closure():\n",
    "        global N, X_h_train, h_train\n",
    "        if torch.enable_grad(): pretraining_optimizer.zero_grad()\n",
    "        # Only focusing on first [:N, :] elements\n",
    "        mse_loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), h_train[:N, :])\n",
    "        if mse_loss.requires_grad: mse_loss.backward(retain_graph=False)\n",
    "        return mse_loss\n",
    "    \n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # See how well the model perform on the test set\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = complex_mse(semisup_model.network(*dimension_slicing(X_star)).detach(), h_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 25000\n",
    "test_idx = np.random.choice(X_star.shape[0], n_test, replace=False)\n",
    "referenced_derivatives, h_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[test_idx, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing V\n",
      "Computing hf^2\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing hf V\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_x V\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xx V\n",
      "Computing h_xxx^2\n",
      "Computing h_xxx V\n",
      "Computing V^2\n",
      "PDE derived using STRidge\n",
      "u_t = (-0.000084 +0.494702i)h_xx\n",
      "    + (0.001434 -0.994890i)hf V\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "derivatives = referenced_derivatives.detach().numpy()\n",
    "\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "\n",
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000, l0_penalty=5, normalize=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, h_train\n",
    "    fd_guidance, unsup_loss = semisup_model(X_train, h_train, include_unsup=True)\n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-61ef6e2e66cc>:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.0003, grad_fn=<AddBackward0>), tensor(1.4120, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0016, grad_fn=<AddBackward0>), tensor(0.7315, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0032, grad_fn=<AddBackward0>), tensor(0.4353, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0025, grad_fn=<AddBackward0>), tensor(0.4947, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0017, grad_fn=<AddBackward0>), tensor(0.5749, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0012, grad_fn=<AddBackward0>), tensor(0.5057, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0012, grad_fn=<AddBackward0>), tensor(0.5784, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0011, grad_fn=<AddBackward0>), tensor(0.3636, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0009, grad_fn=<AddBackward0>), tensor(0.4600, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0009, grad_fn=<AddBackward0>), tensor(0.3899, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0010, grad_fn=<AddBackward0>), tensor(0.4627, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0009, grad_fn=<AddBackward0>), tensor(0.5591, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0008, grad_fn=<AddBackward0>), tensor(0.5558, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0007, grad_fn=<AddBackward0>), tensor(0.3543, grad_fn=<AddBackward0>)]\n",
      "[tensor(0.0007, grad_fn=<AddBackward0>), tensor(0.3789, grad_fn=<AddBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Joint training\n",
    "optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "optimizer.param_groups[0]['lr'] = 1e-11\n",
    "optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# TODO: also need the adversarial examples as well (Use ~idx to sample)\n",
    "for i in range(150):\n",
    "    semisup_model.train()\n",
    "    optimizer.step(pcgrad_closure)\n",
    "    loss = pcgrad_closure(return_list=True)\n",
    "    if i%10==0: print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-11\n",
    "# optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# # TODO: also need the adversarial examples as well (Use ~idx to sample)\n",
    "# for i in range(50):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "#     loss = pcgrad_closure(return_list=True)\n",
    "#     if i%10==0: print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning both the solver and selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A cell for saving and loading\n",
    "# torch.save(semisup_model.state_dict(), \"saved_path_inverse_qho/qho_complex_model_2000labeledsamples_jointtrainwith4000unlabeledsamples.pth\")\n",
    "semisup_model.load_state_dict(torch.load(\"saved_path_inverse_qho/qho_complex_model_2000labeledsamples_jointtrainwith4000unlabeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, history_size=300)\n",
    "# f_opt = LBFGSNew(semisup_model.network.parameters(),\n",
    "#         lr=1e-1, max_iter=300,\n",
    "#         max_eval=int(300*1.25), history_size=150,\n",
    "#         line_search_fn=True, batch_mode=False)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, h_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), h_train)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "# X_selector = (X_selector - semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=200, history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # Am I forget to normalize the derivative features?, NVM\n",
    "    loss = complex_mse(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "max_it = 10\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        print(np.argsort(semisup_model.selector.latest_weighted_features.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Softmax layer's outputs as feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=0.1, save_path=\"../visualization/qho_feature_importances_selector_with_softmax.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(semisup_model.state_dict(), \"saved_path_inverse_qho/qho_complex_model_2000labeledsamples_jointtrainwith4000unlabeledsamplesV2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./tmp_files/X_train_2000labeledsamples\", X_train.detach().numpy())\n",
    "np.save(\"./tmp_files/h_train_2000labeledsamples\", h_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
