{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# always import gbm_algos first !\n",
    "import xgboost, lightgbm, catboost\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from mlens.ensemble import SuperLearner\n",
    "\n",
    "# Let's do facy optimizers\n",
    "from optimizers import Lookahead, AdamGC, SGDGC\n",
    "# Modify at /usr/local/lib/python3.9/site-packages/torch_lr_finder/lr_finder.py\n",
    "from torch_lr_finder import LRFinder\n",
    "from onecyclelr import OneCycleLR\n",
    "import pcgrad\n",
    "from pytorch_stats_loss import torch_wasserstein_loss, torch_energy_loss\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2000 samples\n",
      "Training with 2000 unsup samples\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/Users/pongpisit/Desktop/research/pinn/Solving-Differential-Equations-with-Neural-Networks/SymbolicMathematics/data/burgers_shock.mat\"\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0)\n",
    "\n",
    "N = 2000\n",
    "print(f\"Training with {N} samples\")\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_u_train = X_star[idx, :]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# Unsup data\n",
    "N_res = 1000\n",
    "idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "X_res = X_star[idx_res, :]\n",
    "print(f\"Training with {N} unsup samples\")\n",
    "X_u_train = np.vstack([X_u_train, X_res])\n",
    "u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "# del X_res\n",
    "\n",
    "# Convert to torch.tensor\n",
    "X_u_train = torch.tensor(X_u_train).float().requires_grad_(True)\n",
    "u_train = torch.tensor(u_train).float().requires_grad_(True)\n",
    "X_star = torch.tensor(X_star).float().requires_grad_(True)\n",
    "u_star = torch.tensor(u_star).float().requires_grad_(True)\n",
    "\n",
    "feature_names=['uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking\n",
    "        self.index2features = ('uf', 'u_x',  'u_xx', 'u_tt', 'u_xt', 'u_tx')\n",
    "        self.uf = None\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # first-order derivatives\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        # Homo second-order derivatives\n",
    "        u_tt = self.gradients(u_t,t)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        # Hetero second-order derivatives\n",
    "        u_xt = self.gradients(u_t, x)[0]\n",
    "        u_tx = self.gradients(u_x, t)[0]\n",
    "        \n",
    "        X_selector = torch.cat([uf, u_x, u_xx, u_tt, u_xt, u_tx], dim=1)\n",
    "        y_selector = u_t\n",
    "        \n",
    "        return X_selector, y_selector\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the SeclectorNetwork has to be a neural networks ???\n",
    "class SeclectorNetwork(nn.Module):\n",
    "    def __init__(self, X_train_dim, bn=None):\n",
    "        super().__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        self.nonlinear_model = TorchMLP(dimensions=[X_train_dim, 50, 50, 1], activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1), inp_drop=False)\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        ut_approx = self.nonlinear_model(inn)\n",
    "        return ut_approx\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        return mse_loss\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network(model=TorchMLP(dimensions=[6, 50, 50, 50 ,50, 50, 1], bn=nn.BatchNorm1d))\n",
    "# selector = SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm)\n",
    "\n",
    "### Version without normalized derivatives ###\n",
    "# semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "#                              selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "#                              normalize_derivative_features=False, \n",
    "#                              mini=None, \n",
    "#                              maxi=None)\n",
    "\n",
    "### Version with normalized derivatives ###\n",
    "referenced_derivatives = np.load(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\")\n",
    "semisup_model = SemiSupModel(network=Network(model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1], activation_function=nn.Tanh, bn=nn.LayerNorm, dropout=None)),\n",
    "                             selector=SeclectorNetwork(X_train_dim=6, bn=nn.LayerNorm),\n",
    "                             normalize_derivative_features=True, \n",
    "                             mini=to_tensor(np.min(referenced_derivatives, axis=0), False), \n",
    "                             maxi=to_tensor(np.max(referenced_derivatives, axis=0), False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure():\n",
    "    global N, X_u_train, u_train\n",
    "    uf, unsup_loss = semisup_model(X_u_train)\n",
    "    losses = [F.mse_loss(uf[:N, :], u_train), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)\n",
    "\n",
    "def closure():\n",
    "    global N, X_u_train, u_train\n",
    "    optimizer2.zero_grad()\n",
    "    mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_u_train))[:N, :], u_train)\n",
    "    mse_loss.backward(retain_graph=True)\n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate finding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28107c4609094948bd63b71911d293f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n",
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 8.31E-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = semisup_model.parameters()\n",
    "\n",
    "### For SGD and Adam ###\n",
    "learning_rate1, learning_rate2 = 1e-7, 1e-1\n",
    "\n",
    "### For LBFGS (a good choice already!!!) ###\n",
    "# print(\"Using LBFGS's learning rate set\")\n",
    "# learning_rate1, learning_rate2 = 8e-2, 5e-2 # (1e-1, 5e-2) is also OK!\n",
    "\n",
    "choice = 'Adam'; auto_lr = True\n",
    "if choice == 'LBFGS':\n",
    "    optimizer1 = torch.optim.LBFGS(params, lr=learning_rate1, \n",
    "                                   max_iter=100, max_eval=125, \n",
    "                                  history_size=120, line_search_fn='strong_wolfe')\n",
    "if choice == 'Adam':\n",
    "    optimizer1 = AdamGC(params, lr=learning_rate1, use_gc=True, gc_conv_only=False, gc_loc=False)\n",
    "if choice == 'SGD':\n",
    "    optimizer1 = SGDGC(params, lr=learning_rate1, use_gc=True, nesterov=True, momentum=0.95)\n",
    "\n",
    "if choice != 'LBFGS' and auto_lr:\n",
    "    print('Learning rate finding')\n",
    "    bs = 4000; bs = X_u_train.shape[0] if bs>X_u_train.shape[0] else bs\n",
    "    criterion = LadderLoss(return_list=True)\n",
    "    trainloader = get_dataloader(X_u_train, u_train, bs=bs)\n",
    "    \n",
    "    lr_finder = LRFinder(semisup_model, optimizer=optimizer1, \n",
    "                         closure=pcgrad_update, criterion=criterion, device=\"cpu\")\n",
    "    lr_finder.range_test(trainloader, val_loader=None, end_lr=100, num_iter=300)\n",
    "    \n",
    "    # to inspect the loss-learning rate graph\n",
    "    suggested_lr, _ = lr_finder.plot()\n",
    "    # To prevent divergence during the second stage training.\n",
    "    # suggested_lr = min(suggested_lr, 5e-3)\n",
    "    lr_finder.reset(); plt.show()\n",
    "\n",
    "else: suggested_lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learing_rate to the suggested one.\n",
    "# suggested_lr = float(input())\n",
    "\n",
    "if suggested_lr:\n",
    "    optimizer1 = lr_finder.optimizer\n",
    "    for g in optimizer1.param_groups:\n",
    "        g['lr'] = suggested_lr\n",
    "        \n",
    "epochs1 = 2000; epochs2 = 500;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the fake labels\n",
      "Epoch 0:  24.247135162353516\n",
      "Epoch 100:  0.43441689014434814\n",
      "Epoch 200:  0.3297484815120697\n",
      "Epoch 300:  0.402091383934021\n",
      "Epoch 400:  0.37094998359680176\n",
      "Epoch 500:  0.3270460069179535\n",
      "Epoch 600:  0.3675791621208191\n",
      "Epoch 700:  0.27286022901535034\n",
      "Epoch 800:  0.24477922916412354\n",
      "Epoch 900:  0.22676467895507812\n",
      "Epoch 1000:  0.19680368900299072\n",
      "Epoch 1100:  0.18936821818351746\n",
      "Epoch 1200:  0.1825016736984253\n",
      "Epoch 1300:  0.1650753766298294\n",
      "Epoch 1400:  0.1646546721458435\n",
      "Epoch 1500:  0.16164277493953705\n",
      "Epoch 1600:  0.15098685026168823\n",
      "Epoch 1700:  0.14939561486244202\n",
      "Epoch 1800:  0.1390657126903534\n",
      "Epoch 1900:  0.13724249601364136\n"
     ]
    }
   ],
   "source": [
    "print(\"Deleting the fake labels\")\n",
    "u_train = u_train[:N, :]\n",
    "\n",
    "semisup_model.train()\n",
    "curr_loss = 1000; F_print = 10 if choice == 'LBFGS' else 100\n",
    "\n",
    "# Stage I\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(pcgrad_closure)\n",
    "    l = pcgrad_closure()\n",
    "    if (i % F_print) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "            print(\"Finishing the first stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0007581480895169079\n",
      "Epoch 10:  1.4702807220601244e-06\n",
      "Epoch 20:  1.4410463791136863e-06\n",
      "Epoch 30:  1.1333431757520884e-06\n",
      "Epoch 40:  1.1096418575107236e-06\n",
      "Epoch 50:  1.025389337883098e-06\n",
      "Epoch 60:  8.919445235733292e-07\n",
      "Epoch 70:  7.843273124308325e-07\n",
      "Epoch 80:  7.066945499900612e-07\n",
      "Epoch 90:  6.893075124025927e-07\n",
      "Finishing the second stage\n",
      "Testing\n",
      "Test MSE: 3.7220761441858485e-06\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(semisup_model.network.parameters(), \n",
    "                              lr=learning_rate2, max_iter=100, max_eval=125, \n",
    "                              history_size=120, line_search_fn='strong_wolfe')\n",
    "\n",
    "curr_loss = 1000\n",
    "# Stage II\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 10) == 0:\n",
    "        if l.item() != curr_loss:\n",
    "            curr_loss = l.item()\n",
    "        else:\n",
    "            print(\"Finishing the second stage\")\n",
    "            break\n",
    "        print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "print(\"Testing\")\n",
    "semisup_model.network.eval()\n",
    "# Compare btw the two semi-supervise learning?\n",
    "print('Test MSE:', F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST-2000: 1e-06 (LBFGS)\n",
    "# torch.save(semisup_model.state_dict(), \"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model and testing\n",
    "# semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_burger/semisup_model_with_LayerNormDropout_without_physical_reg_trained2000labeledsamples_trained1000unlabeledsamples.pth\"), strict=False)\n",
    "# semisup_model.eval()\n",
    "# F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivatives_test, dynamics_test = semisup_model.network.get_selector_data(*dimension_slicing(X_star))\n",
    "# derivatives_train, dynamics_train = semisup_model.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "\n",
    "# derivatives_test, dynamics_test = to_numpy(derivatives_test), to_numpy(dynamics_test)\n",
    "# derivatives_train, dynamics_train = to_numpy(derivatives_train), to_numpy(dynamics_train)\n",
    "\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-3000-V1-with-1000unlabledsamples.npy\", derivatives_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-3000-V1-with-1000unlabledsamples.npy\", dynamics_train)\n",
    "# np.save(\"./saved_path_inverse_burger/data/derivatives-25600-V1-with-1000unlabledsamples.npy\", derivatives_test)\n",
    "# np.save(\"./saved_path_inverse_burger/data/dynamics-25600-V1-with-1000unlabledsamples.npy\", dynamics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
