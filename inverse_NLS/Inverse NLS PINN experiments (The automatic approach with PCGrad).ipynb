{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from lightning_utils import *\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, \n",
    "                    ComplexTorchMLP, ComplexSymPyModule, complex_mse)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Loading pre-calculated data for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_xx']\n",
    "\n",
    "### Loading data code here ###\n",
    "print(\"Loading pre-calculated data for reproducibility\")\n",
    "X_train = to_tensor(np.load(\"./tmp_files/X_train_500+500samples.npy\"), True)[:N, :]\n",
    "u_train, v_train = dimension_slicing(to_tensor(np.load(\"./tmp_files/uv_train_500samples.npy\"), False))\n",
    "u_train = u_train[:N, :]\n",
    "v_train = v_train[:N, :]\n",
    "h_train = torch.complex(u_train, v_train)\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn1 = 0.002494+1.002397*1j\n",
    "cn2 = 0.003655+0.500415*1j\n",
    "cns = [cn1, cn2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please ensure that all coefs are different\n",
      "X0*X1 {X1, X0}\n",
      "Please ensure that all coefs are different\n",
      "X2 {X2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexSymPyModule(\n",
       "  (sympymodule): SymPyModule(expressions=(X0*X1, X2))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type the equation got from the symbolic regression step\n",
    "# No need to save the eq save a pickle file before\n",
    "program1 = \"X0*X1\"\n",
    "pde_expr1, variables1,  = build_exp(program1); print(pde_expr1, variables1)\n",
    "\n",
    "program2 = \"X2\"\n",
    "pde_expr2, variables2,  = build_exp(program2); print(pde_expr2, variables2)\n",
    "\n",
    "mod = ComplexSymPyModule(expressions=[pde_expr1, pde_expr2], complex_coeffs=cns); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None):\n",
    "        super(ComplexPINN, self).__init__()\n",
    "        self.model = model\n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        if self.scale and (self.lb is None or self.ub is None): \n",
    "            print(\"Please provide thw lower and upper bounds of your PDE.\")\n",
    "            print(\"Otherwise, there will be error(s)\")\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        H = torch.cat([x, t], dim=1)\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, x, t, y_input, update_network_params=True, update_pde_params=True):\n",
    "        total_loss = []\n",
    "        grads_dict, u_t = self.grads_dict(x, t)\n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            mse_loss = complex_mse(grads_dict['X'+self.feature2index['hf']], y_input)\n",
    "            total_loss.append(mse_loss)\n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            l_eq = complex_mse(self.callable_loss_fn(grads_dict), u_t)\n",
    "            total_loss.append(l_eq)\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = {}\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives['X'+self.feature2index[t]] = cplx2tensor(uf)\n",
    "                derivatives['X1'] = (uf.real**2+uf.imag**2)+0.0j\n",
    "            elif t=='x': derivatives['X'+self.feature2index[t]] = x\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives['X'+self.feature2index['h_'+t[::-1]]] = out\n",
    "        \n",
    "        return derivatives, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    # Must ensure that the implementation of neural_net_scale is consistent\n",
    "    # and hopefully correct\n",
    "    # also, you might not need this function in some datasets\n",
    "    def neural_net_scale(self, inp): \n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "semisup_model_state_dict = cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")\n",
    "parameters = OrderedDict()\n",
    "\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "complex_model.load_state_dict(parameters)\n",
    "\n",
    "pinn = ComplexPINN(model=complex_model, loss_fn=mod, index2features=feature_names, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global X_train, h_train\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad()\n",
    "    losses = pinn.loss(X_train[:, 0:1], X_train[:, 1:2], h_train, update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad:\n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global X_train, h_train\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = pinn.loss(X_train[:, 0:1], X_train[:, 1:2], h_train, update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()): \n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs1, epochs2 = 200, 50\n",
    "# # TODO: Save best state dict and training for more epochs.\n",
    "# optimizer1 = MADGRAD(pinn.parameters(), lr=1e-7, momentum=0.9)\n",
    "# pinn.train(); best_train_loss = 1e6\n",
    "\n",
    "# print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "# for i in range(epochs1):\n",
    "#     optimizer1.step(mtl_closure)\n",
    "#     l = mtl_closure()\n",
    "#     if (i % 10) == 0 or i == epochs1-1:\n",
    "#         print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=300, max_eval=int(300*1.25), history_size=150, line_search_fn='strong_wolfe')\n",
    "# print('2nd Phase optimization using LBFGS')\n",
    "# for i in range(epochs2):\n",
    "#     optimizer2.step(closure)\n",
    "#     l = closure()\n",
    "#     if (i % 5) == 0 or i == epochs2-1:\n",
    "#         print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(pinn.state_dict(), \"./saved_path_inverse_nls/final_finetuned_cpinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "# grounds = np.array([1j, 0+0.5j])\n",
    "\n",
    "# errs = []\n",
    "# for i in range(len(grounds)):\n",
    "#     err = est_coeffs[i]-grounds[i]\n",
    "#     errs.append(100*abs(err.imag)/abs(grounds[i].imag))\n",
    "# errs = np.array(errs)\n",
    "# errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fun with lightning for simple best-practice finetuning procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be aware of the double neural net scaling\n",
    "class LightningComplexPINN(ParentFinetuner):\n",
    "    # Parent's args + additional args\n",
    "    def __init__(self, model, inp_scale=False, bounds=None, max_epochs=1000, lr=1e-3, n_obj=2):\n",
    "        super(LightningComplexPINN, self).__init__(model, inp_scale, bounds, max_epochs=1000)\n",
    "        self.n_obj = n_obj\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        return self.model(*args)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return MADGRAD(self.parameters(), lr=self.lr, momentum=0.9)\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        myopt = self.optimizers()\n",
    "        x, y = train_batch; x = x.view(x.size(0), -1)\n",
    "        spatial, time = dimension_slicing(x)\n",
    "        losses = self.model.loss(spatial, time, y, update_network_params=True, update_pde_params=True)\n",
    "        \n",
    "        # Before calling the trainer.fit function\n",
    "        # automatic optimization is enabled when tuning the learning rate\n",
    "        if self.automatic_optimization:\n",
    "            self.log('train_loss', sum(losses))\n",
    "            return sum(losses)\n",
    "        \n",
    "        # Applying PCGrad algo\n",
    "        updated_grads = []\n",
    "        \n",
    "        for i in range(self.n_obj):\n",
    "            myopt.zero_grad()\n",
    "            self.manual_backward(losses[i], retain_graph=True)\n",
    "\n",
    "            g_task = []\n",
    "            for param in self.model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "                else:\n",
    "                    g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "            # appending the gradients from each task\n",
    "            updated_grads.append(g_task)\n",
    "\n",
    "        updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "        for idx, param in enumerate(pinn.parameters()): \n",
    "            param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "            \n",
    "        myopt.step()\n",
    "        \n",
    "        self.log('train_loss', sum(losses))\n",
    "        return sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "ft = LightningComplexPINN(model=pinn, max_epochs=2000)\n",
    "trainer = pl.Trainer(precision=32, auto_scale_batch_size=False, auto_lr_find=True, deterministic=True, amp_backend='native', max_epochs=ft.max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(ft.model.loss(X_train[:, 0:1], X_train[:, 1:2], h_train, update_network_params=True, update_pde_params=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = XYDataset(X_train, h_train)\n",
    "dataloader = DataLoader(dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping val loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | ComplexPINN | 81.6 K\n",
      "--------------------------------------\n",
      "81.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "81.6 K    Total params\n",
      "0.326     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e350a00be245969a5468584e5a062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Finding best initial lr'), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early after 39 steps due to diverging loss.\n",
      "Restored states from the checkpoint file at /Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/lr_find_temp_model.ckpt\n",
      "Learning rate set to 7.585775750291837e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr_find': <pytorch_lightning.tuner.lr_finder._LRFinder at 0x14b4e76d0>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.automatic_optimization = True\n",
    "trainer.tune(ft, train_dataloader=dataloader, val_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | model | ComplexPINN | 81.6 K\n",
      "--------------------------------------\n",
      "81.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "81.6 K    Total params\n",
      "0.326     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9a3df1404648879e2bfea537fae5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ft.automatic_optimization = False\n",
    "trainer.fit(ft, train_dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0304, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ft.model.loss(X_train[:, 0:1], X_train[:, 1:2], h_train, update_network_params=True, update_pde_params=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = ft.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  7.828387424524408e-06\n",
      "Epoch 5:  7.828387424524408e-06\n",
      "Epoch 10:  7.828387424524408e-06\n",
      "Epoch 15:  7.828387424524408e-06\n",
      "Epoch 20:  7.828387424524408e-06\n",
      "Epoch 25:  7.828387424524408e-06\n",
      "Epoch 30:  7.828387424524408e-06\n",
      "Epoch 35:  7.828387424524408e-06\n",
      "Epoch 40:  7.828387424524408e-06\n",
      "Epoch 45:  7.828387424524408e-06\n",
      "Epoch 49:  7.828387424524408e-06\n"
     ]
    }
   ],
   "source": [
    "epochs2 = 50\n",
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=300, max_eval=int(300*1.25), history_size=150, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    l = closure()\n",
    "    if (i % 5) == 0 or i == epochs2-1:\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
