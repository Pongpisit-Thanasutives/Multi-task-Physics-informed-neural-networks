{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from lightning_utils import *\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, \n",
    "                    ComplexTorchMLP, ComplexSymPyModule, complex_mse)\n",
    "from models import RobustPCANN\n",
    "from pytorch_robust_pca import *\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "import lookahead\n",
    "\n",
    "# BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "from skopt import Optimizer\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Loading pre-calculated (clean) data for reproducibility\n",
      "Noisy (x, t)\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Adding noise\n",
    "noisy_lables = True\n",
    "noisy_xt = True\n",
    "noise_intensity = 0.0\n",
    "if noisy_lables: noise_intensity = 0.01/np.sqrt(2) \n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "# if noisy_xt:\n",
    "#     print(\"Noisy (x, t)\")\n",
    "#     X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "# else: print(\"Clean (x, t)\")\n",
    "\n",
    "# X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "# u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "# v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_xx']\n",
    "\n",
    "### Loading (clean) data code here ###\n",
    "print(\"Loading pre-calculated (clean) data for reproducibility\")\n",
    "X_train = np.load(\"./tmp_files/X_train_500+500samples.npy\")\n",
    "\n",
    "if noise_intensity > 0.0 and noisy_xt:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_train = perturb(X_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "X_train = to_tensor(X_train, True)[:N, :]\n",
    "\n",
    "uv_train = np.load(\"./tmp_files/uv_train_500samples.npy\")\n",
    "u_train = uv_train[:, 0:1]; v_train = uv_train[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed u_train and v_train with intensity = 0.0070710678118654745\n"
     ]
    }
   ],
   "source": [
    "if noise_intensity > 0.0:\n",
    "    noise_u = perturb(u_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    u_train = u_train + noise_u\n",
    "    noise_v = perturb(v_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    v_train = v_train + noise_v\n",
    "    print(\"Perturbed u_train and v_train with intensity =\", float(noise_intensity))\n",
    "u_train = u_train[:N, :]; v_train = v_train[:N, :]\n",
    "\n",
    "u_train, v_train = to_tensor(u_train, False), to_tensor(v_train, False)\n",
    "h_train = torch.complex(u_train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn1 = -0.020763+1.029535*1j\n",
    "cn2 = -0.016447+0.509700*1j\n",
    "cns = [cn1, cn2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0*X1 {X0, X1}\n",
      "X2 {X2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexSymPyModule(\n",
       "  (sympymodule): SymPyModule(expressions=(X0*X1, X2))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type the equation got from the symbolic regression step\n",
    "# No need to save the eq save a pickle file before\n",
    "program1 = \"X0*X1\"\n",
    "pde_expr1, variables1,  = build_exp(program1); print(pde_expr1, variables1)\n",
    "\n",
    "program2 = \"X2\"\n",
    "pde_expr2, variables2,  = build_exp(program2); print(pde_expr2, variables2)\n",
    "\n",
    "mod = ComplexSymPyModule(expressions=[pde_expr1, pde_expr2], complex_coeffs=cns); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None):\n",
    "        super(ComplexPINN, self).__init__()        \n",
    "        self.model = model\n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        if self.scale and (self.lb is None or self.ub is None):\n",
    "            print(\"Please provide thw lower and upper bounds of your PDE.\")\n",
    "            print(\"Otherwise, there will be error(s)\")\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def forward(self, H):\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, HL, y_input, update_network_params=True, update_pde_params=True):\n",
    "        total_loss = []\n",
    "        \n",
    "        # Forwarding\n",
    "        grads_dict, u_t = self.grads_dict(HL[:, 0:1], HL[:, 1:2])\n",
    "        \n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            total_loss.append(complex_mse(grads_dict['X'+self.feature2index['hf']], y_input))\n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            total_loss.append(complex_mse(self.callable_loss_fn(grads_dict), u_t))\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(cat(x, t))\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = {}\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives['X'+self.feature2index[t]] = cplx2tensor(uf)\n",
    "                derivatives['X1'] = (uf.real**2+uf.imag**2)+0.0j\n",
    "            elif t=='x': derivatives['X'+self.feature2index[t]] = x\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives['X'+self.feature2index['h_'+t[::-1]]] = out\n",
    "        \n",
    "        return derivatives, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    # Must ensure that the implementation of neural_net_scale is consistent\n",
    "    # and hopefully correct\n",
    "    # also, you might not need this function in some datasets\n",
    "    def neural_net_scale(self, inp): \n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretrained model\n",
    "semisup_model_state_dict = cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")\n",
    "parameters = OrderedDict()\n",
    "\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "complex_model.load_state_dict(parameters)\n",
    "\n",
    "# pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, index2features=feature_names, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S, x_fft, x_PSD, t_fft, t_PSD\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad(set_to_none=True)\n",
    "    losses = pinn.loss(X_train, h_train, update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad: \n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S, x_fft, x_PSD, t_fft, t_PSD\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = pinn.loss(X_train, h_train, update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad(set_to_none=True)\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in pinn.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(pinn.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best thresold wrt to the first-epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn = ComplexPINN(model=complex_model, loss_fn=mod, index2features=feature_names, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.02876652032136917\n",
      "Epoch 10:  0.01757701113820076\n",
      "Epoch 20:  0.013971677981317043\n",
      "Epoch 30:  0.012121869251132011\n",
      "Epoch 40:  0.010788709856569767\n",
      "Epoch 50:  0.009743858128786087\n",
      "Epoch 60:  0.00919412449002266\n",
      "Epoch 70:  0.008641034364700317\n",
      "Epoch 80:  0.008198561146855354\n",
      "Epoch 90:  0.00783234927803278\n",
      "Epoch 100:  0.007600048091262579\n",
      "Epoch 110:  0.007373086642473936\n",
      "Epoch 120:  0.007169131189584732\n",
      "Epoch 130:  0.007018537260591984\n",
      "Epoch 140:  0.006865114904940128\n",
      "Epoch 150:  0.006750781089067459\n",
      "Epoch 160:  0.006645225919783115\n",
      "Epoch 170:  0.00654009310528636\n",
      "Epoch 180:  0.00651969388127327\n",
      "Epoch 190:  0.006536360364407301\n",
      "Epoch 199:  0.00643113162368536\n"
     ]
    }
   ],
   "source": [
    "epochs1, epochs2 = 200, 30\n",
    "\n",
    "# optimizer1 = torch.optim.LBFGS(list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())+list(pinn.model.parameters())+list(pinn.callable_loss_fn.parameters()), lr=1e-3, line_search_fn='strong_wolfe') also work!\n",
    "optimizer1 = MADGRAD(pinn.parameters(), lr=5e-7, momentum=0.95)\n",
    "\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.0036612548865377903\n",
      "[ 1.3383147e-05+1.0033953j -1.2189113e-03+0.4990163j]\n",
      "Epoch 1:  0.0034946873784065247\n",
      "[-0.00221974+1.001624j   -0.0018779 +0.49773464j]\n",
      "Epoch 2:  0.0034387956839054823\n",
      "[-0.00300295+1.0007726j  -0.00191804+0.49700785j]\n",
      "Epoch 3:  0.003408566117286682\n",
      "[-0.00347568+1.0013471j  -0.00140845+0.49669296j]\n",
      "Epoch 4:  0.0033755991607904434\n",
      "[-0.00316265+1.0005379j  -0.00043661+0.49611694j]\n",
      "Epoch 5:  0.0033424352295696735\n",
      "[-0.00301725+1.0001249j   0.00097409+0.49504527j]\n",
      "Epoch 6:  0.0032808412797749043\n",
      "[-0.00304911+0.99774384j  0.00243741+0.49505j   ]\n",
      "Epoch 7:  0.0031319058034569025\n",
      "[-0.0044014 +0.9966324j   0.00618206+0.49404308j]\n",
      "Epoch 8:  0.002972292248159647\n",
      "[-0.00067986+0.9963625j  0.01213049+0.493819j ]\n",
      "Epoch 9:  0.00274738110601902\n",
      "[-0.00316572+0.9991702j  0.00972581+0.4951677j]\n",
      "Epoch 10:  0.0025557056069374084\n",
      "[-0.00388305+1.0102781j   0.01063359+0.49910393j]\n",
      "Epoch 11:  0.002373293973505497\n",
      "[-0.01336616+1.012565j  0.01486195+0.495773j]\n",
      "Epoch 12:  0.0021433441434055567\n",
      "[-0.0255637 +1.0168743j   0.01006221+0.49525848j]\n",
      "Epoch 13:  0.0019004202913492918\n",
      "[-0.03855234+0.99884367j  0.00315409+0.49537656j]\n",
      "Epoch 14:  0.001704198308289051\n",
      "[-0.04530052+0.9981581j   0.00548455+0.48835775j]\n",
      "Epoch 15:  0.001535384333692491\n",
      "[-0.04834298+0.99812216j  0.00915387+0.48443025j]\n",
      "Epoch 16:  0.0014332833234220743\n",
      "[-0.04942739+0.99291134j  0.01319099+0.47541174j]\n",
      "Epoch 17:  0.0013769876677542925\n",
      "[-0.05329417+0.99529797j  0.01305729+0.4738433j ]\n",
      "Epoch 18:  0.001323796110227704\n",
      "[-0.04398948+0.99588996j  0.01279376+0.4773691j ]\n",
      "Epoch 19:  0.0012802850687876344\n",
      "[-0.04140653+0.9970013j  0.01264542+0.476301j ]\n",
      "Epoch 20:  0.001243632286787033\n",
      "[-0.04531271+0.9918987j  0.01414709+0.4718893j]\n",
      "Epoch 21:  0.0012138093588873744\n",
      "[-0.04841026+0.9902915j   0.01589916+0.47064677j]\n",
      "Epoch 22:  0.001180743332952261\n",
      "[-0.0516275 +0.98461306j  0.02101423+0.46997347j]\n",
      "Epoch 23:  0.0011485746363177896\n",
      "[-0.05053871+0.9804964j  0.02169234+0.469903j ]\n",
      "Epoch 24:  0.0011114728404209018\n",
      "[-0.05356125+0.98470104j  0.02040506+0.4697006j ]\n",
      "Epoch 25:  0.0010706231696531177\n",
      "[-0.05053008+0.9908953j  0.02281196+0.4740539j]\n",
      "Epoch 26:  0.0010335652623325586\n",
      "[-0.04749552+0.99091464j  0.01940071+0.47861654j]\n",
      "Epoch 27:  0.0009906386258080602\n",
      "[-0.03844928+0.99045587j  0.02117298+0.48077452j]\n",
      "Epoch 28:  0.0009406034369021654\n",
      "[-0.03784754+0.9919075j   0.02242445+0.47579232j]\n",
      "Epoch 29:  0.0008933324133977294\n",
      "[-0.03917403+1.0023042j  0.02319373+0.475674j ]\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, \n",
    "                               max_eval=int(500*1.25), history_size=150, \n",
    "                               line_search_fn='strong_wolfe')\n",
    "\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 1) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.5154829025268555, 0.936591625213623)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "print(est_coeffs)\n",
    "# est_coeffs = npar([8.1524391e-05+0.9902236j, 5.3029885e-03+0.49562332j])\n",
    "grounds = np.array([1j, 0+0.5j])\n",
    "\n",
    "errs = []\n",
    "for i in range(len(grounds)):\n",
    "    err = est_coeffs[i]-grounds[i]\n",
    "    errs.append(100*abs(err.imag)/abs(grounds[i].imag))\n",
    "    \n",
    "errs = np.array(errs)\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/o DFT\n",
    "# clean all\n",
    "# [0.00019074+0.99958783j 0.00025855+0.5001493j]\n",
    "# (0.03553926944732666, 0.005677342414855957)\n",
    "\n",
    "# clean (x, t) | noisy lables\n",
    "# [0.00063039+1.0004296j  0.00092531+0.50109816j]\n",
    "# (0.13129711151123047, 0.0883340835571289)\n",
    "\n",
    "# noisy (x, t) | noisy lables\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U + Noise\n",
    "# array([0.00118875+0.9984036j, 0.00168767+0.4998621j], dtype=complex64)\n",
    "# (0.09360909461975098, 0.06603002548217773)\n",
    "# (Parameter containing:\n",
    "#  tensor([-0.0012], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([-0.0022], requires_grad=True))\n",
    "\n",
    "# U + Noise | (x, t) + Noise\n",
    "# (1)\n",
    "# array([8.1524391e-05+0.9902236j, 5.3029885e-03+0.49562332j], dtype=complex64)\n",
    "# (0.9264880000000031, 0.051151999999998754)\n",
    "# (Parameter containing:\n",
    "#  tensor([0.5776], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([-1.0001], requires_grad=True))\n",
    "# (2) (func=lambda x:(torch.exp(-F.relu(x))))\n",
    "# [0.00497404+0.99919987j 0.00771778+0.49703848j]\n",
    "# (0.33615827560424805, 0.25614500045776367)\n",
    "# (Parameter containing:\n",
    "#  tensor([-0.1508], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([-0.0031], requires_grad=True))\n",
    "\n",
    "# U + Noise | (x, t) + Noise | \"./saved_path_inverse_nls/noisy2_final_finetuned_doublebetarpca_fftthcpinnV2.pth\"\n",
    "# U + Noise | (x, t) + Noise\n",
    "# (1)\n",
    "# [0.00297839+1.0081341j  0.00733745+0.50027394j]\n",
    "# (0.4341006278991699, 0.37931203842163086)\n",
    "# (Parameter containing:\n",
    "#  tensor([-0.0341], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([-0.0313], requires_grad=True))\n",
    "# (2) (func=lambda x:(torch.exp(-F.relu(x))))\n",
    "# [ 0.00067096+1.0004205j -0.00044082+0.5008436j]\n",
    "# (0.10538101196289062, 0.06333589553833008)\n",
    "# (Parameter containing:\n",
    "#  tensor([-0.0026], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([0.0011], requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Exact & Clean (x, t)\n",
    "# (0.05885958671569824, 0.021964311599731445)\n",
    "# array([-0.00046226+0.99919176j, -0.00056662+0.49981552j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# (0.6996273994445801, 0.01595020294189453)\n",
    "# array([0.00149273+0.9928442j, 0.00079829+0.5034184j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7112264633178711, 0.00553131103515625)\n",
    "# array([ 3.449592e-03+1.007057j , -7.125967e-05+0.5035838j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7093071937561035, 0.0036716461181640625)\n",
    "# array([ 3.4442921e-03+1.0070564j, -5.4004795e-05+0.5035649j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.1215517520904541, 0.08192658424377441)\n",
    "# array([-8.2360100e-05+0.99960375j, -6.1671366e-05+0.5010174j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.511014461517334, 0.25589466094970703)\n",
    "# array([-0.01472272+1.0076691j, -0.02164156+0.5012756j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Noisy (x, t) & X_train = X_train_L+1*1*X_train_S+beta*NN(X_train_S)\n",
    "# (0.5050361156463623, 0.1848280429840088)\n",
    "# array([ 0.00107117+1.0032021j, -0.01103256+0.5034493j], dtype=complex64)\n",
    "# beta = 0.005178438033908606\n",
    "\n",
    "# Notes\n",
    "# X_star = X_star-X_star_S -> Seems robust but not stable\n",
    "# X_star = X_star_L+X_star_S -> The best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results on Double Beta-RobustFFT ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([-4.01791149e-05+0.9997733j, 1.09734545e-04+0.5006671j], dtype=complex64)\n",
    "# (0.07804334163665771, 0.05537569522857666)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([0.0085], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0027], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([0.00171628+1.0023999j, 0.00308448+0.5002444j], dtype=complex64)\n",
    "# (0.14443397521972656, 0.09555816650390625)\n",
    "# (Parameter containing:\n",
    "#  tensor([0.0029], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([0.0012], requires_grad=True))\n",
    "# --- V2 ---\n",
    "# array([0.00039933+1.0002806j, 0.00156634+0.5011481j], dtype=complex64)\n",
    "# (0.12884140014648438, 0.10077953338623047)\n",
    "# (Parameter containing:\n",
    "#  tensor([0.9966], requires_grad=True),\n",
    "#  Parameter containing:\n",
    "#  tensor([0.9989], requires_grad=True))\n",
    "\n",
    "### Results on Double Beta-RobustPCA ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([0.00077563+1.0028679j, 0.00166233+0.50137794j], dtype=complex64)\n",
    "# (0.2811908721923828, 0.005602836608886719)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0002], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([-0.00045199+1.0037338j, 0.00022461+0.5013247j], dtype=complex64)\n",
    "# (0.31915903091430664, 0.05421638488769531)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0011], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expedia hotel recommendation -> Case study data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  0.0008456204668618739\n",
      "[-0.03843471+1.0095063j   0.02373311+0.48180342j]\n",
      "Epoch 1:  0.0008089127950370312\n",
      "[-0.04077225+1.0160183j   0.025232  +0.48101968j]\n",
      "Epoch 2:  0.0007668425678275526\n",
      "[-0.04660411+1.019314j    0.02780176+0.47812477j]\n",
      "Epoch 3:  0.0007366547361016273\n",
      "[-0.04963372+1.0211407j   0.02907412+0.47664908j]\n",
      "Epoch 4:  0.0007135001942515373\n",
      "[-0.0509875 +1.0194252j   0.02618832+0.47897285j]\n",
      "Epoch 5:  0.0006880747387185693\n",
      "[-0.05309521+1.0174097j  0.02392182+0.4795014j]\n",
      "Epoch 6:  0.0006729239830747247\n",
      "[-0.05398299+1.0159353j   0.02261916+0.47828493j]\n",
      "Epoch 7:  0.0006715587805956602\n",
      "[-0.05401897+1.0158172j   0.02255035+0.47826442j]\n",
      "Epoch 8:  0.0006715587223879993\n",
      "[-0.05401897+1.0158172j   0.02255035+0.47826442j]\n",
      "Epoch 9:  0.0006619006744585931\n",
      "[-0.05380561+1.0148705j   0.02199353+0.47874272j]\n",
      "Epoch 10:  0.0006531650433316827\n",
      "[-0.05360104+1.0149411j   0.02056175+0.47954082j]\n",
      "Epoch 11:  0.0006435803952626884\n",
      "[-0.05317996+1.0155511j   0.01835804+0.48048675j]\n",
      "Epoch 12:  0.0006353980279527605\n",
      "[-0.05322706+1.016167j    0.01630495+0.48158172j]\n",
      "Epoch 13:  0.0006294535123743117\n",
      "[-0.05364105+1.0158968j   0.01521019+0.48258007j]\n",
      "Epoch 14:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 15:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 16:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 17:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 18:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 19:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 20:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 21:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 22:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 23:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 24:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 25:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 26:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 27:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 28:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n",
      "Epoch 29:  0.0006283231778070331\n",
      "[-0.05378481+1.0157889j   0.0151101 +0.48273963j]\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 1) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
