{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.6\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from lightning_utils import *\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, \n",
    "                    ComplexTorchMLP, ComplexSymPyModule, complex_mse)\n",
    "from models import RobustPCANN\n",
    "from pytorch_robust_pca import *\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "import lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Loading pre-calculated (clean) data for reproducibility\n",
      "Noisy (x, t)\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.01/np.sqrt(2)\n",
    "noisy_xt = True\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "# if noisy_xt:\n",
    "#     print(\"Noisy (x, t)\")\n",
    "#     X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "# else: print(\"Clean (x, t)\")\n",
    "\n",
    "# X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "# u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "# v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_xx']\n",
    "\n",
    "### Loading (clean) data code here ###\n",
    "print(\"Loading pre-calculated (clean) data for reproducibility\")\n",
    "X_train = np.load(\"./tmp_files/X_train_500+500samples.npy\")\n",
    "\n",
    "if noise_intensity > 0.0 and noisy_xt:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_train = perturb(X_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "X_train = to_tensor(X_train, True)[:N, :]\n",
    "\n",
    "uv_train = np.load(\"./tmp_files/uv_train_500samples.npy\")\n",
    "u_train = uv_train[:, 0:1]; v_train = uv_train[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed u_train and v_train with intensity = 0.0070710678118654745\n"
     ]
    }
   ],
   "source": [
    "if noise_intensity > 0.0:\n",
    "    noise_u = perturb(u_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    u_train = u_train + noise_u\n",
    "    noise_v = perturb(v_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    v_train = v_train + noise_v\n",
    "    print(\"Perturbed u_train and v_train with intensity =\", float(noise_intensity))\n",
    "u_train = u_train[:N, :]; v_train = v_train[:N, :]\n",
    "\n",
    "u_train, v_train = to_tensor(u_train, False), to_tensor(v_train, False)\n",
    "h_train = torch.complex(u_train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn1 = 0.002494+1.002397*1j\n",
    "cn2 = 0.003655+0.500415*1j\n",
    "cns = [cn1, cn2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0*X1 {X1, X0}\n",
      "X2 {X2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexSymPyModule(\n",
       "  (sympymodule): SymPyModule(expressions=(X0*X1, X2))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type the equation got from the symbolic regression step\n",
    "# No need to save the eq save a pickle file before\n",
    "program1 = \"X0*X1\"\n",
    "pde_expr1, variables1,  = build_exp(program1); print(pde_expr1, variables1)\n",
    "\n",
    "program2 = \"X2\"\n",
    "pde_expr2, variables2,  = build_exp(program2); print(pde_expr2, variables2)\n",
    "\n",
    "mod = ComplexSymPyModule(expressions=[pde_expr1, pde_expr2], complex_coeffs=cns); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustComplexPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, init_cs=(0.0,), init_betas=(0.0, 0.0)):\n",
    "        super(RobustComplexPINN, self).__init__()\n",
    "        # FFTNN\n",
    "        self.fft_nn = FFTNN(c=init_cs[0])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        # Beta-Robust PCA\n",
    "        self.inp_rpca = RobustPCANN(beta=init_betas[0], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "        self.out_rpca = RobustPCANN(beta=init_betas[1], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "\n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        if self.scale and (self.lb is None or self.ub is None):\n",
    "            print(\"Please provide thw lower and upper bounds of your PDE.\")\n",
    "            print(\"Otherwise, there will be error(s)\")\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def forward(self, H):\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, HL, HS, y_input, y_input_S, update_network_params=True, update_pde_params=True):\n",
    "        total_loss = []\n",
    "        \n",
    "        y_input_S = torch.fft.ifft(self.fft_nn(y_input_S[1])*y_input_S[0]).reshape(-1, 1)\n",
    "        y_input_S = y_input - y_input_S\n",
    "        \n",
    "        H = self.inp_rpca(HL, HS, normalize=True)\n",
    "        \n",
    "        y_input = self.out_rpca(cat(y_input.real, y_input.imag), \n",
    "                                cat(y_input_S.real, y_input_S.imag), \n",
    "                                normalize=True)\n",
    "        y_input = torch.complex(y_input[:, 0:1], y_input[:, 1:2])\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(H[:, 0:1], H[:, 1:2])\n",
    "        \n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            total_loss.append(complex_mse(grads_dict['X'+self.feature2index['hf']], y_input))\n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            total_loss.append(complex_mse(self.callable_loss_fn(grads_dict), u_t))\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(cat(x, t))\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = {}\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives['X'+self.feature2index[t]] = cplx2tensor(uf)\n",
    "                derivatives['X1'] = (uf.real**2+uf.imag**2)+0.0j\n",
    "            elif t=='x': derivatives['X'+self.feature2index[t]] = x\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives['X'+self.feature2index['h_'+t[::-1]]] = out\n",
    "        \n",
    "        return derivatives, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    # Must ensure that the implementation of neural_net_scale is consistent\n",
    "    # and hopefully correct\n",
    "    # also, you might not need this function in some datasets\n",
    "    def neural_net_scale(self, inp): \n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "semisup_model_state_dict = cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")\n",
    "parameters = OrderedDict()\n",
    "\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "complex_model.load_state_dict(parameters)\n",
    "\n",
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, index2features=feature_names, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad(set_to_none=True)\n",
    "    losses = pinn.loss(X_train, X_train_S, h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad: \n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = pinn.loss(X_train, X_train_S, h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad(set_to_none=True)\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in list(pinn.model.parameters())+list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters()):\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(list(pinn.model.parameters())+list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best thresold wrt to the first-epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_x, _, _ = fft1d_denoise(X_train[:, 0:1], c=0, return_real=True)\n",
    "noise_x = X_train[:, 0:1]-noise_x\n",
    "noise_t, _, _ = fft1d_denoise(X_train[:, 1:2], c=0, return_real=True)\n",
    "noise_t = X_train[:, 1:2]-noise_t\n",
    "X_train_S = cat(noise_x, noise_t)\n",
    "\n",
    "h_train_S, h_train_fft, h_train_PSD = fft1d_denoise(h_train, c=-1, return_real=False)\n",
    "h_train_S = h_train-h_train_S\n",
    "\n",
    "del noise_x, noise_t\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, \n",
    "                         index2features=feature_names, scale=False, lb=lb, ub=ub, \n",
    "                         init_cs=(0.0,), init_betas=(1.0, 1.0))\n",
    "\n",
    "c_range = np.arange(-5.0, 5.1, step=0.1)\n",
    "\n",
    "performances = []\n",
    "for c in c_range:\n",
    "    pinn.fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c)]), requires_grad=False)\n",
    "    losses = pinn.loss(X_train, X_train_S, h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    performances.append(sum(losses).item())\n",
    "if 'pinn' in globals(): del pinn\n",
    "plt.plot(c_range, performances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best c = 0.9999999999999787\n"
     ]
    }
   ],
   "source": [
    "performances = npar(performances)\n",
    "indices = np.where(performances == performances.min())[0]\n",
    "bcs = (min(c_range[indices.min()], c_range[indices.max()]), )\n",
    "print(\"The best c =\", bcs[0])\n",
    "del c_range, performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.013811797834932804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../lookahead.py:38: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  0.010488282889127731\n",
      "Epoch 20:  0.009133826941251755\n",
      "Epoch 30:  0.008633693680167198\n",
      "Epoch 40:  0.007794161327183247\n",
      "Epoch 50:  0.007390707265585661\n",
      "Epoch 60:  0.007943921722471714\n",
      "Epoch 70:  0.007069484330713749\n",
      "Epoch 80:  0.006837102584540844\n",
      "Epoch 90:  0.0066229673102498055\n",
      "Epoch 99:  0.006479864474385977\n"
     ]
    }
   ],
   "source": [
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, \n",
    "                         index2features=feature_names, scale=False, lb=lb, ub=ub, \n",
    "                         init_cs=bcs, init_betas=(0.0, 0.0))\n",
    "\n",
    "epochs1, epochs2 = 100, 0\n",
    "# list(pinn.model.parameters())+list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())\n",
    "# pinn.parameters()\n",
    "optimizer1 = MADGRAD(list(pinn.model.parameters())+list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters()), lr=1e-7, momentum=0.9)\n",
    "fft_opt = lookahead.LookaheadAdam(pinn.fft_nn.parameters(), lr=1.0)\n",
    "\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    fft_opt.step()\n",
    "    fft_opt.zero_grad()\n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=150, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 5) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.fft_nn.c.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading weights for testing\n",
    "# pinn = load_weights(pinn, \"./saved_path_inverse_nls/final_finetuned_uncert_cpinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.002494+1.002397j, 0.003655+0.500415j], dtype=complex64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "est_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16134977340698242, 0.07834434509277344)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "grounds = np.array([1j, 0+0.5j])\n",
    "\n",
    "errs = []\n",
    "for i in range(len(grounds)):\n",
    "    err = est_coeffs[i]-grounds[i]\n",
    "    errs.append(100*abs(err.imag)/abs(grounds[i].imag))\n",
    "errs = np.array(errs)\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(pinn, \"./saved_path_inverse_nls/noisy2_final_finetuned_doublebetarpca_fftcpinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([-0.0011], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0002], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.inp_rpca.beta, pinn.out_rpca.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Exact & Clean (x, t)\n",
    "# (0.05885958671569824, 0.021964311599731445)\n",
    "# array([-0.00046226+0.99919176j, -0.00056662+0.49981552j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# (0.6996273994445801, 0.01595020294189453)\n",
    "# array([0.00149273+0.9928442j, 0.00079829+0.5034184j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7112264633178711, 0.00553131103515625)\n",
    "# array([ 3.449592e-03+1.007057j , -7.125967e-05+0.5035838j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7093071937561035, 0.0036716461181640625)\n",
    "# array([ 3.4442921e-03+1.0070564j, -5.4004795e-05+0.5035649j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.1215517520904541, 0.08192658424377441)\n",
    "# array([-8.2360100e-05+0.99960375j, -6.1671366e-05+0.5010174j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.511014461517334, 0.25589466094970703)\n",
    "# array([-0.01472272+1.0076691j, -0.02164156+0.5012756j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Noisy (x, t) & X_train = X_train_L+1*1*X_train_S+beta*NN(X_train_S)\n",
    "# (0.5050361156463623, 0.1848280429840088)\n",
    "# array([ 0.00107117+1.0032021j, -0.01103256+0.5034493j], dtype=complex64)\n",
    "# beta = 0.005178438033908606\n",
    "\n",
    "# Notes\n",
    "# X_star = X_star-X_star_S -> Seems robust but not stable\n",
    "# X_star = X_star_L+X_star_S -> The best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results on Double Beta-RobustFFT ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([-4.01791149e-05+0.9997733j, 1.09734545e-04+0.5006671j], dtype=complex64)\n",
    "# (0.07804334163665771, 0.05537569522857666)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([0.0085], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0027], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([0.002494+1.002397j, 0.003655+0.500415j], dtype=complex64)\n",
    "# (0.16134977340698242, 0.07834434509277344)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([0.0003], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0003], requires_grad=True))\n",
    "\n",
    "### Results on Double Beta-RobustPCA ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([0.00077563+1.0028679j, 0.00166233+0.50137794j], dtype=complex64)\n",
    "# (0.2811908721923828, 0.005602836608886719)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0002], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([-0.00045199+1.0037338j, 0.00022461+0.5013247j], dtype=complex64)\n",
    "# (0.31915903091430664, 0.05421638488769531)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0011], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
