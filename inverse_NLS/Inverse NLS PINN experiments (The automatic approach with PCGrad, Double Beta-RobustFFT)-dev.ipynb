{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.6\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from lightning_utils import *\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, \n",
    "                    ComplexTorchMLP, ComplexSymPyModule, complex_mse)\n",
    "from models import RobustPCANN\n",
    "from pytorch_robust_pca import *\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "import lookahead\n",
    "\n",
    "# BayesianOptimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Loading pre-calculated (clean) data for reproducibility\n",
      "Noisy (x, t)\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Adding noise\n",
    "noise_intensity = 0.01/np.sqrt(2)\n",
    "noisy_xt = True\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "# if noisy_xt:\n",
    "#     print(\"Noisy (x, t)\")\n",
    "#     X_star = perturb(X_star, intensity=noise_intensity, noise_type=\"normal\")\n",
    "# else: print(\"Clean (x, t)\")\n",
    "\n",
    "# X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "# u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "# v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_xx']\n",
    "\n",
    "### Loading (clean) data code here ###\n",
    "print(\"Loading pre-calculated (clean) data for reproducibility\")\n",
    "X_train = np.load(\"./tmp_files/X_train_500+500samples.npy\")\n",
    "\n",
    "if noise_intensity > 0.0 and noisy_xt:\n",
    "    print(\"Noisy (x, t)\")\n",
    "    X_train = perturb(X_train, intensity=noise_intensity, noise_type=\"normal\")\n",
    "else: print(\"Clean (x, t)\")\n",
    "\n",
    "X_train = to_tensor(X_train, True)[:N, :]\n",
    "\n",
    "uv_train = np.load(\"./tmp_files/uv_train_500samples.npy\")\n",
    "u_train = uv_train[:, 0:1]; v_train = uv_train[:, 1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed u_train and v_train with intensity = 0.0070710678118654745\n"
     ]
    }
   ],
   "source": [
    "if noise_intensity > 0.0:\n",
    "    noise_u = perturb(u_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    u_train = u_train + noise_u\n",
    "    noise_v = perturb(v_train, intensity=noise_intensity, noise_type=\"normal\", overwrite=False)\n",
    "    v_train = v_train + noise_v\n",
    "    print(\"Perturbed u_train and v_train with intensity =\", float(noise_intensity))\n",
    "u_train = u_train[:N, :]; v_train = v_train[:N, :]\n",
    "\n",
    "u_train, v_train = to_tensor(u_train, False), to_tensor(v_train, False)\n",
    "h_train = torch.complex(u_train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn1 = 0.002494+1.002397*1j\n",
    "cn2 = 0.003655+0.500415*1j\n",
    "cns = [cn1, cn2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X0*X1 {X1, X0}\n",
      "X2 {X2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ComplexSymPyModule(\n",
       "  (sympymodule): SymPyModule(expressions=(X0*X1, X2))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type the equation got from the symbolic regression step\n",
    "# No need to save the eq save a pickle file before\n",
    "program1 = \"X0*X1\"\n",
    "pde_expr1, variables1,  = build_exp(program1); print(pde_expr1, variables1)\n",
    "\n",
    "program2 = \"X2\"\n",
    "pde_expr2, variables2,  = build_exp(program2); print(pde_expr2, variables2)\n",
    "\n",
    "mod = ComplexSymPyModule(expressions=[pde_expr1, pde_expr2], complex_coeffs=cns); mod.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustComplexPINN(nn.Module):\n",
    "    def __init__(self, model, loss_fn, index2features, scale=False, lb=None, ub=None, init_cs=(-1.0, 0.0), init_betas=(0.0, 0.0)):\n",
    "        super(RobustComplexPINN, self).__init__()\n",
    "        # FFTNN\n",
    "        self.in_fft_nn = FFTNN(c=init_cs[0])\n",
    "        self.out_fft_nn = FFTNN(c=init_cs[1])\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        # Beta-Robust PCA\n",
    "        self.inp_rpca = RobustPCANN(beta=init_betas[0], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "        self.out_rpca = RobustPCANN(beta=init_betas[1], is_beta_trainable=True, inp_dims=2, hidden_dims=32)\n",
    "\n",
    "        self.callable_loss_fn = loss_fn\n",
    "        self.index2features = index2features; self.feature2index = {}\n",
    "        for idx, fn in enumerate(self.index2features): self.feature2index[fn] = str(idx)\n",
    "        self.scale = scale; self.lb, self.ub = lb, ub\n",
    "        if self.scale and (self.lb is None or self.ub is None):\n",
    "            print(\"Please provide thw lower and upper bounds of your PDE.\")\n",
    "            print(\"Otherwise, there will be error(s)\")\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        \n",
    "    def forward(self, H):\n",
    "        if self.scale: H = self.neural_net_scale(H)\n",
    "        return self.model(H)\n",
    "    \n",
    "    def loss(self, HL, HS, y_input, y_input_S, update_network_params=True, update_pde_params=True):\n",
    "        total_loss = []\n",
    "        \n",
    "        # Denoising FFT on (x, t)\n",
    "        HS = cat(torch.fft.ifft(self.in_fft_nn(HS[1])*HS[0]).real.reshape(-1, 1), \n",
    "                 torch.fft.ifft(self.in_fft_nn(HS[3])*HS[2]).real.reshape(-1, 1))\n",
    "        HS = HL-HS\n",
    "        \n",
    "        # Denoising FFT on y_input\n",
    "        y_input_S = y_input-torch.fft.ifft(self.out_fft_nn(y_input_S[1])*y_input_S[0]).reshape(-1, 1)\n",
    "        \n",
    "        H = self.inp_rpca(HL, HS, normalize=True)\n",
    "        \n",
    "        y_input = self.out_rpca(cat(y_input.real, y_input.imag), \n",
    "                                cat(y_input_S.real, y_input_S.imag), \n",
    "                                normalize=True)\n",
    "        y_input = torch.complex(y_input[:, 0:1], y_input[:, 1:2])\n",
    "        \n",
    "        grads_dict, u_t = self.grads_dict(H[:, 0:1], H[:, 1:2])\n",
    "        \n",
    "        # MSE Loss\n",
    "        if update_network_params:\n",
    "            total_loss.append(complex_mse(grads_dict['X'+self.feature2index['hf']], y_input))\n",
    "        # PDE Loss\n",
    "        if update_pde_params:\n",
    "            total_loss.append(complex_mse(self.callable_loss_fn(grads_dict), u_t))\n",
    "            \n",
    "        return total_loss\n",
    "    \n",
    "    def grads_dict(self, x, t):\n",
    "        uf = self.forward(cat(x, t))\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = {}\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives['X'+self.feature2index[t]] = cplx2tensor(uf)\n",
    "                derivatives['X1'] = (uf.real**2+uf.imag**2)+0.0j\n",
    "            elif t=='x': derivatives['X'+self.feature2index[t]] = x\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives['X'+self.feature2index['h_'+t[::-1]]] = out\n",
    "        \n",
    "        return derivatives, u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    # Must ensure that the implementation of neural_net_scale is consistent\n",
    "    # and hopefully correct\n",
    "    # also, you might not need this function in some datasets\n",
    "    def neural_net_scale(self, inp): \n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model\n",
    "semisup_model_state_dict = cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")\n",
    "parameters = OrderedDict()\n",
    "\n",
    "# Filter only the parts that I care about renaming (to be similar to what defined in TorchMLP).\n",
    "inner_part = \"network.model.\"\n",
    "for p in semisup_model_state_dict:\n",
    "    if inner_part in p:\n",
    "        parameters[p.replace(inner_part, \"\")] = semisup_model_state_dict[p]\n",
    "complex_model.load_state_dict(parameters)\n",
    "\n",
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, index2features=feature_names, scale=False, lb=lb, ub=ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S, x_fft, x_PSD, t_fft, t_PSD\n",
    "    if torch.is_grad_enabled():\n",
    "        optimizer2.zero_grad(set_to_none=True)\n",
    "    losses = pinn.loss(X_train, (x_fft, x_PSD, t_fft, t_PSD), h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    l = sum(losses)\n",
    "    if l.requires_grad: \n",
    "        l.backward(retain_graph=True)\n",
    "    return l\n",
    "\n",
    "def mtl_closure():\n",
    "    global X_train, X_train_S, h_train, h_train_S, x_fft, x_PSD, t_fft, t_PSD\n",
    "    n_obj = 2 # There are two tasks\n",
    "    losses = pinn.loss(X_train, (x_fft, x_PSD, t_fft, t_PSD), h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(n_obj):\n",
    "        optimizer1.zero_grad(set_to_none=True)\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())+list(pinn.model.parameters())+list(pinn.callable_loss_fn.parameters()):\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())+list(pinn.model.parameters())+list(pinn.callable_loss_fn.parameters())):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    return sum(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best thresold wrt to the first-epoch loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_x, x_fft, x_PSD = fft1d_denoise(X_train[:, 0:1], c=0, return_real=True)\n",
    "noise_x = X_train[:, 0:1]-noise_x\n",
    "noise_t, t_fft, t_PSD = fft1d_denoise(X_train[:, 1:2], c=0, return_real=True)\n",
    "noise_t = X_train[:, 1:2]-noise_t\n",
    "X_train_S = cat(noise_x, noise_t)\n",
    "\n",
    "h_train_S, h_train_fft, h_train_PSD = fft1d_denoise(h_train, c=-1, return_real=False)\n",
    "h_train_S = h_train-h_train_S\n",
    "\n",
    "del noise_x, noise_t\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    c1     |    c2     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.02416 \u001b[0m | \u001b[0m 0.4881  \u001b[0m | \u001b[0m 2.152   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.02233 \u001b[0m | \u001b[95m 1.028   \u001b[0m | \u001b[95m 0.4488  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.7635  \u001b[0m | \u001b[0m 1.459   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.6241  \u001b[0m | \u001b[0m 3.918   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-0.02195 \u001b[0m | \u001b[95m 4.637   \u001b[0m | \u001b[95m-1.166   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.02411 \u001b[0m | \u001b[0m 2.917   \u001b[0m | \u001b[0m 0.2889  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-0.02364 \u001b[0m | \u001b[0m 0.6804  \u001b[0m | \u001b[0m 4.256   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.29    \u001b[0m | \u001b[0m-4.129   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-4.798   \u001b[0m | \u001b[0m 3.326   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.02423 \u001b[0m | \u001b[0m 2.782   \u001b[0m | \u001b[0m 3.7     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.786   \u001b[0m | \u001b[0m 2.992   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.3852  \u001b[0m | \u001b[0m 2.805   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.817   \u001b[0m | \u001b[0m 1.399   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.566   \u001b[0m | \u001b[0m 4.447   \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m-0.02181 \u001b[0m | \u001b[95m 0.2185  \u001b[0m | \u001b[95m-0.8534  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-2.354   \u001b[0m | \u001b[0m 2.742   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-0.02541 \u001b[0m | \u001b[0m-0.4385  \u001b[0m | \u001b[0m 0.6843  \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-4.812   \u001b[0m | \u001b[0m 1.176   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.02309 \u001b[0m | \u001b[0m 1.121   \u001b[0m | \u001b[0m 1.169   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.437   \u001b[0m | \u001b[0m 1.818   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.405   \u001b[0m | \u001b[0m-0.6297  \u001b[0m |\n",
      "| \u001b[95m 22      \u001b[0m | \u001b[95m-0.02133 \u001b[0m | \u001b[95m 1.976   \u001b[0m | \u001b[95m-4.398   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-0.02297 \u001b[0m | \u001b[0m 1.668   \u001b[0m | \u001b[0m 1.706   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.896   \u001b[0m | \u001b[0m-3.711   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.846   \u001b[0m | \u001b[0m-1.363   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.02181 \u001b[0m | \u001b[0m 0.702   \u001b[0m | \u001b[0m-0.614   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.884   \u001b[0m | \u001b[0m-3.98    \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.911   \u001b[0m | \u001b[0m-3.387   \u001b[0m |\n",
      "| \u001b[95m 29      \u001b[0m | \u001b[95m-0.02046 \u001b[0m | \u001b[95m 1.531   \u001b[0m | \u001b[95m-2.467   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.3369  \u001b[0m | \u001b[0m-2.556   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.41    \u001b[0m | \u001b[0m-3.896   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-0.02055 \u001b[0m | \u001b[0m 1.563   \u001b[0m | \u001b[0m-3.618   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.034   \u001b[0m | \u001b[0m-1.313   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.21    \u001b[0m | \u001b[0m-4.029   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-0.02229 \u001b[0m | \u001b[0m 3.379   \u001b[0m | \u001b[0m-4.039   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.765   \u001b[0m | \u001b[0m-0.3135  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.768   \u001b[0m | \u001b[0m 1.048   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.393   \u001b[0m | \u001b[0m-4.608   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.172   \u001b[0m | \u001b[0m-3.798   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.039   \u001b[0m | \u001b[0m-3.813   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.82    \u001b[0m | \u001b[0m-0.8574  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-4.359   \u001b[0m | \u001b[0m 1.925   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-0.02127 \u001b[0m | \u001b[0m 0.666   \u001b[0m | \u001b[0m-2.346   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-0.02181 \u001b[0m | \u001b[0m 0.2325  \u001b[0m | \u001b[0m-4.061   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-0.02364 \u001b[0m | \u001b[0m 0.7595  \u001b[0m | \u001b[0m 4.293   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-1.814   \u001b[0m | \u001b[0m 1.674   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.682   \u001b[0m | \u001b[0m 2.163   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.106   \u001b[0m | \u001b[0m-3.168   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-0.02104 \u001b[0m | \u001b[0m 0.8651  \u001b[0m | \u001b[0m-4.799   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.289   \u001b[0m | \u001b[0m-4.953   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-0.02099 \u001b[0m | \u001b[0m 1.778   \u001b[0m | \u001b[0m-2.3     \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-0.02432 \u001b[0m | \u001b[0m 2.352   \u001b[0m | \u001b[0m 4.622   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-0.02541 \u001b[0m | \u001b[0m-2.512   \u001b[0m | \u001b[0m 0.7616  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-0.02247 \u001b[0m | \u001b[0m 0.9204  \u001b[0m | \u001b[0m 0.7225  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-2.769   \u001b[0m | \u001b[0m 4.527   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.5287  \u001b[0m | \u001b[0m 3.464   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-0.02133 \u001b[0m | \u001b[0m 1.995   \u001b[0m | \u001b[0m-2.026   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.138   \u001b[0m | \u001b[0m-1.035   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-0.0246  \u001b[0m | \u001b[0m 3.811   \u001b[0m | \u001b[0m 0.8127  \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-0.02457 \u001b[0m | \u001b[0m 3.817   \u001b[0m | \u001b[0m 1.925   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m-0.02348 \u001b[0m | \u001b[0m 2.253   \u001b[0m | \u001b[0m 0.01324 \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.561   \u001b[0m | \u001b[0m 1.44    \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.7614  \u001b[0m | \u001b[0m 1.064   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.808   \u001b[0m | \u001b[0m-1.984   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m-0.02051 \u001b[0m | \u001b[0m 1.602   \u001b[0m | \u001b[0m-2.099   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.18    \u001b[0m | \u001b[0m-0.7123  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.645   \u001b[0m | \u001b[0m-2.017   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m-0.02383 \u001b[0m | \u001b[0m 0.6996  \u001b[0m | \u001b[0m 0.9087  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m-0.0238  \u001b[0m | \u001b[0m 0.7433  \u001b[0m | \u001b[0m 1.532   \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m-0.02046 \u001b[0m | \u001b[0m 1.521   \u001b[0m | \u001b[0m-0.6858  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 3.965   \u001b[0m | \u001b[0m-1.324   \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-0.6414  \u001b[0m | \u001b[0m 3.919   \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m-0.02445 \u001b[0m | \u001b[0m 3.062   \u001b[0m | \u001b[0m 2.039   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.998   \u001b[0m | \u001b[0m 4.195   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m-0.02359 \u001b[0m | \u001b[0m 2.142   \u001b[0m | \u001b[0m 4.988   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.506   \u001b[0m | \u001b[0m 3.681   \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.375   \u001b[0m | \u001b[0m 1.156   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.762   \u001b[0m | \u001b[0m 3.48    \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m-0.02447 \u001b[0m | \u001b[0m 3.073   \u001b[0m | \u001b[0m 0.691   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.9282  \u001b[0m | \u001b[0m-4.308   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m-0.02133 \u001b[0m | \u001b[0m 1.974   \u001b[0m | \u001b[0m-0.4646  \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m-0.02432 \u001b[0m | \u001b[0m 2.221   \u001b[0m | \u001b[0m 3.664   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.755   \u001b[0m | \u001b[0m 3.558   \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.883   \u001b[0m | \u001b[0m-1.4     \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.3     \u001b[0m | \u001b[0m-3.284   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m-0.02178 \u001b[0m | \u001b[0m 0.2104  \u001b[0m | \u001b[0m-4.457   \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.0     \u001b[0m | \u001b[0m-4.815   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 2.937   \u001b[0m | \u001b[0m-2.761   \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-1.546   \u001b[0m | \u001b[0m 4.281   \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m-0.02133 \u001b[0m | \u001b[0m 2.044   \u001b[0m | \u001b[0m-4.682   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.353   \u001b[0m | \u001b[0m 1.215   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m-0.02161 \u001b[0m | \u001b[0m 0.7723  \u001b[0m | \u001b[0m-2.621   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.342   \u001b[0m | \u001b[0m 1.14    \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m-0.02406 \u001b[0m | \u001b[0m 0.3563  \u001b[0m | \u001b[0m 0.8991  \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.301   \u001b[0m | \u001b[0m-1.881   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.018   \u001b[0m | \u001b[0m-2.902   \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-3.138   \u001b[0m | \u001b[0m 4.444   \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m-0.02275 \u001b[0m | \u001b[0m 2.396   \u001b[0m | \u001b[0m-0.09541 \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.726   \u001b[0m | \u001b[0m-2.456   \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.42    \u001b[0m | \u001b[0m-0.6558  \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.198   \u001b[0m | \u001b[0m-1.401   \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.626   \u001b[0m | \u001b[0m-2.684   \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.155   \u001b[0m | \u001b[0m-4.122   \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m-0.02098 \u001b[0m | \u001b[0m 1.832   \u001b[0m | \u001b[0m-1.135   \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.822   \u001b[0m | \u001b[0m-4.987   \u001b[0m |\n",
      "| \u001b[95m 106     \u001b[0m | \u001b[95m-0.02023 \u001b[0m | \u001b[95m 1.468   \u001b[0m | \u001b[95m-5.0     \u001b[0m |\n",
      "| \u001b[0m 107     \u001b[0m | \u001b[0m-0.02176 \u001b[0m | \u001b[0m 0.4554  \u001b[0m | \u001b[0m-1.6     \u001b[0m |\n",
      "| \u001b[0m 108     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.2     \u001b[0m | \u001b[0m-2.125   \u001b[0m |\n",
      "| \u001b[0m 109     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-5.0     \u001b[0m |\n",
      "| \u001b[0m 110     \u001b[0m | \u001b[0m-0.02037 \u001b[0m | \u001b[0m 1.406   \u001b[0m | \u001b[0m-4.609   \u001b[0m |\n",
      "| \u001b[0m 111     \u001b[0m | \u001b[0m-0.02023 \u001b[0m | \u001b[0m 1.476   \u001b[0m | \u001b[0m-3.058   \u001b[0m |\n",
      "| \u001b[0m 112     \u001b[0m | \u001b[0m-0.02061 \u001b[0m | \u001b[0m 1.243   \u001b[0m | \u001b[0m-3.436   \u001b[0m |\n",
      "| \u001b[0m 113     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-3.274   \u001b[0m |\n",
      "| \u001b[0m 114     \u001b[0m | \u001b[0m-0.0221  \u001b[0m | \u001b[0m 3.821   \u001b[0m | \u001b[0m-2.394   \u001b[0m |\n",
      "| \u001b[0m 115     \u001b[0m | \u001b[0m-0.02452 \u001b[0m | \u001b[0m 4.964   \u001b[0m | \u001b[0m 4.973   \u001b[0m |\n",
      "| \u001b[0m 116     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.399   \u001b[0m | \u001b[0m-1.011   \u001b[0m |\n",
      "| \u001b[0m 117     \u001b[0m | \u001b[0m-0.02023 \u001b[0m | \u001b[0m 1.513   \u001b[0m | \u001b[0m-1.684   \u001b[0m |\n",
      "| \u001b[0m 118     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.6332  \u001b[0m | \u001b[0m-1.404   \u001b[0m |\n",
      "| \u001b[0m 119     \u001b[0m | \u001b[0m-0.02077 \u001b[0m | \u001b[0m 1.697   \u001b[0m | \u001b[0m-5.0     \u001b[0m |\n",
      "| \u001b[0m 120     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.211   \u001b[0m | \u001b[0m-4.967   \u001b[0m |\n",
      "| \u001b[0m 121     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.765   \u001b[0m | \u001b[0m-5.0     \u001b[0m |\n",
      "| \u001b[0m 122     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m-1.975   \u001b[0m |\n",
      "| \u001b[0m 123     \u001b[0m | \u001b[0m-0.02085 \u001b[0m | \u001b[0m 1.653   \u001b[0m | \u001b[0m-2.897   \u001b[0m |\n",
      "| \u001b[0m 124     \u001b[0m | \u001b[0m-0.02049 \u001b[0m | \u001b[0m 1.304   \u001b[0m | \u001b[0m-2.851   \u001b[0m |\n",
      "| \u001b[0m 125     \u001b[0m | \u001b[0m-0.02046 \u001b[0m | \u001b[0m 1.517   \u001b[0m | \u001b[0m-3.297   \u001b[0m |\n",
      "| \u001b[0m 126     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.13    \u001b[0m | \u001b[0m-4.989   \u001b[0m |\n",
      "| \u001b[0m 127     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 3.91    \u001b[0m | \u001b[0m-3.247   \u001b[0m |\n",
      "| \u001b[0m 128     \u001b[0m | \u001b[0m-0.02055 \u001b[0m | \u001b[0m 1.547   \u001b[0m | \u001b[0m-4.143   \u001b[0m |\n",
      "| \u001b[0m 129     \u001b[0m | \u001b[0m-0.02023 \u001b[0m | \u001b[0m 1.477   \u001b[0m | \u001b[0m-4.837   \u001b[0m |\n",
      "| \u001b[0m 130     \u001b[0m | \u001b[0m-0.02046 \u001b[0m | \u001b[0m 1.519   \u001b[0m | \u001b[0m-1.438   \u001b[0m |\n",
      "| \u001b[0m 131     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.39    \u001b[0m | \u001b[0m-1.824   \u001b[0m |\n",
      "| \u001b[0m 132     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.022   \u001b[0m | \u001b[0m-0.5374  \u001b[0m |\n",
      "| \u001b[0m 133     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.367   \u001b[0m | \u001b[0m-3.119   \u001b[0m |\n",
      "| \u001b[0m 134     \u001b[0m | \u001b[0m-0.02542 \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 135     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m-3.238   \u001b[0m |\n",
      "| \u001b[0m 136     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.896   \u001b[0m | \u001b[0m-4.999   \u001b[0m |\n",
      "| \u001b[0m 137     \u001b[0m | \u001b[0m-0.02049 \u001b[0m | \u001b[0m 1.286   \u001b[0m | \u001b[0m-3.802   \u001b[0m |\n",
      "| \u001b[0m 138     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.4038  \u001b[0m | \u001b[0m-4.993   \u001b[0m |\n",
      "| \u001b[0m 139     \u001b[0m | \u001b[0m-0.02085 \u001b[0m | \u001b[0m 1.658   \u001b[0m | \u001b[0m-1.713   \u001b[0m |\n",
      "| \u001b[0m 140     \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.471   \u001b[0m | \u001b[0m-0.7352  \u001b[0m |\n",
      "| \u001b[0m 141     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.936   \u001b[0m | \u001b[0m-2.982   \u001b[0m |\n",
      "| \u001b[0m 142     \u001b[0m | \u001b[0m-0.02457 \u001b[0m | \u001b[0m 3.581   \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[0m 143     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.38    \u001b[0m | \u001b[0m-1.607   \u001b[0m |\n",
      "| \u001b[0m 144     \u001b[0m | \u001b[0m-0.0205  \u001b[0m | \u001b[0m 0.897   \u001b[0m | \u001b[0m-1.087   \u001b[0m |\n",
      "| \u001b[0m 145     \u001b[0m | \u001b[0m-0.02161 \u001b[0m | \u001b[0m 0.8057  \u001b[0m | \u001b[0m-3.808   \u001b[0m |\n",
      "| \u001b[0m 146     \u001b[0m | \u001b[0m-0.0245  \u001b[0m | \u001b[0m 0.1635  \u001b[0m | \u001b[0m 5.0     \u001b[0m |\n",
      "| \u001b[95m 147     \u001b[0m | \u001b[95m-0.02021 \u001b[0m | \u001b[95m 1.315   \u001b[0m | \u001b[95m-2.467   \u001b[0m |\n",
      "| \u001b[0m 148     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.3411  \u001b[0m | \u001b[0m-3.551   \u001b[0m |\n",
      "| \u001b[0m 149     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.296   \u001b[0m | \u001b[0m-4.283   \u001b[0m |\n",
      "| \u001b[0m 150     \u001b[0m | \u001b[0m-0.02278 \u001b[0m | \u001b[0m-5.0     \u001b[0m | \u001b[0m-0.2096  \u001b[0m |\n",
      "| \u001b[0m 151     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.381   \u001b[0m | \u001b[0m-2.27    \u001b[0m |\n",
      "| \u001b[0m 152     \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.309   \u001b[0m | \u001b[0m-1.857   \u001b[0m |\n",
      "| \u001b[0m 153     \u001b[0m | \u001b[0m-0.02051 \u001b[0m | \u001b[0m 1.63    \u001b[0m | \u001b[0m-0.8512  \u001b[0m |\n",
      "| \u001b[0m 154     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.144   \u001b[0m | \u001b[0m-1.102   \u001b[0m |\n",
      "| \u001b[0m 155     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-1.587   \u001b[0m | \u001b[0m-2.241   \u001b[0m |\n",
      "| \u001b[0m 156     \u001b[0m | \u001b[0m-0.02275 \u001b[0m | \u001b[0m 1.446   \u001b[0m | \u001b[0m 2.759   \u001b[0m |\n",
      "| \u001b[0m 157     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.429   \u001b[0m | \u001b[0m-0.4624  \u001b[0m |\n",
      "| \u001b[0m 158     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.142   \u001b[0m | \u001b[0m-4.531   \u001b[0m |\n",
      "| \u001b[0m 159     \u001b[0m | \u001b[0m-0.02131 \u001b[0m | \u001b[0m 0.6301  \u001b[0m | \u001b[0m-1.152   \u001b[0m |\n",
      "| \u001b[0m 160     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-0.429   \u001b[0m | \u001b[0m-0.5926  \u001b[0m |\n",
      "| \u001b[0m 161     \u001b[0m | \u001b[0m-0.02051 \u001b[0m | \u001b[0m 1.613   \u001b[0m | \u001b[0m-4.621   \u001b[0m |\n",
      "| \u001b[0m 162     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.185   \u001b[0m | \u001b[0m-2.41    \u001b[0m |\n",
      "| \u001b[0m 163     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.383   \u001b[0m | \u001b[0m-2.624   \u001b[0m |\n",
      "| \u001b[0m 164     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.401   \u001b[0m | \u001b[0m-1.94    \u001b[0m |\n",
      "| \u001b[0m 165     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.515   \u001b[0m | \u001b[0m-3.494   \u001b[0m |\n",
      "| \u001b[0m 166     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.993   \u001b[0m | \u001b[0m-4.156   \u001b[0m |\n",
      "| \u001b[0m 167     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.996   \u001b[0m | \u001b[0m-0.824   \u001b[0m |\n",
      "| \u001b[0m 168     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.377   \u001b[0m | \u001b[0m-4.293   \u001b[0m |\n",
      "| \u001b[0m 169     \u001b[0m | \u001b[0m-0.02177 \u001b[0m | \u001b[0m 0.3802  \u001b[0m | \u001b[0m-4.982   \u001b[0m |\n",
      "| \u001b[0m 170     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-3.909   \u001b[0m | \u001b[0m-1.255   \u001b[0m |\n",
      "| \u001b[0m 171     \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.182   \u001b[0m | \u001b[0m-3.405   \u001b[0m |\n",
      "| \u001b[0m 172     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.404   \u001b[0m | \u001b[0m-2.447   \u001b[0m |\n",
      "| \u001b[0m 173     \u001b[0m | \u001b[0m-0.02194 \u001b[0m | \u001b[0m 0.1921  \u001b[0m | \u001b[0m-2.152   \u001b[0m |\n",
      "| \u001b[0m 174     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.582   \u001b[0m | \u001b[0m-0.4914  \u001b[0m |\n",
      "| \u001b[0m 175     \u001b[0m | \u001b[0m-0.02037 \u001b[0m | \u001b[0m 1.438   \u001b[0m | \u001b[0m-0.3201  \u001b[0m |\n",
      "| \u001b[0m 176     \u001b[0m | \u001b[0m-0.02062 \u001b[0m | \u001b[0m 1.206   \u001b[0m | \u001b[0m-0.3784  \u001b[0m |\n",
      "| \u001b[0m 177     \u001b[0m | \u001b[0m-0.02218 \u001b[0m | \u001b[0m 3.252   \u001b[0m | \u001b[0m-0.4519  \u001b[0m |\n",
      "| \u001b[0m 178     \u001b[0m | \u001b[0m-0.02255 \u001b[0m | \u001b[0m 0.3113  \u001b[0m | \u001b[0m-3.124   \u001b[0m |\n",
      "| \u001b[0m 179     \u001b[0m | \u001b[0m-0.02249 \u001b[0m | \u001b[0m 0.9156  \u001b[0m | \u001b[0m 3.394   \u001b[0m |\n",
      "| \u001b[0m 180     \u001b[0m | \u001b[0m-0.02133 \u001b[0m | \u001b[0m 1.994   \u001b[0m | \u001b[0m-3.799   \u001b[0m |\n",
      "| \u001b[0m 181     \u001b[0m | \u001b[0m-0.02161 \u001b[0m | \u001b[0m 0.764   \u001b[0m | \u001b[0m-4.417   \u001b[0m |\n",
      "| \u001b[0m 182     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.965   \u001b[0m | \u001b[0m-4.495   \u001b[0m |\n",
      "| \u001b[0m 183     \u001b[0m | \u001b[0m-0.0205  \u001b[0m | \u001b[0m 0.9011  \u001b[0m | \u001b[0m-1.767   \u001b[0m |\n",
      "| \u001b[0m 184     \u001b[0m | \u001b[0m-0.0205  \u001b[0m | \u001b[0m 0.9201  \u001b[0m | \u001b[0m-1.495   \u001b[0m |\n",
      "| \u001b[0m 185     \u001b[0m | \u001b[0m-0.02046 \u001b[0m | \u001b[0m 1.525   \u001b[0m | \u001b[0m-1.137   \u001b[0m |\n",
      "| \u001b[0m 186     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.395   \u001b[0m | \u001b[0m-0.4582  \u001b[0m |\n",
      "| \u001b[0m 187     \u001b[0m | \u001b[0m-0.02065 \u001b[0m | \u001b[0m 1.263   \u001b[0m | \u001b[0m-2.623   \u001b[0m |\n",
      "| \u001b[0m 188     \u001b[0m | \u001b[0m-0.02432 \u001b[0m | \u001b[0m 2.207   \u001b[0m | \u001b[0m 2.546   \u001b[0m |\n",
      "| \u001b[0m 189     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-4.99    \u001b[0m | \u001b[0m-2.606   \u001b[0m |\n",
      "| \u001b[0m 190     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.311   \u001b[0m | \u001b[0m-4.521   \u001b[0m |\n",
      "| \u001b[0m 191     \u001b[0m | \u001b[0m-0.02458 \u001b[0m | \u001b[0m-4.135   \u001b[0m | \u001b[0m 0.06656 \u001b[0m |\n",
      "| \u001b[0m 192     \u001b[0m | \u001b[0m-0.02395 \u001b[0m | \u001b[0m 4.998   \u001b[0m | \u001b[0m 0.1371  \u001b[0m |\n",
      "| \u001b[0m 193     \u001b[0m | \u001b[0m-0.02274 \u001b[0m | \u001b[0m-2.408   \u001b[0m | \u001b[0m-1.816   \u001b[0m |\n",
      "| \u001b[0m 194     \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.562   \u001b[0m | \u001b[0m-3.85    \u001b[0m |\n",
      "| \u001b[0m 195     \u001b[0m | \u001b[0m-0.02037 \u001b[0m | \u001b[0m 1.418   \u001b[0m | \u001b[0m-2.905   \u001b[0m |\n",
      "| \u001b[0m 196     \u001b[0m | \u001b[0m-0.02213 \u001b[0m | \u001b[0m 2.365   \u001b[0m | \u001b[0m-2.435   \u001b[0m |\n",
      "| \u001b[0m 197     \u001b[0m | \u001b[0m-0.02195 \u001b[0m | \u001b[0m 4.492   \u001b[0m | \u001b[0m-0.6727  \u001b[0m |\n",
      "| \u001b[0m 198     \u001b[0m | \u001b[0m-0.02038 \u001b[0m | \u001b[0m 1.363   \u001b[0m | \u001b[0m-4.056   \u001b[0m |\n",
      "| \u001b[0m 199     \u001b[0m | \u001b[0m-0.02457 \u001b[0m | \u001b[0m 3.669   \u001b[0m | \u001b[0m 3.224   \u001b[0m |\n",
      "| \u001b[0m 200     \u001b[0m | \u001b[0m-0.02127 \u001b[0m | \u001b[0m 0.6765  \u001b[0m | \u001b[0m-1.916   \u001b[0m |\n",
      "=================================================\n",
      "{'target': -0.02020838111639023, 'params': {'c1': 1.315229317593679, 'c2': -2.4666411184083548}}\n"
     ]
    }
   ],
   "source": [
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, \n",
    "                         index2features=feature_names, scale=False, lb=lb, ub=ub, \n",
    "                         init_cs=(0.0, -1.0), init_betas=(1.0, 1.0))\n",
    "pinn.eval()\n",
    "\n",
    "pbounds = {'c1': (-5, 5), 'c2': (-5, 5)}\n",
    "\n",
    "def inference(c1, c2):\n",
    "    global pinn\n",
    "    pinn.in_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c1)]), requires_grad=False)\n",
    "    pinn.out_fft_nn.c = nn.Parameter(data=torch.FloatTensor([float(c2)]), requires_grad=False)\n",
    "    losses = pinn.loss(X_train, (x_fft, x_PSD, t_fft, t_PSD), h_train, (h_train_fft, h_train_PSD), update_network_params=True, update_pde_params=True)\n",
    "    return -sum(losses).item()\n",
    "\n",
    "bayes_opt = BayesianOptimization(f=inference, pbounds=pbounds, random_state=0)\n",
    "bayes_opt.maximize(init_points=100, n_iter=100)\n",
    "print(bayes_opt.max)\n",
    "\n",
    "if 'pinn' in globals(): del pinn\n",
    "# plt.plot(c_range, performances)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.315229317593679, -2.4666411184083548)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcs = tuple(bayes_opt.max[\"params\"].values()); bcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performances = npar(performances)\n",
    "# indices = np.where(performances == performances.min())[0]\n",
    "# if c_range[indices.min()] > 0.0: \n",
    "#     bcs = (min(c_range[indices.min()], c_range[indices.max()]), -1.0)\n",
    "# else: \n",
    "#     bcs = (max(c_range[indices.min()], c_range[indices.max()]), -1.0)\n",
    "# print(\"The best c =\", bcs[0])\n",
    "# del c_range, performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Phase optimization using Adam with PCGrad gradient modification\n",
      "Epoch 0:  0.013774620369076729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../lookahead.py:38: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)\n",
      "  slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  0.010370826348662376\n",
      "Epoch 20:  0.009517624974250793\n",
      "Epoch 30:  0.008737301453948021\n",
      "Epoch 40:  0.008249996230006218\n",
      "Epoch 50:  0.007855195552110672\n",
      "Epoch 60:  0.008324065245687962\n",
      "Epoch 70:  0.007572786882519722\n",
      "Epoch 80:  0.0075779566541314125\n",
      "Epoch 90:  0.007142988964915276\n",
      "Epoch 100:  0.0068914638832211494\n",
      "Epoch 110:  0.006760624703019857\n",
      "Epoch 120:  0.00667624082416296\n",
      "Epoch 130:  0.007410834543406963\n",
      "Epoch 140:  0.0075071947649121284\n",
      "Epoch 150:  0.0072290352545678616\n",
      "Epoch 160:  0.006754206493496895\n",
      "Epoch 170:  0.0065666730515658855\n",
      "Epoch 180:  0.006475353613495827\n",
      "Epoch 190:  0.006402359344065189\n",
      "Epoch 199:  0.006335781421512365\n"
     ]
    }
   ],
   "source": [
    "pinn = RobustComplexPINN(model=complex_model, loss_fn=mod, \n",
    "                         index2features=feature_names, scale=False, lb=lb, ub=ub, \n",
    "                         init_cs=bcs, init_betas=(0.0, 0.0))\n",
    "\n",
    "epochs1, epochs2 = 200, 10\n",
    "optimizer1 = MADGRAD(list(pinn.inp_rpca.parameters())+list(pinn.out_rpca.parameters())+list(pinn.model.parameters())+list(pinn.callable_loss_fn.parameters()), lr=1e-7, momentum=0.9)\n",
    "in_fft_opt = lookahead.LookaheadAdam(pinn.in_fft_nn.parameters(), lr=1.0)\n",
    "out_fft_opt = lookahead.LookaheadAdam(pinn.out_fft_nn.parameters(), lr=1.0)\n",
    "\n",
    "pinn.train(); best_train_loss = 1e6\n",
    "print('1st Phase optimization using Adam with PCGrad gradient modification')\n",
    "for i in range(epochs1):\n",
    "    optimizer1.step(mtl_closure)\n",
    "    in_fft_opt.step(); in_fft_opt.zero_grad()\n",
    "    out_fft_opt.step(); out_fft_opt.zero_grad()\n",
    "    if (i % 10) == 0 or i == epochs1-1:\n",
    "        l = mtl_closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Phase optimization using LBFGS\n",
      "Epoch 0:  0.003190835937857628\n",
      "[0.00378109+1.0029296j  0.00533206+0.50625813j]\n",
      "Epoch 5:  0.002527982695028186\n",
      "[0.0117909 +1.0024627j 0.01076918+0.504511j ]\n",
      "Epoch 9:  0.002150128362700343\n",
      "[0.0139333 +0.9997645j  0.01753757+0.49868146j]\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = torch.optim.LBFGS(pinn.parameters(), lr=1e-1, max_iter=500, max_eval=int(500*1.25), history_size=150, line_search_fn='strong_wolfe')\n",
    "print('2nd Phase optimization using LBFGS')\n",
    "for i in range(epochs2):\n",
    "    optimizer2.step(closure)\n",
    "    if (i % 5) == 0 or i == epochs2-1:\n",
    "        l = closure()\n",
    "        print(\"Epoch {}: \".format(i), l.item())\n",
    "        print(pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.315274715423584, -2.4666411876678467)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.in_fft_nn.c.item(), pinn.out_fft_nn.c.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading weights for testing\n",
    "# pinn = load_weights(pinn, \"./saved_path_inverse_nls/final_finetuned_uncert_cpinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0139333 +0.9997645j , 0.01753757+0.49868146j], dtype=complex64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "est_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1436293125152588, 0.12007951736450195)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_coeffs = pinn.callable_loss_fn.complex_coeffs().detach().numpy().ravel()\n",
    "grounds = np.array([1j, 0+0.5j])\n",
    "\n",
    "errs = []\n",
    "for i in range(len(grounds)):\n",
    "    err = est_coeffs[i]-grounds[i]\n",
    "    errs.append(100*abs(err.imag)/abs(grounds[i].imag))\n",
    "errs = np.array(errs)\n",
    "errs.mean(), errs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(pinn, \"./saved_path_inverse_nls/noisy2_final_finetuned_doublebetarpca_fftcpinn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.2402], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1080], requires_grad=True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinn.inp_rpca.beta, pinn.out_rpca.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy Exact & Clean (x, t)\n",
    "# (0.05885958671569824, 0.021964311599731445)\n",
    "# array([-0.00046226+0.99919176j, -0.00056662+0.49981552j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# (0.6996273994445801, 0.01595020294189453)\n",
    "# array([0.00149273+0.9928442j, 0.00079829+0.5034184j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7112264633178711, 0.00553131103515625)\n",
    "# array([ 3.449592e-03+1.007057j , -7.125967e-05+0.5035838j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star-X_star_S\n",
    "# (0.7093071937561035, 0.0036716461181640625)\n",
    "# array([ 3.4442921e-03+1.0070564j, -5.4004795e-05+0.5035649j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Clean (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.1215517520904541, 0.08192658424377441)\n",
    "# array([-8.2360100e-05+0.99960375j, -6.1671366e-05+0.5010174j], dtype=complex64)\n",
    "# Noisy Exact & Noisy (x, t) & X_star = X_star_L+1*X_star_S\n",
    "# (0.511014461517334, 0.25589466094970703)\n",
    "# array([-0.01472272+1.0076691j, -0.02164156+0.5012756j], dtype=complex64)\n",
    "\n",
    "# Noisy Exact & Noisy (x, t) & X_train = X_train_L+1*1*X_train_S+beta*NN(X_train_S)\n",
    "# (0.5050361156463623, 0.1848280429840088)\n",
    "# array([ 0.00107117+1.0032021j, -0.01103256+0.5034493j], dtype=complex64)\n",
    "# beta = 0.005178438033908606\n",
    "\n",
    "# Notes\n",
    "# X_star = X_star-X_star_S -> Seems robust but not stable\n",
    "# X_star = X_star_L+X_star_S -> The best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New results on Double Beta-RobustFFT ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([-4.01791149e-05+0.9997733j, 1.09734545e-04+0.5006671j], dtype=complex64)\n",
    "# (0.07804334163665771, 0.05537569522857666)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([0.0085], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0027], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([0.0139333 +0.9997645j , 0.01753757+0.49868146j], dtype=complex64)\n",
    "# (0.1436293125152588, 0.12007951736450195)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([0.2402], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.1080], requires_grad=True))\n",
    "\n",
    "### Results on Double Beta-RobustPCA ###\n",
    "# Noisy Exact & Clean (x, t)\n",
    "# array([0.00077563+1.0028679j, 0.00166233+0.50137794j], dtype=complex64)\n",
    "# (0.2811908721923828, 0.005602836608886719)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([0.0002], requires_grad=True))\n",
    "\n",
    "# Noisy Exact & Noisy (x, t)\n",
    "# array([-0.00045199+1.0037338j, 0.00022461+0.5013247j], dtype=complex64)\n",
    "# (0.31915903091430664, 0.05421638488769531)\n",
    "# (pinn.inp_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0011], requires_grad=True),\n",
    "#  pinn.out_rpca.beta Parameter containing:\n",
    "#  tensor([-0.0002], requires_grad=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
