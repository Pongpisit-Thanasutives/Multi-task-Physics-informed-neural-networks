{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, ComplexTorchMLP, complex_mse\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Training with 500 unsup samples\n",
      "Loading pre-calculated data for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500; include_N_res = 1\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res>0:\n",
    "    N_res = int(N*include_N_res)\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = idx_res[:N_res]\n",
    "    X_res = to_tensor(X_star[idx_res, :], True)\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_train = torch.vstack([X_train, X_res])\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n",
    "\n",
    "### Loading data code here ###\n",
    "print(\"Loading pre-calculated data for reproducibility\")\n",
    "X_train = to_tensor(np.load(\"./tmp_files/X_train_500+500samples.npy\"), True)\n",
    "u_train, v_train = dimension_slicing(to_tensor(np.load(\"./tmp_files/uv_train_500samples.npy\"), False))\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_dim = x.shape[0]\n",
    "time_dim = t.shape[0]\n",
    "\n",
    "dt = (t[1]-t[0])[0]\n",
    "dx = (x[2]-x[1])[0]\n",
    "\n",
    "fd_h_t = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_x = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xxx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "\n",
    "for i in range(spatial_dim):\n",
    "    fd_h_t[i,:] = FiniteDiff(Exact[i,:], dt, 1)\n",
    "for i in range(time_dim):\n",
    "    fd_h_x[:,i] = FiniteDiff(Exact[:,i], dx, 1)\n",
    "    fd_h_xx[:,i] = FiniteDiff(Exact[:,i], dx, 2)\n",
    "    fd_h_xxx[:,i] = FiniteDiff(Exact[:,i], dx, 3)\n",
    "    \n",
    "fd_h_t = np.reshape(fd_h_t, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_x = np.reshape(fd_h_x, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xx = np.reshape(fd_h_xx, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xxx = np.reshape(fd_h_xxx, (spatial_dim*time_dim,1), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )\n",
    "\n",
    "# complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_cpinn_model.pth\"))\n",
    "complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals\n",
    "(1) Re-implement the semisup_model for a complex network.\n",
    "\n",
    "(2) Implement the self.gradients function.\n",
    "- complex_model(input) -> diff(u_pred, x) & diff(v_pred, x) -> combine 2 diff terms as 1 complex vector -> compute PDE loss / passing to the selector network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:124: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train[:N, :], True))\n",
    "predictions = complex_model(cat(xx, tt))\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.00830211490392685\n",
      "MSE Loss 3.642554656835273e-05\n"
     ]
    }
   ],
   "source": [
    "# PDE Loss 1.1325556442898232e-05\n",
    "# MSE Loss 4.512887699092971e-06\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions, u_train+1j*v_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00+0.0000000e+00j,  1.8287402e-01+9.5849973e-01j,\n",
       "         9.5216465e-01+0.0000000e+00j, ...,\n",
       "         1.7851934e+01-4.6664425e+01j,  3.1865616e+01-9.3093842e+01j,\n",
       "         5.6193420e+01-1.8545619e+02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.7636567e-01+1.4373595e-01j,\n",
       "         4.7813055e-01+0.0000000e+00j, ...,\n",
       "        -2.3702056e+00+5.3106542e+00j,  6.6533599e+00-1.3724621e+01j,\n",
       "        -1.8578472e+01+3.5425545e+01j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.0341483e-01+2.3708993e-01j,\n",
       "         4.2032108e-01+0.0000000e+00j, ...,\n",
       "        -4.2094916e-01+4.9473611e-01j, -8.7446731e-01+1.8341792e+00j,\n",
       "        -1.1678257e+00+6.2480083e+00j],\n",
       "       ...,\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  3.6971271e-02+5.2270472e-02j,\n",
       "         4.0990775e-03+0.0000000e+00j, ...,\n",
       "         3.8725277e-04+4.5041209e-03j, -4.1001476e-07+3.6970994e-03j,\n",
       "        -2.5966717e-04+3.0123498e-03j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  1.5098518e-01+1.0398209e-01j,\n",
       "         3.3608802e-02+0.0000000e+00j, ...,\n",
       "         1.6517833e-02+3.3025961e-02j, -1.2393138e-02-2.4222329e-02j,\n",
       "         9.2946738e-03+1.7763579e-02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  5.4721653e-02+4.6720326e-02j,\n",
       "         5.1772478e-03+0.0000000e+00j, ...,\n",
       "         4.0766774e-03+6.3557024e-03j, -2.9551531e-03-5.7514533e-03j,\n",
       "         2.0485490e-03+5.1446026e-03j]], dtype=complex64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "complex_poly_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.008224 +0.498694i)h_xx\n",
      "    + (-0.006229 +0.997566i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, 1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation w/ and w/o Finite difference guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives.append(cplx2tensor(uf))\n",
    "                derivatives.append((uf.real**2+uf.imag**2)+0.0j)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=-1), u_t\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex_network = ComplexNetwork(model=complex_model, index2features=feature_names, scale=True, lb=lb, ub=ub)\n",
    "# X_selector, y_selector = complex_network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexAttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.01):\n",
    "        super(ComplexAttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = CplxLinear(layers[0], layers[0], bias=True)\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = ComplexTorchMLP(dimensions=layers, activation_function=CplxToCplx[F.relu](), bn=bn, dropout_rate=0.0)\n",
    "        self.latest_weighted_features = None\n",
    "#         self.th = 0.1\n",
    "        self.th = 1/layers[0]\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        feature_importances = self.weighted_features(inn)\n",
    "        inn = inn*feature_importances\n",
    "        return self.nonlinear_model(inn)\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n",
    "        self.latest_weighted_features = self.latest_weighted_features.mean(dim=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = complex_mse(ut_approx, y_input)\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        return mse_loss+self.reg_intensity*(torch.norm(reg_term, p=0)+(torch.tensor([1.0, 1.0, 1.0, 2.0, 3.0])*reg_term).sum())\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None, uncert=False):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.weights = None\n",
    "        if uncert: \n",
    "            self.weights = torch.tensor([0.0, 0.0])\n",
    "        \n",
    "    def forward(self, X_h_train, h_train, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_h_train))\n",
    "        \n",
    "        h_row = h_train.shape[0]\n",
    "        fd_guidance = complex_mse(self.network.uf[:h_row, :], h_train)\n",
    "        \n",
    "        # I am not sure a good way to normalize/scale a complex tensor\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "            \n",
    "        if include_unsup and self.weights is not None:\n",
    "            return (torch.exp(-self.weights[0])*fd_guidance)+self.weights[0], (torch.exp(-self.weights[1])*unsup_loss)+self.weights[1]\n",
    "        else:\n",
    "            return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "h_star = (u_star+1j*v_star)\n",
    "\n",
    "fd_derivatives = np.hstack([h_star, h_star.real**2+h_star.imag**2, fd_h_x, fd_h_xx, fd_h_xxx])\n",
    "\n",
    "semisup_model = SemiSupModel(\n",
    "    network=ComplexNetwork(model=complex_model, index2features=feature_names, scale=False, lb=lb, ub=ub),\n",
    "    selector=ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=F.softmax, bn=True),\n",
    "    normalize_derivative_features=False,\n",
    "    mini=torch.tensor(np.abs(fd_derivatives).min(axis=0), dtype=torch.cfloat),\n",
    "    maxi=torch.tensor(np.abs(fd_derivatives).max(axis=0), dtype=torch.cfloat),\n",
    "    uncert=True,\n",
    ")\n",
    "\n",
    "del h_star, fd_derivatives, fd_h_x, fd_h_xx, fd_h_xxx\n",
    "\n",
    "# semisup_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_UNCERT = True\n",
    "def pcgrad_closure(return_list=False):\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_train, u_train+1j*v_train, include_unsup=True)      \n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-11\n",
    "# optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# best_loss = 1000; best_state = None\n",
    "# # TODO: also need the adversarial examples as well (Use ~idx to sample)\n",
    "# for i in range(500):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "#     loss = pcgrad_closure(return_list=True)\n",
    "    \n",
    "#     if i%10==0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.007301265839487314\n",
      "MSE Loss 1.4892339095240459e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-f31165840332>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    }
   ],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    }
   ],
   "source": [
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003655 +0.500415i)h_xx\n",
      "    + (0.002494 +1.002397i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(semisup_model.state_dict(), \"saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning both the solver and selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading weights\")\n",
    "semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4987566575873643e-06\n",
      "1.4900882661095238e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, history_size=150)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train+1j*v_train)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.007301265839487314\n",
      "MSE Loss 1.4892339095240459e-06\n",
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-3e99f5f7ba17>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003655 +0.500415i)h_xx\n",
      "    + (0.002494 +1.002397i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)\n",
    "\n",
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n",
    "\n",
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "\n",
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "# Due to the different loss calculaition in ComplexAttentionSelectorNetwork's forward pass\n",
    "# Reinit the selector network weights in a bad way\n",
    "# Reinit != slow convergence if the # of data samples are small\n",
    "semisup_model.selector.nonlinear_model = ComplexTorchMLP(dimensions=[len(feature_names), 50, 50, 1], activation_function=CplxToCplx[F.relu](), bn=True, dropout_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-ba084f34cba4>:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009462682530283928\n",
      "[2 4 3 1 0]\n",
      "0.00012346268340479583\n",
      "[2 4 3 1 0]\n",
      "4.246818571118638e-05\n",
      "[4 2 3 1 0]\n",
      "2.7820395189337432e-05\n",
      "[4 2 3 1 0]\n",
      "2.1012918296037242e-05\n",
      "[4 2 3 1 0]\n",
      "2.1220354028628208e-05\n",
      "[4 2 3 1 0]\n",
      "2.1100586309330538e-05\n",
      "[4 2 3 1 0]\n",
      "2.1043266315246e-05\n",
      "[4 2 3 1 0]\n",
      "2.1002655557822436e-05\n",
      "[4 2 3 1 0]\n",
      "2.0970772311557084e-05\n",
      "[4 2 3 1 0]\n",
      "2.097493052133359e-05\n",
      "[4 2 3 1 0]\n",
      "2.0998120817239396e-05\n",
      "[4 2 3 1 0]\n",
      "2.094088267767802e-05\n",
      "[4 2 3 1 0]\n",
      "2.0928497178829275e-05\n",
      "[4 2 3 1 0]\n",
      "2.0904442862956785e-05\n",
      "[4 2 3 1 0]\n",
      "2.089530971716158e-05\n",
      "[4 2 3 1 0]\n",
      "2.0890165615128353e-05\n",
      "[4 2 3 1 0]\n",
      "2.087976099574007e-05\n",
      "[4 2 3 1 0]\n",
      "2.0877710994682275e-05\n",
      "[4 2 3 1 0]\n",
      "2.1003785150242038e-05\n",
      "[4 2 3 1 0]\n",
      "2.0901590687572025e-05\n",
      "[4 2 3 1 0]\n",
      "2.087394750560634e-05\n",
      "[4 2 3 1 0]\n",
      "2.085621417791117e-05\n",
      "[4 2 3 1 0]\n",
      "2.0839179342146963e-05\n",
      "[4 2 3 1 0]\n",
      "2.082792343571782e-05\n",
      "[4 2 3 1 0]\n",
      "2.0829787899856456e-05\n",
      "[4 2 3 1 0]\n",
      "2.0933934138156474e-05\n",
      "[4 2 3 1 0]\n",
      "2.0871442757197656e-05\n",
      "[4 2 3 1 0]\n",
      "2.0893181499559432e-05\n",
      "[4 2 3 1 0]\n",
      "2.0832574591622688e-05\n",
      "[4 2 3 1 0]\n",
      "2.080792182823643e-05\n",
      "[4 2 3 1 0]\n",
      "2.0810308342333883e-05\n",
      "[4 2 3 1 0]\n",
      "2.080411468341481e-05\n",
      "[4 2 3 1 0]\n",
      "2.080065314657986e-05\n",
      "[4 2 3 1 0]\n",
      "2.0888453946099617e-05\n",
      "[4 2 3 1 0]\n",
      "2.0811443391721696e-05\n",
      "[4 2 3 1 0]\n",
      "2.0834559109061956e-05\n",
      "[4 2 3 1 0]\n",
      "2.0821918951696716e-05\n",
      "[4 2 3 1 0]\n",
      "2.0800576749024913e-05\n",
      "[4 2 3 1 0]\n",
      "2.0778257749043405e-05\n",
      "[4 2 3 1 0]\n",
      "2.0761317500728182e-05\n",
      "[4 2 3 1 0]\n",
      "2.075908014376182e-05\n",
      "[4 2 3 1 0]\n",
      "2.0755223886226304e-05\n",
      "[4 2 3 1 0]\n",
      "2.074473559332546e-05\n",
      "[4 2 3 1 0]\n",
      "2.074236181215383e-05\n",
      "[4 2 3 1 0]\n",
      "2.0737463273690082e-05\n",
      "[4 2 3 1 0]\n",
      "2.073261930490844e-05\n",
      "[4 2 3 1 0]\n",
      "2.073872929031495e-05\n",
      "[4 2 3 1 0]\n",
      "2.0834691895288415e-05\n",
      "[4 2 3 1 0]\n",
      "2.07913872145582e-05\n",
      "[4 2 3 1 0]\n",
      "2.080231934087351e-05\n",
      "[4 2 3 1 0]\n",
      "2.0741357730003074e-05\n",
      "[4 2 3 1 0]\n",
      "2.0709141608676873e-05\n",
      "[4 2 3 1 0]\n",
      "2.0714134734589607e-05\n",
      "[4 2 3 1 0]\n",
      "2.071488597721327e-05\n",
      "[4 2 3 1 0]\n",
      "2.0697963918792084e-05\n",
      "[4 2 3 1 0]\n",
      "2.069629408651963e-05\n",
      "[4 2 3 1 0]\n",
      "2.0691019017249346e-05\n",
      "[4 2 3 1 0]\n",
      "2.0729607058456168e-05\n",
      "[4 2 3 1 0]\n",
      "2.086601671180688e-05\n",
      "[4 2 3 1 0]\n",
      "2.078776560665574e-05\n",
      "[4 2 3 1 0]\n",
      "2.0740922991535626e-05\n",
      "[4 2 3 1 0]\n",
      "2.0715411665150896e-05\n",
      "[4 2 3 1 0]\n",
      "2.0696588762803003e-05\n",
      "[4 2 3 1 0]\n",
      "2.0687206415459514e-05\n",
      "[4 2 3 1 0]\n",
      "2.0678224245784804e-05\n",
      "[4 2 3 1 0]\n",
      "2.067221430479549e-05\n",
      "[4 2 3 1 0]\n",
      "2.067798050120473e-05\n",
      "[4 2 3 1 0]\n",
      "2.0751973352162167e-05\n",
      "[4 2 3 1 0]\n",
      "2.0783678337465972e-05\n",
      "[4 2 3 1 0]\n",
      "2.069482798106037e-05\n",
      "[4 2 3 1 0]\n",
      "2.065406988549512e-05\n",
      "[4 2 3 1 0]\n",
      "2.067689274554141e-05\n",
      "[4 2 3 1 0]\n",
      "2.0652058083214797e-05\n",
      "[4 2 3 1 0]\n",
      "2.0644196411012672e-05\n",
      "[4 2 3 1 0]\n",
      "2.0643306925194338e-05\n",
      "[4 2 3 1 0]\n",
      "2.0633102394640446e-05\n",
      "[4 2 3 1 0]\n",
      "2.063107240246609e-05\n",
      "[4 2 3 1 0]\n",
      "2.0626161131076515e-05\n",
      "[4 2 3 1 0]\n",
      "2.0619954739231616e-05\n",
      "[4 2 3 1 0]\n",
      "2.061582199530676e-05\n",
      "[4 2 3 1 0]\n",
      "2.0612711523426697e-05\n",
      "[4 2 3 1 0]\n",
      "2.061418490484357e-05\n",
      "[4 2 3 1 0]\n",
      "2.0654606487369165e-05\n",
      "[4 2 3 1 0]\n",
      "2.085082269331906e-05\n",
      "[4 2 3 1 0]\n",
      "2.0603254597517662e-05\n",
      "[4 2 3 1 0]\n",
      "2.0663876057369635e-05\n",
      "[4 2 3 1 0]\n",
      "2.0651394152082503e-05\n",
      "[4 2 3 1 0]\n",
      "2.0602978111128323e-05\n",
      "[4 2 3 1 0]\n",
      "2.0595580281224102e-05\n",
      "[4 2 3 1 0]\n",
      "2.0601815776899457e-05\n",
      "[4 2 3 1 0]\n",
      "2.0589737687259912e-05\n",
      "[4 2 3 1 0]\n",
      "2.0583593141054735e-05\n",
      "[4 2 3 1 0]\n",
      "2.058325662801508e-05\n",
      "[4 2 3 1 0]\n",
      "2.0584615413099527e-05\n",
      "[4 2 3 1 0]\n",
      "2.0657396817114204e-05\n",
      "[4 2 3 1 0]\n",
      "2.0683914044639096e-05\n",
      "[4 2 3 1 0]\n",
      "2.0634435713873245e-05\n",
      "[4 2 3 1 0]\n",
      "2.0577945178956725e-05\n",
      "[4 2 3 1 0]\n",
      "2.0571045752149075e-05\n",
      "[4 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the selector network\n",
    "# Redefining the f_opt and the finetuning_closure function\n",
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=150, history_size=150)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # Am I forget to normalize the derivative features?, NVM\n",
    "    loss = complex_mse(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "for i in range(500):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        print(np.argsort(semisup_model.selector.latest_weighted_features.detach().numpy()))\n",
    "        \n",
    "        # Changing the optimizer\n",
    "        if i==20: f_opt = MADGRAD(semisup_model.selector.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "hf 0.5301014\n",
      "|hf| 0.20531888\n",
      "h_xx 0.10464061\n",
      "h_x 0.081372954\n",
      "h_xxx 0.07856609\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Softmax layer's outputs as feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=1/len(feature_names), save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SHAP to compute neural network's feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0589570340234786e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5301, 0.2053, 0.0814, 0.1046, 0.0786], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.eval()\n",
    "loss = complex_mse(semisup_model.selector(X_selector), y_selector)\n",
    "print(loss.item())\n",
    "semisup_model.selector.latest_weighted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting attributes on ParameterDict is not supported.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=F.softmax, bn=True)\n",
    "sel.load_state_dict(semisup_model.selector.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(nn.Module):\n",
    "    def __init__(self, sel):\n",
    "        super(Selector, self).__init__()\n",
    "        self.model = sel\n",
    "    def forward(self, inp):\n",
    "        out = self.model(inp)\n",
    "        out = torch.hstack([out.real, out.imag])\n",
    "        return out\n",
    "    \n",
    "selector = Selector(sel=sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = shap.DeepExplainer(selector, X_selector)\n",
    "# shap_values = e.shap_values(X_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector = X_selector.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Using 1000 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "fn = lambda inp : selector(torch.tensor(inp, dtype=torch.cfloat)).detach().numpy()\n",
    "explainer = shap.KernelExplainer(model=fn, data=X_selector, link=\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7576784c092a4240bf5a9cda31f01fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(X=X_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.summary_plot(shap_values, feature_names=feature_names, class_names=['Real', 'Imaginary'], show=False)\n",
    "plt.savefig(\"../visualization/nls_shap_feature_importances_selector_with_softmax.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
