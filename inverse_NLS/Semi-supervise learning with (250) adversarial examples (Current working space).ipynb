{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, ComplexTorchMLP, complex_mse\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Training with 500 unsup samples\n",
      "Loading pre-calculated data for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500; include_N_res = 1\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res>0:\n",
    "    N_res = int(N*include_N_res)\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = idx_res[:N_res]\n",
    "    X_res = to_tensor(X_star[idx_res, :], True)\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_train = torch.vstack([X_train, X_res])\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n",
    "\n",
    "### Loading data code here ###\n",
    "print(\"Loading pre-calculated data for reproducibility\")\n",
    "X_train = to_tensor(np.load(\"./tmp_files/X_train_500+500samples.npy\"), True)\n",
    "u_train, v_train = dimension_slicing(to_tensor(np.load(\"./tmp_files/uv_train_500samples.npy\"), False))\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_dim = x.shape[0]\n",
    "time_dim = t.shape[0]\n",
    "\n",
    "dt = (t[1]-t[0])[0]\n",
    "dx = (x[2]-x[1])[0]\n",
    "\n",
    "fd_h_t = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_x = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xxx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "\n",
    "for i in range(spatial_dim):\n",
    "    fd_h_t[i,:] = FiniteDiff(Exact[i,:], dt, 1)\n",
    "for i in range(time_dim):\n",
    "    fd_h_x[:,i] = FiniteDiff(Exact[:,i], dx, 1)\n",
    "    fd_h_xx[:,i] = FiniteDiff(Exact[:,i], dx, 2)\n",
    "    fd_h_xxx[:,i] = FiniteDiff(Exact[:,i], dx, 3)\n",
    "    \n",
    "fd_h_t = np.reshape(fd_h_t, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_x = np.reshape(fd_h_x, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xx = np.reshape(fd_h_xx, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xxx = np.reshape(fd_h_xxx, (spatial_dim*time_dim,1), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )\n",
    "\n",
    "# complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_cpinn_model.pth\"))\n",
    "complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goals\n",
    "(1) Re-implement the semisup_model for a complex network.\n",
    "\n",
    "(2) Implement the self.gradients function.\n",
    "- complex_model(input) -> diff(u_pred, x) & diff(v_pred, x) -> combine 2 diff terms as 1 complex vector -> compute PDE loss / passing to the selector network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train[:N, :], True))\n",
    "predictions = complex_model(cat(xx, tt))\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.00830211490392685\n",
      "MSE Loss 3.642554656835273e-05\n"
     ]
    }
   ],
   "source": [
    "# PDE Loss 1.1325556442898232e-05\n",
    "# MSE Loss 4.512887699092971e-06\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions, u_train+1j*v_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00+0.0000000e+00j,  1.8287402e-01+9.5849973e-01j,\n",
       "         9.5216465e-01+0.0000000e+00j, ...,\n",
       "         1.7851934e+01-4.6664425e+01j,  3.1865616e+01-9.3093842e+01j,\n",
       "         5.6193420e+01-1.8545619e+02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.7636567e-01+1.4373595e-01j,\n",
       "         4.7813055e-01+0.0000000e+00j, ...,\n",
       "        -2.3702056e+00+5.3106542e+00j,  6.6533599e+00-1.3724621e+01j,\n",
       "        -1.8578472e+01+3.5425545e+01j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.0341483e-01+2.3708993e-01j,\n",
       "         4.2032108e-01+0.0000000e+00j, ...,\n",
       "        -4.2094916e-01+4.9473611e-01j, -8.7446731e-01+1.8341792e+00j,\n",
       "        -1.1678257e+00+6.2480083e+00j],\n",
       "       ...,\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  3.6971271e-02+5.2270472e-02j,\n",
       "         4.0990775e-03+0.0000000e+00j, ...,\n",
       "         3.8725277e-04+4.5041209e-03j, -4.1001476e-07+3.6970994e-03j,\n",
       "        -2.5966717e-04+3.0123498e-03j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  1.5098518e-01+1.0398209e-01j,\n",
       "         3.3608802e-02+0.0000000e+00j, ...,\n",
       "         1.6517833e-02+3.3025961e-02j, -1.2393138e-02-2.4222329e-02j,\n",
       "         9.2946738e-03+1.7763579e-02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  5.4721653e-02+4.6720326e-02j,\n",
       "         5.1772478e-03+0.0000000e+00j, ...,\n",
       "         4.0766774e-03+6.3557024e-03j, -2.9551531e-03-5.7514533e-03j,\n",
       "         2.0485490e-03+5.1446026e-03j]], dtype=complex64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "complex_poly_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.008224 +0.498694i)h_xx\n",
      "    + (-0.006229 +0.997566i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, 1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation w/ and w/o Finite difference guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives.append(cplx2tensor(uf))\n",
    "                derivatives.append((uf.real**2+uf.imag**2)+0.0j)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=-1), u_t\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complex_network = ComplexNetwork(model=complex_model, index2features=feature_names, scale=True, lb=lb, ub=ub)\n",
    "# X_selector, y_selector = complex_network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexAttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.01):\n",
    "        super(ComplexAttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = CplxLinear(layers[0], layers[0], bias=True)\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = ComplexTorchMLP(dimensions=layers, activation_function=CplxToCplx[F.relu](), bn=bn, dropout_rate=0.0)\n",
    "        self.latest_weighted_features = None\n",
    "#         self.th = 0.1\n",
    "        self.th = 1/layers[0]\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        feature_importances = self.weighted_features(inn)\n",
    "        inn = inn*feature_importances\n",
    "        return self.nonlinear_model(inn)\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n",
    "        self.latest_weighted_features = self.latest_weighted_features.mean(dim=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = complex_mse(ut_approx, y_input)\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        return mse_loss+self.reg_intensity*(torch.norm(reg_term, p=0)+(torch.tensor([1.0, 1.0, 1.0, 2.0, 3.0])*reg_term).sum())\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None, uncert=False):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.weights = None\n",
    "        if uncert: \n",
    "            self.weights = torch.tensor([0.0, 0.0])\n",
    "        \n",
    "    def forward(self, X_h_train, h_train, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_h_train))\n",
    "        \n",
    "        h_row = h_train.shape[0]\n",
    "        fd_guidance = complex_mse(self.network.uf[:h_row, :], h_train)\n",
    "        \n",
    "        # I am not sure a good way to normalize/scale a complex tensor\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "            \n",
    "        if include_unsup and self.weights is not None:\n",
    "            return (torch.exp(-self.weights[0])*fd_guidance)+self.weights[0], (torch.exp(-self.weights[1])*unsup_loss)+self.weights[1]\n",
    "        else:\n",
    "            return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "h_star = (u_star+1j*v_star)\n",
    "\n",
    "fd_derivatives = np.hstack([h_star, h_star.real**2+h_star.imag**2, fd_h_x, fd_h_xx, fd_h_xxx])\n",
    "\n",
    "semisup_model = SemiSupModel(\n",
    "    network=ComplexNetwork(model=complex_model, index2features=feature_names, scale=False, lb=lb, ub=ub),\n",
    "    selector=ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=F.softmax, bn=True),\n",
    "    normalize_derivative_features=False,\n",
    "    mini=torch.tensor(np.abs(fd_derivatives).min(axis=0), dtype=torch.cfloat),\n",
    "    maxi=torch.tensor(np.abs(fd_derivatives).max(axis=0), dtype=torch.cfloat),\n",
    "    uncert=True,\n",
    ")\n",
    "\n",
    "del h_star, fd_derivatives, fd_h_x, fd_h_xx, fd_h_xxx\n",
    "\n",
    "# semisup_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_UNCERT = True\n",
    "def pcgrad_closure(return_list=False):\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_train, u_train+1j*v_train, include_unsup=True)      \n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-11\n",
    "# optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# best_loss = 1000; best_state = None\n",
    "# # TODO: also need the adversarial examples as well (Use ~idx to sample)\n",
    "# for i in range(500):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "#     loss = pcgrad_closure(return_list=True)\n",
    "    \n",
    "#     if i%10==0:\n",
    "#         print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.007301265839487314\n",
      "MSE Loss 1.4892339095240459e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-f31165840332>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    }
   ],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    }
   ],
   "source": [
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003655 +0.500415i)h_xx\n",
      "    + (0.002494 +1.002397i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(semisup_model.state_dict(), \"saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning both the solver and selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading weights\")\n",
    "semisup_model.load_state_dict(torch.load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples_jointtrainwith500unlabeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4987566575873643e-06\n",
      "1.4900882661095238e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n",
      "1.4892339095240459e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, history_size=150)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train+1j*v_train)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.007301265839487314\n",
      "MSE Loss 1.4892339095240459e-06\n",
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-3e99f5f7ba17>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003655 +0.500415i)h_xx\n",
      "    + (0.002494 +1.002397i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)\n",
    "\n",
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n",
    "\n",
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "\n",
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:587: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "# Due to the different loss calculaition in ComplexAttentionSelectorNetwork's forward pass\n",
    "# Reinit the selector network weights in a bad way\n",
    "# Reinit != slow convergence if the # of data samples are small\n",
    "semisup_model.selector.nonlinear_model = ComplexTorchMLP(dimensions=[len(feature_names), 50, 50, 1], activation_function=CplxToCplx[F.relu](), bn=True, dropout_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-ba084f34cba4>:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  self.latest_weighted_features = self.prob_activation(cplx2tensor(self.linear1(inn)).abs())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005335964262485504\n",
      "[4 2 3 1 0]\n",
      "9.335909271612763e-05\n",
      "[4 2 3 1 0]\n",
      "3.366429154993966e-05\n",
      "[4 2 3 1 0]\n",
      "2.104513805534225e-05\n",
      "[4 2 3 1 0]\n",
      "1.67073703778442e-05\n",
      "[4 2 3 1 0]\n",
      "1.682903712207917e-05\n",
      "[4 2 3 1 0]\n",
      "1.676961619523354e-05\n",
      "[4 2 3 1 0]\n",
      "1.6730016795918345e-05\n",
      "[4 2 3 1 0]\n",
      "1.669245830271393e-05\n",
      "[4 2 3 1 0]\n",
      "1.6684312868164852e-05\n",
      "[4 2 3 1 0]\n",
      "1.6668720490997657e-05\n",
      "[4 2 3 1 0]\n",
      "1.6665322618791834e-05\n",
      "[4 2 3 1 0]\n",
      "1.66656645887997e-05\n",
      "[4 2 3 1 0]\n",
      "1.6654146747896448e-05\n",
      "[4 2 3 1 0]\n",
      "1.665886156843044e-05\n",
      "[4 2 3 1 0]\n",
      "1.666183197812643e-05\n",
      "[4 2 3 1 0]\n",
      "1.6645484720356762e-05\n",
      "[4 2 3 1 0]\n",
      "1.6654910723445937e-05\n",
      "[4 2 3 1 0]\n",
      "1.6679203326930292e-05\n",
      "[4 2 3 1 0]\n",
      "1.666817297518719e-05\n",
      "[4 2 3 1 0]\n",
      "1.66475037985947e-05\n",
      "[4 2 3 1 0]\n",
      "1.66279078257503e-05\n",
      "[4 2 3 1 0]\n",
      "1.6618734662188217e-05\n",
      "[4 2 3 1 0]\n",
      "1.6622310795355588e-05\n",
      "[4 2 3 1 0]\n",
      "1.6612822946626693e-05\n",
      "[4 2 3 1 0]\n",
      "1.660857378738001e-05\n",
      "[4 2 3 1 0]\n",
      "1.6613010302535258e-05\n",
      "[4 2 3 1 0]\n",
      "1.6762875020503998e-05\n",
      "[4 2 3 1 0]\n",
      "1.6664409486111253e-05\n",
      "[4 2 3 1 0]\n",
      "1.6649442841298878e-05\n",
      "[4 2 3 1 0]\n",
      "1.6631167454761453e-05\n",
      "[4 2 3 1 0]\n",
      "1.661218084336724e-05\n",
      "[4 2 3 1 0]\n",
      "1.6599999071331695e-05\n",
      "[4 2 3 1 0]\n",
      "1.65933324751677e-05\n",
      "[4 2 3 1 0]\n",
      "1.6589185179327615e-05\n",
      "[4 2 3 1 0]\n",
      "1.6585219782427885e-05\n",
      "[4 2 3 1 0]\n",
      "1.6579873772570863e-05\n",
      "[4 2 3 1 0]\n",
      "1.6575253539485857e-05\n",
      "[4 2 3 1 0]\n",
      "1.6573419998167083e-05\n",
      "[4 2 3 1 0]\n",
      "1.6573245375184342e-05\n",
      "[4 2 3 1 0]\n",
      "1.666981006565038e-05\n",
      "[4 2 3 1 0]\n",
      "1.659999179537408e-05\n",
      "[4 2 3 1 0]\n",
      "1.660076668485999e-05\n",
      "[4 2 3 1 0]\n",
      "1.6590631275903434e-05\n",
      "[4 2 3 1 0]\n",
      "1.6576817870372906e-05\n",
      "[4 2 3 1 0]\n",
      "1.656616041145753e-05\n",
      "[4 2 3 1 0]\n",
      "1.65610654221382e-05\n",
      "[4 2 3 1 0]\n",
      "1.6558606148464605e-05\n",
      "[4 2 3 1 0]\n",
      "1.6556774426135235e-05\n",
      "[4 2 3 1 0]\n",
      "1.656151289353147e-05\n",
      "[4 2 3 1 0]\n",
      "1.6700396372471005e-05\n",
      "[4 2 3 1 0]\n",
      "1.655245250731241e-05\n",
      "[4 2 3 1 0]\n",
      "1.6552537999814376e-05\n",
      "[4 2 3 1 0]\n",
      "1.6551402950426564e-05\n",
      "[4 2 3 1 0]\n",
      "1.6553854948142543e-05\n",
      "[4 2 3 1 0]\n",
      "1.655468986427877e-05\n",
      "[4 2 3 1 0]\n",
      "1.6552239685552195e-05\n",
      "[4 2 3 1 0]\n",
      "1.65483434102498e-05\n",
      "[4 2 3 1 0]\n",
      "1.654422885621898e-05\n",
      "[4 2 3 1 0]\n",
      "1.6540452634217218e-05\n",
      "[4 2 3 1 0]\n",
      "1.6537425835849717e-05\n",
      "[4 2 3 1 0]\n",
      "1.6535042959731072e-05\n",
      "[4 2 3 1 0]\n",
      "1.6532925656065345e-05\n",
      "[4 2 3 1 0]\n",
      "1.653060462558642e-05\n",
      "[4 2 3 1 0]\n",
      "1.652791070227977e-05\n",
      "[4 2 3 1 0]\n",
      "1.6525516912224703e-05\n",
      "[4 2 3 1 0]\n",
      "1.652324863243848e-05\n",
      "[4 2 3 1 0]\n",
      "1.652088758419268e-05\n",
      "[4 2 3 1 0]\n",
      "1.651862112339586e-05\n",
      "[4 2 3 1 0]\n",
      "1.6516449250048026e-05\n",
      "[4 2 3 1 0]\n",
      "1.651435377425514e-05\n",
      "[4 2 3 1 0]\n",
      "1.6512560250703245e-05\n",
      "[4 2 3 1 0]\n",
      "1.651382990530692e-05\n",
      "[4 2 3 1 0]\n",
      "1.6546438928344287e-05\n",
      "[4 2 3 1 0]\n",
      "1.6702539141988382e-05\n",
      "[4 2 3 1 0]\n",
      "1.6524147213203833e-05\n",
      "[4 2 3 1 0]\n",
      "1.6510528439539485e-05\n",
      "[4 2 3 1 0]\n",
      "1.6524541933904402e-05\n",
      "[4 2 3 1 0]\n",
      "1.652595165069215e-05\n",
      "[4 2 3 1 0]\n",
      "1.6515437891939655e-05\n",
      "[4 2 3 1 0]\n",
      "1.6503583537996747e-05\n",
      "[4 2 3 1 0]\n",
      "1.6499816410942003e-05\n",
      "[4 2 3 1 0]\n",
      "1.6500325727974996e-05\n",
      "[4 2 3 1 0]\n",
      "1.649706973694265e-05\n",
      "[4 2 3 1 0]\n",
      "1.6493819202878512e-05\n",
      "[4 2 3 1 0]\n",
      "1.6492482245666906e-05\n",
      "[4 2 3 1 0]\n",
      "1.6490281268488616e-05\n",
      "[4 2 3 1 0]\n",
      "1.6492533177370206e-05\n",
      "[4 2 3 1 0]\n",
      "1.652514038141817e-05\n",
      "[4 2 3 1 0]\n",
      "1.6545509424759075e-05\n",
      "[4 2 3 1 0]\n",
      "1.652233004278969e-05\n",
      "[4 2 3 1 0]\n",
      "1.649339719733689e-05\n",
      "[4 2 3 1 0]\n",
      "1.6485400919918902e-05\n",
      "[4 2 3 1 0]\n",
      "1.64878110808786e-05\n",
      "[4 2 3 1 0]\n",
      "1.6488433175254613e-05\n",
      "[4 2 3 1 0]\n",
      "1.648460965952836e-05\n",
      "[4 2 3 1 0]\n",
      "1.648002944421023e-05\n",
      "[4 2 3 1 0]\n",
      "1.647869066800922e-05\n",
      "[4 2 3 1 0]\n",
      "1.6477897588629276e-05\n",
      "[4 2 3 1 0]\n",
      "1.6480025806231424e-05\n",
      "[4 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the selector network\n",
    "# Redefining the f_opt and the finetuning_closure function\n",
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=1e-1, max_iter=150, history_size=150)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = complex_mse(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "for i in range(500):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        print(np.argsort(semisup_model.selector.latest_weighted_features.detach().numpy()))\n",
    "        \n",
    "        # Changing the optimizer\n",
    "        if i==20: f_opt = MADGRAD(semisup_model.selector.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "hf 0.5308258\n",
      "|hf| 0.21773937\n",
      "h_xx 0.10504037\n",
      "h_x 0.07604277\n",
      "h_xxx 0.07035169\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Softmax layer's outputs as feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=1/len(feature_names), save_path='../visualization/nls_feature_importances_selector_with_softmax.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
