{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.6\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, \n",
    "                    cplx2tensor, ComplexTorchMLP, complex_mse, TanhProb)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "from madgrad import MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running on cpu\n",
      "Training with 500 unsup samples\n",
      "Loading pre-calculated data for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# torch device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You're running on\", device)\n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "DATA_PATH = '../experimental_data/NLS.mat'\n",
    "data = io.loadmat(DATA_PATH)\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = to_column_vector(Exact_u.T)\n",
    "v_star = to_column_vector(Exact_v.T)\n",
    "\n",
    "N = 500; include_N_res = 1\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "# idx = np.arange(N) # Just have an easy dataset for experimenting\n",
    "\n",
    "lb = to_tensor(lb, False).to(device)\n",
    "ub = to_tensor(ub, False).to(device)\n",
    "\n",
    "X_train = to_tensor(X_star[idx, :], True).to(device)\n",
    "u_train = to_tensor(u_star[idx, :], False).to(device)\n",
    "v_train = to_tensor(v_star[idx, :], False).to(device)\n",
    "\n",
    "# Unsup data\n",
    "if include_N_res>0:\n",
    "    N_res = int(N*include_N_res)\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = idx_res[:N_res]\n",
    "    X_res = to_tensor(X_star[idx_res, :], True)\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_train = torch.vstack([X_train, X_res])\n",
    "\n",
    "feature_names = ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n",
    "\n",
    "### Loading data code here ###\n",
    "print(\"Loading pre-calculated data for reproducibility\")\n",
    "X_train = to_tensor(np.load(\"./tmp_files/X_train_500+500samples.npy\"), True)\n",
    "u_train, v_train = dimension_slicing(to_tensor(np.load(\"./tmp_files/uv_train_500samples.npy\"), False))\n",
    "### ----- ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_dim = x.shape[0]\n",
    "time_dim = t.shape[0]\n",
    "\n",
    "dt = (t[1]-t[0])[0]\n",
    "dx = (x[2]-x[1])[0]\n",
    "\n",
    "fd_h_t = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_x = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "fd_h_xxx = np.zeros((spatial_dim, time_dim), dtype=np.complex64)\n",
    "\n",
    "for i in range(spatial_dim):\n",
    "    fd_h_t[i,:] = FiniteDiff(Exact[i,:], dt, 1)\n",
    "for i in range(time_dim):\n",
    "    fd_h_x[:,i] = FiniteDiff(Exact[:,i], dx, 1)\n",
    "    fd_h_xx[:,i] = FiniteDiff(Exact[:,i], dx, 2)\n",
    "    fd_h_xxx[:,i] = FiniteDiff(Exact[:,i], dx, 3)\n",
    "    \n",
    "fd_h_t = np.reshape(fd_h_t, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_x = np.reshape(fd_h_x, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xx = np.reshape(fd_h_xx, (spatial_dim*time_dim,1), order='F')\n",
    "fd_h_xxx = np.reshape(fd_h_xxx, (spatial_dim*time_dim,1), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_dimension = 2\n",
    "act = CplxToCplx[torch.tanh]\n",
    "complex_model = CplxSequential(\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 100, bias=True),\n",
    "                            act(),\n",
    "                            CplxLinear(100, 1, bias=True),\n",
    "                            )\n",
    "\n",
    "complex_model = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(inp_dimension, 200),\n",
    "                                    RealToCplx(),\n",
    "                                    complex_model\n",
    "                                    )\n",
    "\n",
    "# complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_cpinn_model.pth\"))\n",
    "complex_model.load_state_dict(cpu_load(\"./saved_path_inverse_nls/NLS_complex_model_500labeledsamples.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train[:N, :], True))\n",
    "predictions = complex_model(cat(xx, tt))\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.00830211490392685\n",
      "MSE Loss 3.642554656835273e-05\n"
     ]
    }
   ],
   "source": [
    "# PDE Loss 1.1325556442898232e-05\n",
    "# MSE Loss 4.512887699092971e-06\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions, u_train+1j*v_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "# dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000000e+00+0.0000000e+00j,  1.8287402e-01+9.5849973e-01j,\n",
       "         9.5216465e-01+0.0000000e+00j, ...,\n",
       "         1.7851934e+01-4.6664425e+01j,  3.1865616e+01-9.3093842e+01j,\n",
       "         5.6193420e+01-1.8545619e+02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.7636567e-01+1.4373595e-01j,\n",
       "         4.7813055e-01+0.0000000e+00j, ...,\n",
       "        -2.3702056e+00+5.3106542e+00j,  6.6533599e+00-1.3724621e+01j,\n",
       "        -1.8578472e+01+3.5425545e+01j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  6.0341483e-01+2.3708993e-01j,\n",
       "         4.2032108e-01+0.0000000e+00j, ...,\n",
       "        -4.2094916e-01+4.9473611e-01j, -8.7446731e-01+1.8341792e+00j,\n",
       "        -1.1678257e+00+6.2480083e+00j],\n",
       "       ...,\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  3.6971271e-02+5.2270472e-02j,\n",
       "         4.0990775e-03+0.0000000e+00j, ...,\n",
       "         3.8725277e-04+4.5041209e-03j, -4.1001476e-07+3.6970994e-03j,\n",
       "        -2.5966717e-04+3.0123498e-03j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  1.5098518e-01+1.0398209e-01j,\n",
       "         3.3608802e-02+0.0000000e+00j, ...,\n",
       "         1.6517833e-02+3.3025961e-02j, -1.2393138e-02-2.4222329e-02j,\n",
       "         9.2946738e-03+1.7763579e-02j],\n",
       "       [ 1.0000000e+00+0.0000000e+00j,  5.4721653e-02+4.6720326e-02j,\n",
       "         5.1772478e-03+0.0000000e+00j, ...,\n",
       "         4.0766774e-03+6.3557024e-03j, -2.9551531e-03-5.7514533e-03j,\n",
       "         2.0485490e-03+5.1446026e-03j]], dtype=complex64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "complex_poly_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.008224 +0.498694i)h_xx\n",
      "    + (-0.006229 +0.997566i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation w/ and w/o Finite difference guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetwork(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(ComplexNetwork, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = complex_diff(uf, t)\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='hf': \n",
    "                derivatives.append(cplx2tensor(uf))\n",
    "                derivatives.append((uf.real**2+uf.imag**2)+0.0j)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = complex_diff(out, x)\n",
    "                elif c=='t': out = complex_diff(out, t)\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=-1), u_t\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexAttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=5e-2):\n",
    "        super(ComplexAttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = CplxLinear(layers[0], layers[0], bias=True)\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = ComplexTorchMLP(dimensions=layers, activation_function=CplxToCplx[F.relu](), bn=bn, dropout_rate=0.0)\n",
    "        self.latest_weighted_features = None\n",
    "#         self.th = 0.1\n",
    "        self.th = (1/layers[0])+(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = torch.tensor([1.0, 1.0, 2.0, 2.0, 3.0])/10\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*F.threshold(self.weighted_features(inn), self.th, 0.0))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "#         self.latest_weighted_features = cplx2tensor(self.linear1(inn)).abs().mean(dim=0)\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn).real).mean(dim=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        l1 = complex_mse(ut_approx, y_input)\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1 + self.reg_intensity*l2\n",
    "\n",
    "# Only the SemiSupModel has changed to work with the finite difference guidance\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None, uncert=False):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.weights = None\n",
    "        if uncert: \n",
    "            self.weights = torch.tensor([0.0, 0.0])\n",
    "        \n",
    "    def forward(self, X_h_train, h_train, include_unsup=True):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_h_train))\n",
    "        \n",
    "        h_row = h_train.shape[0]\n",
    "        fd_guidance = complex_mse(self.network.uf[:h_row, :], h_train)\n",
    "        \n",
    "        # I am not sure a good way to normalize/scale a complex tensor\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        \n",
    "        if include_unsup: unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        else: unsup_loss = None\n",
    "            \n",
    "        if include_unsup and self.weights is not None:\n",
    "            return (torch.exp(-self.weights[0])*fd_guidance)+self.weights[0], (torch.exp(-self.weights[1])*unsup_loss)+self.weights[1]\n",
    "        else:\n",
    "            return fd_guidance, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering ['hf', '|hf|', 'h_x', 'h_xx', 'h_xxx']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "h_star = (u_star+1j*v_star)\n",
    "\n",
    "fd_derivatives = np.hstack([h_star, h_star.real**2+h_star.imag**2, fd_h_x, fd_h_xx, fd_h_xxx])\n",
    "\n",
    "semisup_model = SemiSupModel(\n",
    "    network=ComplexNetwork(model=complex_model, index2features=feature_names, scale=False, lb=lb, ub=ub),\n",
    "    selector=ComplexAttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=TanhProb(), bn=True),\n",
    "    normalize_derivative_features=False,\n",
    "    mini=torch.tensor(np.abs(fd_derivatives).min(axis=0), dtype=torch.cfloat),\n",
    "    maxi=torch.tensor(np.abs(fd_derivatives).max(axis=0), dtype=torch.cfloat),\n",
    "    uncert=True,\n",
    ")\n",
    "\n",
    "del h_star, fd_derivatives, fd_h_x, fd_h_xx, fd_h_xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_UNCERT = True\n",
    "def pcgrad_closure(return_list=False):\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    fd_guidance, unsup_loss = semisup_model(X_train, u_train+1j*v_train, include_unsup=True)      \n",
    "    losses = [fd_guidance, unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 1e-7\n",
    "# optimizer.param_groups[1]['lr'] = 1e-1\n",
    "\n",
    "# best_loss = 1000; best_state = None\n",
    "# for i in range(100):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "    \n",
    "#     if i%10==0: \n",
    "#         loss = pcgrad_closure(return_list=True); print(loss)\n",
    "#         fi = semisup_model.selector.latest_weighted_features\n",
    "#         print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pongpisit/Desktop/Multi-task-Physics-informed-neural-networks/inverse_NLS/../utils.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(arr).float().requires_grad_(g)\n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.007333296816796064\n",
      "MSE Loss 3.642554656835273e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-f31165840332>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    }
   ],
   "source": [
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading weights\")\n",
    "semisup_model.load_state_dict(torch.load(\"tmp.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning both the solver and selector network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1693950909830164e-06\n",
      "1.167013579106424e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n",
      "1.1668932984321145e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=500, history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global IS_UNCERT, N, X_train, u_train, v_train, fd_derivatives, fd_u_t\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = complex_mse(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train+1j*v_train)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss 0.006740952841937542\n",
      "MSE Loss 1.1668932984321145e-06\n",
      "Computing hf\n",
      "Computing |hf|\n",
      "Computing h_x\n",
      "Computing h_xx\n",
      "Computing h_xxx\n",
      "Computing hf^2\n",
      "Computing hf |hf|\n",
      "Computing hf h_x\n",
      "Computing hf h_xx\n",
      "Computing hf h_xxx\n",
      "Computing |hf|^2\n",
      "Computing |hf| h_x\n",
      "Computing |hf| h_xx\n",
      "Computing |hf| h_xxx\n",
      "Computing h_x^2\n",
      "Computing h_x h_xx\n",
      "Computing h_x h_xxx\n",
      "Computing h_xx^2\n",
      "Computing h_xx h_xxx\n",
      "Computing h_xxx^2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-3e99f5f7ba17>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (0.003456 +0.501238i)h_xx\n",
      "    + (0.002515 +1.003159i)hf |hf|\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "xx, tt = dimension_slicing(to_tensor(X_train, True))\n",
    "predictions = semisup_model.network(xx, tt)\n",
    "h = cplx2tensor(predictions)\n",
    "h_x = complex_diff(predictions, xx)\n",
    "h_xx = complex_diff(h_x, xx)\n",
    "h_xxx = complex_diff(h_xx, xx)\n",
    "h_t = complex_diff(predictions, tt)\n",
    "\n",
    "f = 1j*h_t+0.5*h_xx+(h.abs()**2)*h\n",
    "real_loss = (f.real**2).mean(); imag_loss = (f.imag**2).mean()\n",
    "avg_loss = (real_loss+imag_loss)*0.5\n",
    "print(\"PDE Loss\", avg_loss.item())\n",
    "print(\"MSE Loss\", complex_mse(predictions[:N, :], torch.tensor(u_train+1j*v_train, dtype=torch.cfloat)).item())\n",
    "\n",
    "derivatives = to_numpy(cat(h, h.abs()**2, h_x, h_xx, h_xxx))\n",
    "dictionary = {}\n",
    "for i in range(len(feature_names)): dictionary[feature_names[i]] = get_feature(derivatives, i)\n",
    "\n",
    "c_poly = ComplexPolynomialFeatures(feature_names, dictionary)\n",
    "complex_poly_features = c_poly.fit()\n",
    "\n",
    "w = TrainSTRidge(complex_poly_features, to_numpy(h_t), 1e-10, d_tol=1000, maxit=1000)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, c_poly.poly_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the different loss calculaition in ComplexAttentionSelectorNetwork's forward pass\n",
    "# Reinit the selector network weights in a bad way\n",
    "# Reinit != slow convergence if the # of data samples are small\n",
    "\n",
    "# semisup_model.selector.nonlinear_model = ComplexTorchMLP(dimensions=[len(feature_names), 50, 50, 1], activation_function=CplxToCplx[F.relu](), bn=True, dropout_rate=0.0)\n",
    "# semisup_model.selector.th = 1/len(feature_names)+(1e-10)\n",
    "semisup_model.selector.reg_intensity = 1e-3 # V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037240830715745687\n",
      "[0.9329563  0.54446095 0.16231604 0.2599501  0.08875209]\n",
      "[4 2 3 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the selector network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=5e-2, max_iter=300, history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # Am I forget to normalize the derivative features?, NVM\n",
    "    loss = semisup_model.selector.loss(X_selector, y_selector) # V2\n",
    "#     loss = complex_mse(semisup_model.selector(X_selector), y_selector) # V1\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train()\n",
    "\n",
    "for i in range(1):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0:\n",
    "        with torch.no_grad():\n",
    "            loss = finetuning_closure()\n",
    "            print(loss.item())\n",
    "            \n",
    "            fi = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "            print(fi)\n",
    "            print(np.argsort(fi))\n",
    "\n",
    "            # Changing the optimizer\n",
    "            if i==20: f_opt = MADGRAD(semisup_model.selector.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"semisup_modelV2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "hf 0.9329563\n",
      "|hf| 0.54446095\n",
      "h_xx 0.2599501\n",
      "h_x 0.16231604\n",
      "h_xxx 0.08875209\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Feature importance\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=1/len(feature_names), save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
