{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm, catboost, xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, \n",
    "                    cplx2tensor, ComplexTorchMLP, complex_mse, TanhProb)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from rkstiff import grids\n",
    "# from rkstiff import if34\n",
    "\n",
    "# # Computing the KS sol\n",
    "# # uniform grid spacing, real-valued u -> construct_x_kx_rfft\n",
    "# N = 512\n",
    "# a, b = -10, 10\n",
    "# x, kx = grids.construct_x_kx_rfft(N, a, b)\n",
    "\n",
    "# L = kx**2*(1-kx**2)\n",
    "# def NL(uFFT):\n",
    "#     u = np.fft.irfft(uFFT)\n",
    "#     ux = np.fft.irfft(1j*kx*uFFT)\n",
    "#     return -np.fft.rfft(u*ux)\n",
    "\n",
    "# u0 = -np.sin(np.pi*x/10)\n",
    "# u0FFT = np.fft.rfft(u0)\n",
    "# solver = if34.IF34(linop=L,NLfunc=NL)\n",
    "# ufFFT = solver.evolve(u0FFT, t0=0, tf=10, h_init=0.2) # store every 20th step in solver.u and solver.t\n",
    "\n",
    "# U = []\n",
    "# for uFFT in solver.u:\n",
    "#     U.append(np.fft.irfft(uFFT))\n",
    "# U = np.array(U)\n",
    "# t = np.array(solver.t)\n",
    "\n",
    "# X_sol, T_sol = np.meshgrid(x, t)\n",
    "# Exact = U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../deephpms_data/KS_simple3.pkl\n"
     ]
    }
   ],
   "source": [
    "# Loading the KS sol\n",
    "data = pickle_load(\"../deephpms_data/KS_simple3.pkl\")\n",
    "\n",
    "t = data['t']\n",
    "x = data['x']\n",
    "\n",
    "X_sol, T_sol = np.meshgrid(x, t)\n",
    "Exact = data['u'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 500 samples\n",
      "Training with 500 unsup samples\n"
     ]
    }
   ],
   "source": [
    "x_star = X_sol.flatten()[:,None]\n",
    "t_star = T_sol.flatten()[:,None]\n",
    "\n",
    "X_star = np.hstack((x_star, t_star))\n",
    "u_star = Exact.T.flatten()[:,None]\n",
    "\n",
    "# DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "# X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True, uniform=True, x_limit=None, t_limit=None)\n",
    "# X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Bound\n",
    "ub = X_star.max(axis=0)\n",
    "lb = X_star.min(axis=0)\n",
    "\n",
    "# For identification\n",
    "N = 500\n",
    "# idx = np.arange(N)\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "print(\"Training with\", N, \"samples\")\n",
    "\n",
    "# Unsup data\n",
    "include_N_res = True; portion = 1\n",
    "if include_N_res:\n",
    "    N_res = int(portion)*N\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]\n",
    "        derivatives = []\n",
    "        derivatives.append(uf)\n",
    "        derivatives.append(u_x)\n",
    "        derivatives.append(u_xx)\n",
    "        derivatives.append(u_xxxx)\n",
    "        derivatives.append(u_xxx)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=1e-2):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        self.linear1.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])-(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 4.0, 3.0])\n",
    "#         self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 3.0, 3.0])\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*(F.threshold(self.weighted_features(inn), self.th, 0.0)))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        \n",
    "        l1 = mse_loss\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1+self.reg_intensity*(l2)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "pretrained_state_dict = cpu_load(\"./saved_path_inverse_small_KS/simple3_semisup_model_state_dict_250labeledsamples.pth\")\n",
    "# pretrained_state_dict = None\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=TanhProb(), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "#     if best_state_dict is not None: semisup_model.load_state_dict(best_state_dict)\n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00012057299318257719"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance without semi-supervised training & adversarial exmaples\n",
    "F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_test = 50000\n",
    "# n_test = min(n_test, X_star.shape[0])\n",
    "# idx_test = np.arange(n_test)\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "# alpha = 1\n",
    "# const_range = (-1.5, 1.5)\n",
    "\n",
    "# X_input = referenced_derivatives\n",
    "# y_input = u_t\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# X_input = poly.fit_transform(X_input)\n",
    "\n",
    "# poly_feature_names = poly.get_feature_names(feature_names)\n",
    "# for i, f in enumerate(poly_feature_names):\n",
    "#     poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set normalize=1\n",
    "# w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 1000, maxit=1000, STR_iters=100, split=0.8, l0_penalty=1, normalize=1)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    predictions, unsup_loss = semisup_model(X_train)\n",
    "    losses = [F.mse_loss(predictions[:N, :], u_train[:N, :]), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 5e-7\n",
    "# optimizer.param_groups[1]['lr'] = 5e-2\n",
    "\n",
    "# # Use ~idx to sample adversarial data points\n",
    "# for i in range(150):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "    \n",
    "#     if i%25==0: \n",
    "#         loss = pcgrad_closure(return_list=True)\n",
    "#         print(loss)\n",
    "        \n",
    "#         fi = semisup_model.selector.latest_weighted_features\n",
    "#         print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model's weights properly\n"
     ]
    }
   ],
   "source": [
    "semisup_model = load_weights(semisup_model, \"semisup_model_500_500_unfinetuned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6090585859274142e-06\n",
      "1.4372204759638407e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "1.436693537471001e-06\n",
      "2.718012001423631e-05\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        \n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "X_selector = (X_selector - semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0201, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0201, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0196, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0203, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n",
      "tensor(0.0197, grad_fn=<MseLossBackward>)\n",
      "tensor([0.4231, 0.2315, 0.6544, 0.7772, 0.1938], grad_fn=<MeanBackward1>)\n",
      "tensor([4, 1, 0, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=5e-2, max_iter=500, max_eval=int(1.25*500), history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = F.mse_loss(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train() \n",
    "\n",
    "max_it = 100\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss)\n",
    "        \n",
    "        fi = semisup_model.selector.latest_weighted_features\n",
    "        print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4230953 , 0.23147082, 0.6544171 , 0.77721953, 0.19377734],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.latest_weighted_features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "u_xxxx 0.77721953\n",
      "u_xx 0.6544171\n",
      "uf 0.4230953\n",
      "u_x 0.23147082\n",
      "u_xxx 0.19377734\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.615383 +0.000000i)u_xx\n",
      "    + (-0.533011 +0.000000i)u_xxxx\n",
      "    + (-0.495500 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "n_test = 50000\n",
    "n_test = min(n_test, X_star.shape[0])\n",
    "idx_test = np.arange(n_test)\n",
    "\n",
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))\n",
    "referenced_derivatives = to_numpy(X_selector); u_t = to_numpy(y_selector)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "\n",
    "# w = TrainSTRidge(X_input[:, :], y_input, 1e-3, d_tol=500 or 1000, STR_iters=100, split=0.8, l0_penalty=1, normalize=1) for noisy all\n",
    "# Set normalize=1\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 1000, maxit=1000, STR_iters=100, split=0.8, l0_penalty=1, normalize=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use this... with 2 digits acc.\n",
    "# array([0.26119122, 0.24299507, 0.63743615, 0.37287104, 0.19907106],\n",
    "#       dtype=float32)\n",
    "\n",
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 4.5], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=0.2, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7500 samples\n",
    "\n",
    "# Clean all\n",
    "# u_t = (-0.744498 +0.000000i)u_xx\n",
    "#     + (-0.649956 +0.000000i)u_xxxx\n",
    "#     + (-0.562872 +0.000000i)uf*u_x\n",
    "\n",
    "# Clean (x, t) and noisy labels\n",
    "# u_t = (-0.665316 +0.000000i)u_xx\n",
    "#     + (-0.589087 +0.000000i)u_xxxx\n",
    "#     + (-0.528588 +0.000000i)uf*u_x\n",
    "\n",
    "# Noisy (x, t) and noisy labels, 7500 samples, 2 epochs\n",
    "# u_t = (-0.209252 +0.000000i)u_xx\n",
    "#     + (-0.158271 +0.000000i)u_xxxx\n",
    "#     + (-0.174638 +0.000000i)uf*u_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
