{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Python 3.9.7\n",
      "You can use npar for np.array\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm, catboost, xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import (TorchComplexMLP, ImaginaryDimensionAdder, \n",
    "                    cplx2tensor, ComplexTorchMLP, complex_mse, TanhProb)\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "# from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from rkstiff import grids\n",
    "# from rkstiff import if34\n",
    "\n",
    "# # Computing the KS sol\n",
    "# # uniform grid spacing, real-valued u -> construct_x_kx_rfft\n",
    "# N = 512\n",
    "# a, b = -10, 10\n",
    "# x, kx = grids.construct_x_kx_rfft(N, a, b)\n",
    "\n",
    "# L = kx**2*(1-kx**2)\n",
    "# def NL(uFFT):\n",
    "#     u = np.fft.irfft(uFFT)\n",
    "#     ux = np.fft.irfft(1j*kx*uFFT)\n",
    "#     return -np.fft.rfft(u*ux)\n",
    "\n",
    "# u0 = -np.sin(np.pi*x/10)\n",
    "# u0FFT = np.fft.rfft(u0)\n",
    "# solver = if34.IF34(linop=L,NLfunc=NL)\n",
    "# ufFFT = solver.evolve(u0FFT, t0=0, tf=10, h_init=0.2) # store every 20th step in solver.u and solver.t\n",
    "\n",
    "# U = []\n",
    "# for uFFT in solver.u:\n",
    "#     U.append(np.fft.irfft(uFFT))\n",
    "# U = np.array(U)\n",
    "# t = np.array(solver.t)\n",
    "\n",
    "# X_sol, T_sol = np.meshgrid(x, t)\n",
    "# Exact = U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from ../deephpms_data/KS_simple3.pkl\n"
     ]
    }
   ],
   "source": [
    "# Loading the KS sol\n",
    "data = pickle_load(\"../deephpms_data/KS_simple3.pkl\")\n",
    "\n",
    "t = data['t']\n",
    "x = data['x']\n",
    "\n",
    "X_sol, T_sol = np.meshgrid(x, t)\n",
    "Exact = data['u'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 500 samples\n",
      "Training with 500 unsup samples\n"
     ]
    }
   ],
   "source": [
    "x_star = X_sol.flatten()[:,None]\n",
    "t_star = T_sol.flatten()[:,None]\n",
    "\n",
    "X_star = np.hstack((x_star, t_star))\n",
    "u_star = Exact.T.flatten()[:,None]\n",
    "\n",
    "# DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "# X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True, uniform=True, x_limit=None, t_limit=None)\n",
    "# X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Bound\n",
    "ub = X_star.max(axis=0)\n",
    "lb = X_star.min(axis=0)\n",
    "\n",
    "# For identification\n",
    "N = 500\n",
    "# idx = np.arange(N)\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "print(\"Training with\", N, \"samples\")\n",
    "\n",
    "# Unsup data\n",
    "include_N_res = True; portion = 1\n",
    "if include_N_res:\n",
    "    N_res = int(portion)*N\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx', 'u_xxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # 'uf', 'u_x', 'u_xx', 'u_xxxx', 'u_xxx'\n",
    "        u_x = self.gradients(uf, x)[0]\n",
    "        u_xx = self.gradients(u_x, x)[0]\n",
    "        u_xxx = self.gradients(u_xx, x)[0]\n",
    "        u_xxxx = self.gradients(u_xxx, x)[0]\n",
    "        u_xxxxx = self.gradients(u_xxxx, x)[0]\n",
    "        derivatives = []\n",
    "        derivatives.append(uf)\n",
    "        derivatives.append(u_x)\n",
    "        derivatives.append(u_xx)\n",
    "        derivatives.append(u_xxx)\n",
    "        derivatives.append(u_xxxx)\n",
    "        derivatives.append(u_xxxxx)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=1e-3):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        self.linear1.bias.data.fill_(0.01)\n",
    "        \n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = (1/layers[0])-(1e-10)\n",
    "        self.reg_intensity = reg_intensity\n",
    "        self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "#         self.w = (0.1)*torch.tensor([1.0, 1.0, 2.0, 3.0, 3.0])\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*(F.threshold(self.weighted_features(inn), self.th, 0.0)))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        \n",
    "        l1 = mse_loss\n",
    "        l2 = torch.norm(reg_term, p=0)+torch.dot(self.w, reg_term)\n",
    "        return l1+self.reg_intensity*(l2)\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx', 'u_xxxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Use pretrained weights\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = None\n",
    "pretrained_state_dict = cpu_load(\"./pretrained_semisup_model.pth\")\n",
    "use_pretrained_weights = True\n",
    "lets_pretrain = False\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=TanhProb(), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "#     if best_state_dict is not None: semisup_model.load_state_dict(best_state_dict)\n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3842526388762053e-06"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance without semi-supervised training & adversarial exmaples\n",
    "F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_test = 50000\n",
    "# n_test = min(n_test, X_star.shape[0])\n",
    "# idx_test = np.arange(n_test)\n",
    "# referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "# alpha = 1\n",
    "# const_range = (-1.5, 1.5)\n",
    "\n",
    "# X_input = referenced_derivatives\n",
    "# y_input = u_t\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "# X_input = poly.fit_transform(X_input)\n",
    "\n",
    "# poly_feature_names = poly.get_feature_names(feature_names)\n",
    "# for i, f in enumerate(poly_feature_names):\n",
    "#     poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set normalize=1\n",
    "# w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 1000, maxit=1000, STR_iters=100, split=0.8, l0_penalty=1, normalize=1)\n",
    "# print(\"PDE derived using STRidge\")\n",
    "# print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcgrad_closure(return_list=False):\n",
    "    global N, X_train, u_train\n",
    "    predictions, unsup_loss = semisup_model(X_train)\n",
    "    losses = [F.mse_loss(predictions[:N, :], u_train[:N, :]), unsup_loss]\n",
    "    updated_grads = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        losses[i].backward(retain_graph=True)\n",
    "\n",
    "        g_task = []\n",
    "        for param in semisup_model.parameters():\n",
    "            if param.grad is not None:\n",
    "                g_task.append(Variable(param.grad.clone(), requires_grad=False))\n",
    "            else:\n",
    "                g_task.append(Variable(torch.zeros(param.shape), requires_grad=False))\n",
    "        # appending the gradients from each task\n",
    "        updated_grads.append(g_task)\n",
    "\n",
    "    updated_grads = list(pcgrad.pc_grad_update(updated_grads))[0]\n",
    "    for idx, param in enumerate(semisup_model.parameters()):\n",
    "        param.grad = (updated_grads[0][idx]+updated_grads[1][idx])\n",
    "        \n",
    "    if not return_list: return sum(losses)\n",
    "    else: return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Joint training\n",
    "# optimizer = MADGRAD([{'params':semisup_model.network.parameters()}, {'params':semisup_model.selector.parameters()}], lr=1e-6)\n",
    "# optimizer.param_groups[0]['lr'] = 5e-7\n",
    "# optimizer.param_groups[1]['lr'] = 5e-2\n",
    "\n",
    "# # Use ~idx to sample adversarial data points\n",
    "# for i in range(150):\n",
    "#     semisup_model.train()\n",
    "#     optimizer.step(pcgrad_closure)\n",
    "    \n",
    "#     if i%25==0: \n",
    "#         loss = pcgrad_closure(return_list=True)\n",
    "#         print(loss)\n",
    "        \n",
    "#         fi = semisup_model.selector.latest_weighted_features\n",
    "#         print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model's weights properly\n"
     ]
    }
   ],
   "source": [
    "semisup_model = load_weights(semisup_model, \"semisup_model_500_500_uxxxxx_unfinetuned.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning the solver network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1858071502501843e-06\n",
      "1.1109449360446888e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "1.1106244528491516e-06\n",
      "3.0216292543627787e-06\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning the solver network\n",
    "f_opt = torch.optim.LBFGS(semisup_model.network.parameters(), lr=1e-1, max_iter=300, max_eval=int(1.25*300), history_size=300)\n",
    "\n",
    "def finetuning_closure():\n",
    "    global N, X_train, u_train\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    # the solver network only consider the first N samples.\n",
    "    loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.train()\n",
    "semisup_model.selector.eval()\n",
    "\n",
    "for i in range(200):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    if i%10==0:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss.item())\n",
    "        \n",
    "print(F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(semisup_model, \"tmp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "X_selector = (X_selector - semisup_model.mini)/(semisup_model.maxi-semisup_model.mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "tensor([0.6371, 0.8879, 0.6166, 0.1602, 0.3779, 0.1538],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([5, 3, 4, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "f_opt = torch.optim.LBFGS(semisup_model.selector.parameters(), lr=5e-2, max_iter=500, max_eval=int(1.25*500), history_size=500)\n",
    "\n",
    "def finetuning_closure():\n",
    "    if torch.is_grad_enabled(): f_opt.zero_grad()\n",
    "    loss = F.mse_loss(semisup_model.selector(X_selector), y_selector)\n",
    "    if loss.requires_grad: loss.backward(retain_graph=True)\n",
    "    return loss\n",
    "\n",
    "semisup_model.network.eval()\n",
    "semisup_model.selector.train() \n",
    "\n",
    "max_it = 100\n",
    "for i in range(max_it):\n",
    "    f_opt.step(finetuning_closure)\n",
    "    \n",
    "    if i%5==0 or i==max_it-1:\n",
    "        loss = finetuning_closure()\n",
    "        print(loss)\n",
    "        \n",
    "        fi = semisup_model.selector.latest_weighted_features\n",
    "        print(fi); print(torch.argsort(fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6370662 , 0.88786477, 0.6165936 , 0.16017805, 0.37794724,\n",
       "       0.15380639], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semisup_model.selector.latest_weighted_features.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature importance ranking ---\n",
      "u_x 0.88786477\n",
      "uf 0.6370662\n",
      "u_xx 0.6165936\n",
      "u_xxxx 0.37794724\n",
      "u_xxx 0.16017805\n",
      "u_xxxxx 0.15380639\n"
     ]
    }
   ],
   "source": [
    "feature_importance = semisup_model.selector.latest_weighted_features.detach().numpy()\n",
    "print(\"--- Feature importance ranking ---\")\n",
    "for idx in np.argsort(feature_importance)[::-1]:\n",
    "    print(feature_names[idx], feature_importance[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.700235 +0.000000i)u_xx\n",
      "    + (-0.611180 +0.000000i)u_xxxx\n",
      "    + (-0.514702 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "n_test = 50000\n",
    "n_test = min(n_test, X_star.shape[0])\n",
    "idx_test = np.arange(n_test)\n",
    "\n",
    "X_selector, y_selector = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))\n",
    "referenced_derivatives = to_numpy(X_selector); u_t = to_numpy(y_selector)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")\n",
    "    \n",
    "# Set normalize=1\n",
    "w = TrainSTRidge(X_input[:, :], y_input, 1e-6, 1000, maxit=1000, STR_iters=100, split=0.8, l0_penalty=1, normalize=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use this... with 2 digits acc.\n",
    "# array([0.26119122, 0.24299507, 0.63743615, 0.37287104, 0.19907106],\n",
    "#       dtype=float32)\n",
    "\n",
    "# --- Feature importance ranking ---\n",
    "# u_x 0.88786477\n",
    "# uf 0.6370662\n",
    "# u_xx 0.6165936\n",
    "# u_xxxx 0.37794724\n",
    "# u_xxx 0.16017805\n",
    "# u_xxxxx 0.15380639\n",
    "\n",
    "def plot_feature_importance_neural_net(feature_importances, feature_names, threshold=0.2, save_path=None):\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(feature_importance - threshold, 0)\n",
    "    below_threshold = np.minimum(feature_importance, threshold)\n",
    "\n",
    "    # and plot it\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(feature_names, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(feature_names, above_threshold, 0.35, color=\"r\",\n",
    "            bottom=below_threshold)\n",
    "    # horizontal line indicating the threshold\n",
    "    ax.plot([0., 5.], [threshold, threshold], \"k--\")\n",
    "    plt.xlabel(\"Partial derivative features (possible candidates)\")\n",
    "    plt.ylabel(\"Feature importances\")\n",
    "    \n",
    "    if save_path is not None: fig.savefig(save_path, dpi=200)\n",
    "\n",
    "plot_feature_importance_neural_net(feature_importance, feature_names,threshold=1/6, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
