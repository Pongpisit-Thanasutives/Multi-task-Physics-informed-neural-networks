{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as io\n",
    "from pyDOE import lhs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from complexPyTorch.complexLayers import ComplexLinear\n",
    "\n",
    "import cplxmodule\n",
    "from cplxmodule import cplx\n",
    "from cplxmodule.nn import RealToCplx, CplxToReal, CplxSequential, CplxToCplx\n",
    "from cplxmodule.nn import CplxLinear, CplxModReLU, CplxAdaptiveModReLU, CplxModulus, CplxAngle\n",
    "\n",
    "# To access the contents of the parent dir\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from utils import *\n",
    "from models import TorchComplexMLP, ImaginaryDimensionAdder, cplx2tensor, ComplexTorchMLP, complex_mse\n",
    "from preprocess import *\n",
    "\n",
    "# Model selection\n",
    "from sparsereg.model import STRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from pde_diff import TrainSTRidge, FiniteDiff, print_pde\n",
    "from RegscorePy.bic import bic\n",
    "\n",
    "# Fancy optimizers\n",
    "from madgrad import MADGRAD\n",
    "from lbfgsnew import LBFGSNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10000 samples\n",
      "Not including N_res\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('../deephpms_data/KS.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol'])\n",
    "\n",
    "X_sol, T_sol = np.meshgrid(x, t)\n",
    "\n",
    "x_star = X_sol.flatten()[:,None]\n",
    "t_star = T_sol.flatten()[:,None]\n",
    "\n",
    "X_star = np.hstack((x_star, t_star))\n",
    "u_star = Exact.T.flatten()[:,None]\n",
    "\n",
    "# DATA_PATH = '../PDE_FIND_experimental_datasets/kuramoto_sivishinky.mat'\n",
    "# X, T, Exact = space_time_grid(data_path=DATA_PATH, real_solution=True, uniform=True, x_limit=None, t_limit=None)\n",
    "# X_star, u_star = get_trainable_data(X, T, Exact)\n",
    "\n",
    "# Bound\n",
    "ub = X_star.max(axis=0)\n",
    "lb = X_star.min(axis=0)\n",
    "\n",
    "# For identification\n",
    "N = 10000\n",
    "# idx = np.arange(N)\n",
    "idx = np.random.choice(X_star.shape[0], N, replace=False)\n",
    "X_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "print(\"Training with\", N, \"samples\")\n",
    "\n",
    "# Unsup data\n",
    "include_N_res = False\n",
    "if include_N_res:\n",
    "    N_res = N//2\n",
    "    idx_res = np.array(range(X_star.shape[0]-1))[~idx]\n",
    "    idx_res = np.random.choice(idx_res.shape[0], N_res, replace=True)\n",
    "    X_res = X_star[idx_res, :]\n",
    "    print(f\"Training with {N_res} unsup samples\")\n",
    "    X_u_train = np.vstack([X_train, X_res])\n",
    "    u_train = np.vstack([u_train, torch.rand(X_res.shape[0], 1) - 1000])\n",
    "    # del X_res\n",
    "else: print(\"Not including N_res\")\n",
    "    \n",
    "# Convert to torch.tensor\n",
    "X_train = to_tensor(X_train, True)\n",
    "u_train = to_tensor(u_train, False)\n",
    "X_star = to_tensor(X_star, True)\n",
    "u_star = to_tensor(u_star, False)\n",
    "\n",
    "# lb and ub are used in adversarial training\n",
    "scaling_factor = 1.0\n",
    "lb = scaling_factor*to_tensor(lb, False)\n",
    "ub = scaling_factor*to_tensor(ub, False)\n",
    "\n",
    "# Feature names\n",
    "feature_names=('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, model, index2features=None, scale=False, lb=None, ub=None):\n",
    "        super(Network, self).__init__()\n",
    "        # pls init the self.model before\n",
    "        self.model = model\n",
    "        # For tracking, the default tup is for the burgers' equation.\n",
    "        self.index2features = index2features\n",
    "        print(\"Considering\", self.index2features)\n",
    "        self.diff_flag = diff_flag(self.index2features)\n",
    "        self.uf = None\n",
    "        self.scale = scale\n",
    "        self.lb, self.ub = lb, ub\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if not self.scale: self.uf = self.model(torch.cat([x, t], dim=1))\n",
    "        else: self.uf = self.model(self.neural_net_scale(torch.cat([x, t], dim=1)))\n",
    "        return self.uf\n",
    "    \n",
    "    def get_selector_data(self, x, t):\n",
    "        uf = self.forward(x, t)\n",
    "        u_t = self.gradients(uf, t)[0]\n",
    "        \n",
    "        ### PDE Loss calculation ###\n",
    "        # Without calling grad\n",
    "        derivatives = []\n",
    "        for t in self.diff_flag[0]:\n",
    "            if t=='uf': derivatives.append(uf)\n",
    "            elif t=='x': derivatives.append(x)\n",
    "        # With calling grad\n",
    "        for t in self.diff_flag[1]:\n",
    "            out = uf\n",
    "            for c in t:\n",
    "                if c=='x': out = self.gradients(out, x)[0]\n",
    "                elif c=='t': out = self.gradients(out, t)[0]\n",
    "            derivatives.append(out)\n",
    "        \n",
    "        return torch.cat(derivatives, dim=1), u_t\n",
    "    \n",
    "    def gradients(self, func, x):\n",
    "        return grad(func, x, create_graph=True, retain_graph=True, grad_outputs=torch.ones(func.shape))\n",
    "    \n",
    "    def neural_net_scale(self, inp):\n",
    "        return 2*(inp-self.lb)/(self.ub-self.lb)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSelectorNetwork(nn.Module):\n",
    "    def __init__(self, layers, prob_activation=torch.sigmoid, bn=None, reg_intensity=0.1):\n",
    "        super(AttentionSelectorNetwork, self).__init__()\n",
    "        # Nonlinear model, Training with PDE reg.\n",
    "        assert len(layers) > 1\n",
    "        self.linear1 = nn.Linear(layers[0], layers[0])\n",
    "        self.prob_activation = prob_activation\n",
    "        self.nonlinear_model = TorchMLP(dimensions=layers, activation_function=nn.Tanh, bn=bn, dropout=nn.Dropout(p=0.1))\n",
    "        self.latest_weighted_features = None\n",
    "        self.th = 0.1\n",
    "        self.reg_intensity = reg_intensity\n",
    "        \n",
    "    def xavier_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward(self, inn):\n",
    "        return self.nonlinear_model(inn*self.weighted_features(inn))\n",
    "    \n",
    "    def weighted_features(self, inn):\n",
    "        self.latest_weighted_features = self.prob_activation(self.linear1(inn)).mean(axis=0)\n",
    "        return self.latest_weighted_features\n",
    "    \n",
    "    def loss(self, X_input, y_input):\n",
    "        ut_approx = self.forward(X_input)\n",
    "        mse_loss = F.mse_loss(ut_approx, y_input, reduction='mean')\n",
    "        reg_term = F.relu(self.latest_weighted_features-self.th)\n",
    "        return mse_loss+self.reg_intensity*(torch.norm(reg_term, p=0)+(torch.tensor([1.0, 1.0, 2.0, 3.0, 4.0])*reg_term).sum())\n",
    "\n",
    "class SemiSupModel(nn.Module):\n",
    "    def __init__(self, network, selector, normalize_derivative_features=False, mini=None, maxi=None):\n",
    "        super(SemiSupModel, self).__init__()\n",
    "        self.network = network\n",
    "        self.selector = selector\n",
    "        self.normalize_derivative_features = normalize_derivative_features\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        \n",
    "    def forward(self, X_u_train):\n",
    "        X_selector, y_selector = self.network.get_selector_data(*dimension_slicing(X_u_train))\n",
    "        if self.normalize_derivative_features:\n",
    "            X_selector = (X_selector-self.mini)/(self.maxi-self.mini)\n",
    "        unsup_loss = self.selector.loss(X_selector, y_selector)\n",
    "        return self.network.uf, unsup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n",
      "Considering ('uf', 'u_x', 'u_xx', 'u_xxx', 'u_xxxx')\n",
      "Using old implementation of TorchMLP. See models.py for more new model-related source code.\n"
     ]
    }
   ],
   "source": [
    "### Version with normalized derivatives ###\n",
    "# pretrained_state_dict = cpu_load(\"../inverse_KS/saved_path_inverse_ks/semisup_model_with_LayerNormDropout_without_physical_reg_trained30000labeledsamples_trained0unlabeledsamples.pth\")\n",
    "# pretrained_state_dict = cpu_load(\"semisup_model_state_dict.pth\")\n",
    "pretrained_state_dict = None\n",
    "use_pretrained_weights = False\n",
    "lets_pretrain = True\n",
    "\n",
    "semisup_model = SemiSupModel(network=Network(\n",
    "                                    model=TorchMLP(dimensions=[2, 50, 50, 50 ,50, 50, 1],\n",
    "                                                   activation_function=nn.Tanh,\n",
    "                                                   bn=nn.LayerNorm, dropout=None),\n",
    "                                    index2features=feature_names, scale=True, lb=lb, ub=ub),\n",
    "                            selector=AttentionSelectorNetwork([len(feature_names), 50, 50, 1], prob_activation=nn.Softmax(dim=1), bn=nn.LayerNorm),\n",
    "                            normalize_derivative_features=True,\n",
    "                            mini=None,\n",
    "                            maxi=None)\n",
    "\n",
    "if use_pretrained_weights:\n",
    "    print(\"Use pretrained weights\")\n",
    "    semisup_model.load_state_dict(pretrained_state_dict)\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "#     semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "#     semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining\n",
      "Epoch 0:  0.003545314073562622\n",
      "Test MSE: 3.8e-03\n",
      "Epoch 10:  2.7006650270777754e-05\n",
      "Test MSE: 3.0e-05\n",
      "Epoch 20:  1.9923425497836433e-05\n",
      "Test MSE: 2.3e-05\n",
      "Epoch 30:  1.8414441001368687e-05\n",
      "Test MSE: 2.1e-05\n",
      "Epoch 40:  1.579972740728408e-05\n",
      "Test MSE: 1.8e-05\n",
      "Epoch 50:  1.4198953067534603e-05\n",
      "Test MSE: 1.6e-05\n",
      "Epoch 60:  1.3893720279156696e-05\n",
      "Test MSE: 1.6e-05\n",
      "Epoch 70:  1.3472734281094745e-05\n",
      "Test MSE: 1.5e-05\n",
      "Epoch 80:  1.31282895381446e-05\n",
      "Test MSE: 1.5e-05\n",
      "Epoch 90:  1.2774522474501282e-05\n",
      "Test MSE: 1.5e-05\n",
      "Epoch 100:  1.2243061064509675e-05\n",
      "Test MSE: 1.4e-05\n",
      "Epoch 110:  1.1576370525290258e-05\n",
      "Test MSE: 1.3e-05\n",
      "Computing derivatives features\n"
     ]
    }
   ],
   "source": [
    "if lets_pretrain:\n",
    "    print(\"Pretraining\")\n",
    "    pretraining_optimizer = LBFGSNew(semisup_model.network.parameters(),\n",
    "                                     lr=1e-1, max_iter=300,\n",
    "                                     max_eval=int(300*1.25), history_size=150,\n",
    "                                     line_search_fn=True, batch_mode=False)\n",
    "\n",
    "    semisup_model.network.train()    \n",
    "    for i in range(120):\n",
    "        def pretraining_closure():\n",
    "            global N, X_u_train, u_train\n",
    "            if torch.is_grad_enabled():\n",
    "                pretraining_optimizer.zero_grad()\n",
    "            # Only focusing on first [:N, :] elements\n",
    "            mse_loss = F.mse_loss(semisup_model.network(*dimension_slicing(X_train[:N, :])), u_train[:N, :])\n",
    "            if mse_loss.requires_grad:\n",
    "                mse_loss.backward(retain_graph=False)\n",
    "            return mse_loss\n",
    "\n",
    "        pretraining_optimizer.step(pretraining_closure)\n",
    "            \n",
    "        if (i%10)==0:\n",
    "            l = pretraining_closure()\n",
    "            curr_loss = l.item()\n",
    "            print(\"Epoch {}: \".format(i), curr_loss)\n",
    "\n",
    "            # Sneak on the test performance...\n",
    "            semisup_model.network.eval()\n",
    "            test_performance = F.mse_loss(semisup_model.network(*dimension_slicing(X_star)).detach(), u_star).item()\n",
    "            string_test_performance = scientific2string(test_performance)\n",
    "            print('Test MSE:', string_test_performance)\n",
    "    \n",
    "#     if best_state_dict is not None: semisup_model.load_state_dict(best_state_dict)\n",
    "    print(\"Computing derivatives features\")\n",
    "    semisup_model.eval()\n",
    "    referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_train))\n",
    "    semisup_model.mini = torch.min(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "    semisup_model.maxi = torch.max(referenced_derivatives, axis=0)[0].detach().requires_grad_(False)\n",
    "\n",
    "#     semisup_model.mini = tmp.min(axis=0)[0].requires_grad_(False)\n",
    "#     semisup_model.maxi = tmp.max(axis=0)[0].requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 10000\n",
    "idx_test = np.arange(n_test)\n",
    "referenced_derivatives, u_t = semisup_model.network.get_selector_data(*dimension_slicing(X_star[idx_test, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0050, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((((u_t+referenced_derivatives[:, 4:5]+(referenced_derivatives[:, 0:1]*referenced_derivatives[:, 1:2])+referenced_derivatives[:, 2:3]))**2).mean())\n",
    "referenced_derivatives = to_numpy(referenced_derivatives); u_t = to_numpy(u_t)\n",
    "\n",
    "alpha = 1\n",
    "const_range = (-1.5, 1.5)\n",
    "\n",
    "X_input = referenced_derivatives\n",
    "y_input = u_t\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)\n",
    "X_input = poly.fit_transform(X_input)\n",
    "\n",
    "poly_feature_names = poly.get_feature_names(feature_names)\n",
    "for i, f in enumerate(poly_feature_names):\n",
    "    poly_feature_names[i] = f.replace(\" \", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE derived using STRidge\n",
      "u_t = (-0.840458 +0.000000i)u_xx\n",
      "    + (-0.741539 +0.000000i)u_xxxx\n",
      "    + (-0.790747 +0.000000i)uf*u_x\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "w = TrainSTRidge(X_input[:, :10], y_input, 1e-10, 1000, maxit=1000, STR_iters=10, split=0.8, l0_penalty=1)\n",
    "print(\"PDE derived using STRidge\")\n",
    "print_pde(w, poly_feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not use this class with complex-valued input arrays.\n",
      "This class applies 1 transpose to the Exact before doing the job.\n",
      "dx = 0.039138943248532954\n",
      "dt = 0.2\n"
     ]
    }
   ],
   "source": [
    "fdc = FinDiffCalculator(X_sol, T_sol, Exact.T, acc_order=10)\n",
    "\n",
    "# Even if you use the true vals of dx and dt -> The finite difference is not working with a non-uniform grid\n",
    "# fdc.dx = 0.09817477042468103\n",
    "# fdc.dt = 0.4\n",
    "\n",
    "fd_u_t = to_tensor(fdc.finite_diff(1, diff_order=1), False)\n",
    "fd_derivatives = fdc.finite_diff_from_feature_names(feature_names)\n",
    "for d in fd_derivatives: fd_derivatives[d] = to_tensor(fd_derivatives[d], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating uf\n",
      "Concatenating u_x\n",
      "Concatenating u_xx\n",
      "Concatenating u_xxx\n",
      "Concatenating u_xxxx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.749761425657198e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = None\n",
    "for f in feature_names:\n",
    "    print('Concatenating', f)\n",
    "    if tmp == None: tmp = fd_derivatives[f]\n",
    "    else: tmp = torch.cat([tmp, fd_derivatives[f]], dim=-1)\n",
    "\n",
    "fd_derivatives = tmp[:, :]\n",
    "(((fd_u_t+fd_derivatives[:, 4:5]+(fd_derivatives[:, 0:1]*fd_derivatives[:, 1:2])+fd_derivatives[:, 2:3]))**2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
